{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "described-booking",
   "metadata": {},
   "source": [
    "## Try these two experiments first\n",
    "1. Create original input image to inchi string translator and get accuracy and loss\n",
    "2. Create inchi image to inchi string translator and get accuracy and loss (Use [AutoEncoders](https://www.youtube.com/watch?v=E28CVTbNoSA&ab_channel=PascalPoupart))\n",
    "\n",
    "Now, compare the two. If the accuracy of inchi image -> inchi string is significantly higher than the original image -> inchi string then think about ***reconstruction experiments*** from original image to inchi image. [try this then](https://www.google.com/search?channel=fs&client=ubuntu&q=converting+shapes+from+one+to+another+using+deep+learning).\n",
    "\n",
    "# InChI decoding \n",
    "[Source](https://link.springer.com/content/pdf/10.1186%2Fs13321-015-0068-4.pdf)\n",
    "\n",
    "1. **Skeletal connections layer** This layer prefixed with `/c` represents connections between skeletal atoms by listing the canonical numbers in the chain of connected atoms. \n",
    "2. ***branches are given in parentheses***\n",
    "3. The canonical atomic numbers, which are used throughout the InChI, are always given in the formula’s element order. i.e. precendence is given to element according to periodic table while numbering elements. For example, `/C10H16N5O13P3` (the beginning of InChI for adenosine triphosphate) implies that atoms numbered 1–10 are carbons, 11–15 arenitrogens, 16–28 are oxygens, and 29–31 are phosporus. Hydrogen atoms are not explicitly numbered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-memorial",
   "metadata": {
    "tags": []
   },
   "source": [
    "## image to inchi string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spatial-nashville",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:48.814753Z",
     "iopub.status.busy": "2021-04-10T18:04:48.814476Z",
     "iopub.status.idle": "2021-04-10T18:04:48.818339Z",
     "shell.execute_reply": "2021-04-10T18:04:48.817645Z",
     "shell.execute_reply.started": "2021-04-10T18:04:48.814718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Special Packages\n",
    "# !pip install PeriodicElements\n",
    "# !pip install albumentations\n",
    "# !pip install timm\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "great-banks",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:48.819600Z",
     "iopub.status.busy": "2021-04-10T18:04:48.819349Z",
     "iopub.status.idle": "2021-04-10T18:04:48.834847Z",
     "shell.execute_reply": "2021-04-10T18:04:48.834234Z",
     "shell.execute_reply.started": "2021-04-10T18:04:48.819572Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "active-personal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:48.835766Z",
     "iopub.status.busy": "2021-04-10T18:04:48.835568Z",
     "iopub.status.idle": "2021-04-10T18:04:50.886996Z",
     "shell.execute_reply": "2021-04-10T18:04:50.886440Z",
     "shell.execute_reply.started": "2021-04-10T18:04:48.835741Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import torch, torchmetrics, timm, re, pickle, Levenshtein\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from fastprogress import progress_bar\n",
    "from typing import Optional, Union, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from elements import elements\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from preprocessing import preprocess_image\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "torch.manual_seed(manualSeed);\n",
    "\n",
    "# This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "presidential-vertex",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:50.887987Z",
     "iopub.status.busy": "2021-04-10T18:04:50.887809Z",
     "iopub.status.idle": "2021-04-10T18:04:50.915784Z",
     "shell.execute_reply": "2021-04-10T18:04:50.915120Z",
     "shell.execute_reply.started": "2021-04-10T18:04:50.887967Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHKPTDIR = Path(\"TranslationFULLChkpts\")\n",
    "DATADIR = \"data/bms-molecular-translation\"\n",
    "LABELS_CSV_PATH = \"data/train_labels.csv\"\n",
    "VOCAB_FILEPATH = CHKPTDIR/\"vocab_dict.pt\"\n",
    "TRAINPATHS_PATH = CHKPTDIR/\"train_paths.feather\"\n",
    "TESTPATHS_PATH = CHKPTDIR/\"test_paths.feather\"\n",
    "CHKPTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tb_logger = pl.loggers.TensorBoardLogger(CHKPTDIR, name=\"InchINet\")\n",
    "\n",
    "LAYERS_SEQ = ('main_layer', 'c_layer', 'h_layer', 'b_layer', 't_layer', 'm_layer', 's_layer', 'i_layer')\n",
    "NULL_TOKEN = \"99999\"\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 256\n",
    "PRECISION = 16\n",
    "EMB_SIZE = 512\n",
    "INP_SIZE = (128, 128)\n",
    "LR = 1e-2\n",
    "EPOCHS = 2\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "computational-female",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:50.918008Z",
     "iopub.status.busy": "2021-04-10T18:04:50.917745Z",
     "iopub.status.idle": "2021-04-10T18:04:51.079026Z",
     "shell.execute_reply": "2021-04-10T18:04:51.078330Z",
     "shell.execute_reply.started": "2021-04-10T18:04:50.917979Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test  train  train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATADIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-drawing",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-directory",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vocab and Tokenizer\n",
    "\n",
    "#### *Fix this at end:* in some samples /t and /m layers are repeated after /i layer. For now, I mixed the repeated part to the first one. Either separate them in post processing or add t2 and m2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behavioral-bottle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:51.080617Z",
     "iopub.status.busy": "2021-04-10T18:04:51.080416Z",
     "iopub.status.idle": "2021-04-10T18:04:51.116083Z",
     "shell.execute_reply": "2021-04-10T18:04:51.115620Z",
     "shell.execute_reply.started": "2021-04-10T18:04:51.080590Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dissect_inchi(inchi_string:str) -> Union[dict, str]:\n",
    "    if len(inchi_string) > 0:\n",
    "        layers_dict = {} \n",
    "        inchi_string = inchi_string.split(\"/\")\n",
    "        layers_counter = len(inchi_string) - 1 # layers_counter is for sanity check, -1 for InChI=1S\n",
    "        # Get InChI standard format and main layer from string\n",
    "        assert inchi_string[0] == 'InChI=1S', \"Error in `dissect_inchi` function, string must start with `InChI=1S`\"\n",
    "        layers_dict[\"main_layer\"] = inchi_string[1]\n",
    "        del inchi_string[0:2]\n",
    "\n",
    "        for layer in inchi_string:\n",
    "            if len(layer) == 0:\n",
    "                layers_counter -= 1\n",
    "            elif len(layer) == 1:\n",
    "                lyr_name = f\"{layer}_layer\"\n",
    "                if lyr_name in layers_dict.keys():\n",
    "                    layers_dict[lyr_name] = layers_dict[lyr_name] + \",99999\"\n",
    "                    layers_counter -= 1\n",
    "                else:\n",
    "                    layers_dict[lyr_name] = \"99999\"\n",
    "            else:\n",
    "                lyr_name = f\"{layer[0]}_layer\"\n",
    "                if lyr_name in layers_dict.keys():\n",
    "                    layers_dict[lyr_name] = layers_dict[lyr_name] + \",\" + layer[1:]\n",
    "                    layers_counter -= 1\n",
    "                else:\n",
    "                    layers_dict[lyr_name] = layer[1:]\n",
    "\n",
    "        assert layers_counter == len(layers_dict.keys()), \\\n",
    "        f\"\"\"Error in `dissect_inchi` function. String is not fully analysed.\n",
    "        Expected {layers_counter} layers but got {len(layers_dict.keys())} layers.\n",
    "        {inchi_string}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add null token to empty layers for further convenience\n",
    "        seq_dict = {}\n",
    "        dissected_keys = layers_dict.keys()\n",
    "        for layer_name in LAYERS_SEQ:\n",
    "            if layer_name in dissected_keys:\n",
    "                seq_dict[layer_name] = layers_dict[layer_name]\n",
    "            else:\n",
    "                seq_dict[layer_name] = '99999'\n",
    "                \n",
    "        return seq_dict\n",
    "    return ''\n",
    "    \n",
    "# idx = 1\n",
    "# print(dm.df.InChI[1][:-2])\n",
    "# dissect_inchi(dm.df.InChI[1][:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "perceived-overhead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:51.117022Z",
     "iopub.status.busy": "2021-04-10T18:04:51.116858Z",
     "iopub.status.idle": "2021-04-10T18:04:51.166239Z",
     "shell.execute_reply": "2021-04-10T18:04:51.165741Z",
     "shell.execute_reply.started": "2021-04-10T18:04:51.117004Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab_dict:dict[list], max_lengths:dict[int], patterns_dict:dict[str], add_special_tokens:bool=True) -> None:\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.max_lengths = max_lengths\n",
    "        self.patterns_dict = patterns_dict\n",
    "        if add_special_tokens:\n",
    "            # correct max lengths if special tokens are added\n",
    "            self.max_lengths = {k: v + 2 for k, v in self.max_lengths.items()}\n",
    "            self.pad_token, self.unk_token, self.bos_token, self.eos_token = \"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"\n",
    "            self.null_token = \"99999\" # for empty sublayers i.e. layers having prefixes only. e.g. /t, /i\n",
    "            self.ctoi_dict = {}\n",
    "            for k, v in self.vocab_dict.items():\n",
    "                if self.pad_token in self.vocab_dict[k]:\n",
    "                    self.vocab_dict[k] = v\n",
    "                else:\n",
    "                    self.vocab_dict[k] = [self.pad_token, self.null_token, self.unk_token,self.bos_token,self.eos_token] + v\n",
    "                self.ctoi_dict[k] = defaultdict(self.handle_unk_char, {c:i for i, c in enumerate(self.vocab_dict[k])})\n",
    "    \n",
    "    def handle_unk_char(self):\n",
    "        return self.vocab_dict[\"main_layer\"].index(self.unk_token)\n",
    "        \n",
    "    def main_layer_tokenizer(self, string:str) -> list[str]:\n",
    "        tokens = re.split(self.patterns_dict[\"main_layer\"], string)\n",
    "        tokens = list(filter(None, tokens))\n",
    "        return tokens\n",
    "    \n",
    "    def slash_layer_tokenizer(self, string:str) -> list[str]:\n",
    "        tokens = re.split(self.patterns_dict[\"slash_layer\"], string)\n",
    "        tokens = list(filter(None, tokens))\n",
    "        return tokens\n",
    "    \n",
    "    def ctoi(self, c:str, layer_name:str) -> int:\n",
    "        return self.ctoi_dict[layer_name][c]\n",
    "    \n",
    "    def itoc(self, i:int, layer_name:str) -> str:\n",
    "        return self.vocab_dict[layer_name][i]\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> dict:\n",
    "        return {k:len(v) for k,v in self.vocab_dict.items()}\n",
    "    \n",
    "    def save_vocab(self, path:str) -> None:\n",
    "        torch.save({\"vocab_dict\": self.vocab_dict, \"max_lengths\": self.max_lengths, \"patterns_dict\": self.patterns_dict}, path)\n",
    "        print(\"Saved @\", path)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_file(cls, path:str, add_special_tokens:bool=True) -> object:\n",
    "        vocab = torch.load(path)\n",
    "        vocab_dict, max_lengths, patterns_dict = vocab[\"vocab_dict\"], vocab[\"max_lengths\"], vocab[\"patterns_dict\"]\n",
    "        return cls(vocab_dict, max_lengths, patterns_dict, add_special_tokens)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe_column(cls, inchi_column:pd.Series, add_special_tokens:bool=True, verbose=True) -> object:\n",
    "        # Create corpus from inchi strings\n",
    "        corpus_dict = Vocab.get_inchi_corpus(inchi_column)\n",
    "        return cls.from_corpus(corpus_dict, add_special_tokens, verbose)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus_dict:dict[str], add_special_tokens:bool=True, verbose=True) -> object:\n",
    "        names_list = list(corpus_dict.keys())\n",
    "        names_list.remove(\"main_layer\")\n",
    "        vocab_dict, max_lengths = {}, {}\n",
    "        # Create vocab for main layer\n",
    "        if verbose: print(f\"Creating main layer vocab...\", end=' ')\n",
    "        vocab_dict[\"main_layer\"], max_len, main_layer_pattern = Vocab.create_main_layer_vocab(corpus_dict[\"main_layer\"], True)\n",
    "        max_lengths[\"main_layer\"] = max_len\n",
    "        if verbose: print(\"done!\")\n",
    "            \n",
    "        # Create vocabs for rest of the inchi string layers\n",
    "        for layer_name in names_list:\n",
    "            if verbose: print(f\"Creating {layer_name} vocab...\", end=' ')\n",
    "            vocab_dict[layer_name], max_len, slash_layer_pattern = Vocab.create_slash_layer_vocab(corpus_dict[layer_name], True)\n",
    "            max_lengths[layer_name] = max_len\n",
    "            if verbose: print(\"done!\")\n",
    "        \n",
    "        # Patterns\n",
    "        patterns_dict = {\"main_layer\": main_layer_pattern, \"slash_layer\": slash_layer_pattern}\n",
    "    \n",
    "        return cls(vocab_dict, max_lengths, patterns_dict, add_special_tokens)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_inchi_corpus(inchi_column:pd.Series) -> dict[str]:\n",
    "        corpus, counter = {}, 0\n",
    "        for string in progress_bar(inchi_column):\n",
    "            try:\n",
    "                dissected = dissect_inchi(string)\n",
    "                for k, v in dissected.items():\n",
    "                    if k not in corpus:\n",
    "                        corpus[k] = [v]\n",
    "                    else:\n",
    "                        corpus[k].append(v)\n",
    "            except AssertionError:\n",
    "                counter += 1\n",
    "        if counter > 0:\n",
    "            print(f\"[Warning] Found {counter} assertions in data while creating layer-wise corpus.\")\n",
    "        return corpus\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_main_layer_vocab(main_layer_corpus:list[str], return_pattern:bool=False) -> Union[Tuple[list, int], Tuple[list, int, str]]:\n",
    "        def tokenize(string):\n",
    "            tokens = re.split(pattern, string)\n",
    "            return list(filter(None, tokens))\n",
    "\n",
    "        data = elements.Elements\n",
    "        # All elements in periodic table\n",
    "        elems = sorted(data, key=lambda i:i.AtomicNumber)  # Based on their AtomicNumber\n",
    "        # Sort longer names first for regex pattern formation\n",
    "        element_symbols = sorted([e.Symbol for e in elems], key=lambda e: len(e), reverse=True)\n",
    "        # Create regex pattern\n",
    "        pattern = f\"({'|'.join([f'{e}[0-9]*' for e in element_symbols])})\"\n",
    "        vocab = [tokenize(string) for string in main_layer_corpus]\n",
    "        max_length = max(map(len, vocab))\n",
    "        vocab = np.unique([w for string in vocab for w in string]).tolist()\n",
    "        if return_pattern:\n",
    "            return vocab, max_length, pattern\n",
    "        return vocab, max_length\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_slash_layer_vocab(layer_corpus:list[str], return_pattern:bool=False) -> Union[Tuple[list, int], Tuple[list, int, str]]:\n",
    "        def tokenize(string):\n",
    "            tokens = re.split(pattern, string)\n",
    "            return list(filter(None, tokens))\n",
    "            \n",
    "        pattern = r\"([\\d]*)\"\n",
    "        vocab = [tokenize(string) for string in layer_corpus]\n",
    "        max_length = max(map(len, vocab))\n",
    "        vocab = np.unique([w for string in vocab for w in string]).tolist()\n",
    "        if return_pattern:\n",
    "            return vocab, max_length, pattern\n",
    "        return vocab, max_length\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab:Vocab=None) -> None:\n",
    "        self.vocab = vocab # Vocab class instance\n",
    "    \n",
    "    def tokenize(self, x:Union[str, dict], layer_name:str=None) -> Union[list, dict[list]]:\n",
    "        if isinstance(x, str):\n",
    "            if layer_name == None:\n",
    "                raise AttributeError(\"`layer_name` is required when `x` is a string\")\n",
    "            return eval(f\"self.vocab.{layer_name}_tokenizer\")(x)\n",
    "        elif isinstance(x, dict):\n",
    "            for k,v in x.items():\n",
    "                if k == \"main_layer\": \n",
    "                    x[k] = self.vocab.main_layer_tokenizer(v)\n",
    "                else:\n",
    "                    x[k] = self.vocab.slash_layer_tokenizer(v)\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"`x` must be either string or dict(layer_name=string)\")\n",
    "    \n",
    "    def encode(self, inchi_string:str, pad_to_max_len:bool=True) -> dict[dict[list]]:\n",
    "        dissected = dissect_inchi(inchi_string)\n",
    "        tokens_dict = self.tokenize(dissected)\n",
    "        if hasattr(self.vocab, \"bos_token\"): # Add start and end special tokens\n",
    "            tokens_dict = {k: [self.vocab.bos_token] + v + [self.vocab.eos_token] for k, v in tokens_dict.items()}\n",
    "        inp_seq_dict = {k: [self.vocab.ctoi(c, k) for c in v] for k, v in tokens_dict.items()}\n",
    "        attn_mask_dict = {k: [1] * len(v) for k, v in inp_seq_dict.items()}\n",
    "\n",
    "        # Add padding for max lengths\n",
    "        if pad_to_max_len:\n",
    "            for k, v in inp_seq_dict.items():\n",
    "                extra_len = self.vocab.max_lengths[k] - len(inp_seq_dict[k])\n",
    "                inp_seq_dict[k] = v + [self.vocab.ctoi(self.vocab.pad_token, k)] * extra_len\n",
    "                attn_mask_dict[k] = attn_mask_dict[k] + [0] * extra_len\n",
    "        return {\"inp_seq_dict\": inp_seq_dict, \"attn_mask_dict\": attn_mask_dict}\n",
    "    \n",
    "    def decode(self, inp_seq_dict:dict[list]) -> str:\n",
    "        decoded_string, inp_keys = 'InChI=1S/', inp_seq_dict.keys()\n",
    "        for k in LAYERS_SEQ:\n",
    "            if k in inp_keys:\n",
    "                substring = [self.vocab.itoc(t, k) for t in inp_seq_dict[k]]\n",
    "                _from = substring.index(self.vocab.bos_token) + 1 # 1 is for excluding start token from string\n",
    "                _to = substring.index(self.vocab.eos_token)\n",
    "                substring = ''.join(substring[_from:_to])\n",
    "                if k != 'main_layer':\n",
    "                    decoded_string += f\"/{k[0]}{substring}\"\n",
    "                else:\n",
    "                    decoded_string += substring\n",
    "        return decoded_string\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, path:str, add_special_tokens:bool=True) -> object:\n",
    "        vocab = torch.load(path)\n",
    "        vocab_dict, max_lengths, patterns_dict = vocab[\"vocab_dict\"], vocab[\"max_lengths\"], vocab[\"patterns_dict\"]\n",
    "        vocab_obj = Vocab(vocab_dict, max_lengths, patterns_dict, add_special_tokens)\n",
    "        return cls(vocab_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "becoming-water",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:51.167118Z",
     "iopub.status.busy": "2021-04-10T18:04:51.166935Z",
     "iopub.status.idle": "2021-04-10T18:04:51.193077Z",
     "shell.execute_reply": "2021-04-10T18:04:51.192473Z",
     "shell.execute_reply.started": "2021-04-10T18:04:51.167100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vocab_unit_tests(vocab:Vocab, corpus_dict:dict[list], \n",
    "                     max_lengths:dict[int], add_special_tokens:bool=True) -> str:\n",
    "    max_lengths = {k: v - 2 for k, v in max_lengths.items()}\n",
    "    print(\"Checking main string length...\", end=' ')\n",
    "    main_flag = False\n",
    "    for string in progress_bar(corpus_dict['main_layer']):\n",
    "        if len(vocab.slash_layer_tokenizer(string)) == max_lengths[\"main_layer\"]:\n",
    "            main_flag = True\n",
    "    assert main_flag == True, \"Main string test failed :(\"\n",
    "    print(\"done!\")\n",
    "    del corpus_dict[\"main_layer\"]\n",
    "    for layer_name in corpus_dict.keys():\n",
    "        flag = False\n",
    "        print(f\"Checking {layer_name} string length...\", end=' ')\n",
    "        for string in progress_bar(corpus_dict[layer_name]):\n",
    "            if len(vocab.slash_layer_tokenizer(string)) == max_lengths[layer_name]:\n",
    "                flag = True\n",
    "        assert flag == True, f\"{layer_name} string test failed :(\"\n",
    "        print(\"done!\")\n",
    "    return \"ALL GOOD!\"\n",
    "\n",
    "def tokenizer_unit_test(tokenizer:Tokenizer, df:pd.DataFrame, sample_size:int=10000) -> str:\n",
    "    print(\"Running tokenizer test...\")\n",
    "    for idx in progress_bar(np.random.randint(0, len(df), size=sample_size)):\n",
    "        inp_string = df.InChI[idx]\n",
    "        inp_seq_dict = tokenizer.encode(inp_string)['inp_seq_dict']\n",
    "        out_string = tokenizer.decode(inp_seq_dict)\n",
    "        if Levenshtein.distance(inp_string, out_string) != 0:\n",
    "            assert IOError(\"Input InChI string is equal to output InChI string.\")\n",
    "    return \"ALL GOOD!\"\n",
    "\n",
    "# Unit Testing\n",
    "# test_corpus_dict = Vocab.get_inchi_corpus(df.InChI)\n",
    "# test_vocab = Vocab.from_corpus(test_corpus_dict)\n",
    "# test_tokenizer = Tokenizer(test_vocab)\n",
    "# tokenizer_unit_test(test_tokenizer, df)\n",
    "# vocab_unit_tests(test_vocab, test_corpus_dict, test_vocab.max_lengths, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-wayne",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "applicable-grass",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lightning Data Module\n",
    "### LightningDataModule API\n",
    "\n",
    "To define a DataModule define 5 methods:\n",
    "1. prepare_data (how to download(), tokenize, etc…)\n",
    "2. setup (how to split, etc…)\n",
    "3. train_dataloader\n",
    "4. val_dataloader(s)\n",
    "5. test_dataloader(s)\n",
    "\n",
    "#### prepare_data\n",
    "Use this method to do things that might write to disk or that need to be done only from a single process in distributed settings.\n",
    "1. download\n",
    "2. tokenize\n",
    "3. etc…\n",
    "\n",
    "#### setup\n",
    "There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "1. count number of classes\n",
    "2. build vocabulary\n",
    "3. perform train/val/test splits\n",
    "4. apply transforms (defined explicitly in your datamodule or assigned in init)\n",
    "5. etc…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "handy-might",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:51.194018Z",
     "iopub.status.busy": "2021-04-10T18:04:51.193851Z",
     "iopub.status.idle": "2021-04-10T18:04:51.233761Z",
     "shell.execute_reply": "2021-04-10T18:04:51.233288Z",
     "shell.execute_reply.started": "2021-04-10T18:04:51.194000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImgtoInChIDataset(Dataset):\n",
    "    def __init__(self, paths:list, df:pd.DataFrame=None, tsfms:A.Compose=None) -> None:\n",
    "        self.paths = paths\n",
    "        if df is not None:\n",
    "            self.idtoinchi_dict = {\n",
    "                _id:_inchi for _id, _inchi in\n",
    "                zip(df[\"image_id\"].values.tolist(), df[\"InChI\"].values.tolist())\n",
    "            }\n",
    "        self.tsfms = tsfms\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, str]:\n",
    "        imgpath = self.paths[idx]\n",
    "        imgid = Path(imgpath).stem\n",
    "        img = np.array(preprocess_image(imgpath, out_size=INP_SIZE), dtype=np.float32)/255.\n",
    "        if self.tsfms is not None:\n",
    "            img = self.tsfms(image=img)[\"image\"]\n",
    "        \n",
    "        if hasattr(self, \"idtoinchi_dict\"):\n",
    "            target = self.idtoinchi_dict[imgid]\n",
    "#             target = target.split(\"/\")[1]\n",
    "            return img, target\n",
    "\n",
    "        return img, \"test_placeholder\"\n",
    "    \n",
    "class ImgToInChIDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tb_logger, valset_ratio=0.05) -> None:\n",
    "        super().__init__()\n",
    "        self.tb_logger = tb_logger\n",
    "        self.valset_ratio = valset_ratio\n",
    "        self.dims = (1, *INP_SIZE)\n",
    "        \n",
    "        self.train_tsfms = A.Compose([\n",
    "#             A.Resize(*INP_SIZE, always_apply=True),\n",
    "#             A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "#             A.RandomCrop(*INP_SIZE),\n",
    "#             A.RandomBrightnessContrast(p=0.5),\n",
    "#             A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.test_tsfms = A.Compose([\n",
    "#             A.Resize(*INP_SIZE, always_apply=True),\n",
    "#             A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self, verbose:bool=False) -> None:\n",
    "        \"\"\"Use this method to do things that might write to disk or that\n",
    "        need to be done only from a single process in distributed settings.\"\"\"\n",
    "        # Load labels in DataFrame\n",
    "        if verbose: print(\"Loading labels data...\", end=' ')\n",
    "        self.df = pd.read_csv(LABELS_CSV_PATH)\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Load image paths\n",
    "        if verbose: print(\"Loading paths...\", end=' ')\n",
    "        if TRAINPATHS_PATH.exists():\n",
    "            self.train_paths = pd.read_feather(TRAINPATHS_PATH)\n",
    "            self.train_paths = self.train_paths.train_paths.tolist()\n",
    "        else:\n",
    "            self.train_paths = pd.DataFrame(list((Path(DATADIR)/\"train\").rglob(\"*.*\")), columns=[\"train_paths\"])\n",
    "            self.train_paths = self.train_paths.applymap(lambda x: str(x))\n",
    "            self.train_paths.to_feather(TRAINPATHS_PATH)\n",
    "            self.train_paths = self.train_paths.train_paths.tolist()\n",
    "        if TESTPATHS_PATH.exists():\n",
    "            self.test_paths = pd.read_feather(TESTPATHS_PATH)\n",
    "            self.test_paths = self.test_paths.test_paths.tolist()\n",
    "        else:\n",
    "            self.test_paths = pd.DataFrame(list((Path(DATADIR)/\"test\").rglob(\"*.*\")), columns=[\"test_paths\"])\n",
    "            self.test_paths = self.test_paths.applymap(lambda x: str(x))\n",
    "            self.test_paths.to_feather(TESTPATHS_PATH)\n",
    "            self.test_paths = self.test_paths.test_paths.tolist()\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Get Vocab and Tokenizer\n",
    "        if verbose: print(\"Loading vocab and tokenizer...\", end=' ')\n",
    "        \n",
    "        \n",
    "        if VOCAB_FILEPATH.exists():\n",
    "            self.tokenizer = Tokenizer.from_file(VOCAB_FILEPATH)\n",
    "        else:\n",
    "            self.vocab = Vocab.from_dataframe_column(self.df.InChI)\n",
    "            self.tokenizer = Tokenizer(self.vocab)\n",
    "            self.tokenizer.vocab.save_vocab(VOCAB_FILEPATH)\n",
    "            \n",
    "        self.vocab_size = self.tokenizer.vocab.vocab_size\n",
    "        if verbose: print(\"DONE!\")\n",
    "                \n",
    "    def setup(self, stage:Optional[str]=None) -> None:\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            trainpaths, valpaths = train_test_split(self.train_paths, test_size=self.valset_ratio)\n",
    "            self.trainset = ImgtoInChIDataset(trainpaths, self.df, self.train_tsfms)\n",
    "            self.valset = ImgtoInChIDataset(valpaths, self.df, self.test_tsfms)\n",
    "            \n",
    "            # Sample batch\n",
    "            imgs, inp_seqs, attn_masks = next(iter(self.train_dataloader()))\n",
    "            self.tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.testset = ImgtoInChIDataset(self.test_paths, tsfms=self.test_tsfms)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.trainset, BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valset, BATCH_SIZE, shuffle=False, \n",
    "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
    "    \n",
    "    def collate_fn(self, batch:tuple) -> Tuple[torch.Tensor, dict[list], dict[list]]:\n",
    "        imgs = torch.cat([ins[0].unsqueeze(0) for ins in batch])\n",
    "        targets = [ins[1] for ins in batch]\n",
    "        targets = [self.tokenizer.encode(t) for t in targets]\n",
    "        \n",
    "        batch_inp_seqs = {k: [] for k in LAYERS_SEQ}\n",
    "        for sample in [t[\"inp_seq_dict\"] for t in targets]:\n",
    "            for layer_name in LAYERS_SEQ:\n",
    "                batch_inp_seqs[layer_name].append(sample[layer_name])\n",
    "        \n",
    "        batch_attn_masks = {k: [] for k in LAYERS_SEQ}\n",
    "        for sample in [t[\"attn_mask_dict\"] for t in targets]:\n",
    "            for layer_name in LAYERS_SEQ:\n",
    "                batch_attn_masks[layer_name].append(sample[layer_name])\n",
    "        return imgs, batch_inp_seqs, batch_attn_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sweet-stranger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T18:04:51.234741Z",
     "iopub.status.busy": "2021-04-10T18:04:51.234538Z",
     "iopub.status.idle": "2021-04-10T18:04:51.258518Z",
     "shell.execute_reply": "2021-04-10T18:04:51.257679Z",
     "shell.execute_reply.started": "2021-04-10T18:04:51.234700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imgs, inp_seqs, attn_masks = next(iter(trainloader))\n",
    "# tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
    "\n",
    "\n",
    "# print(\"SAMPLE BATCH =\", imgs.shape)\n",
    "# fig, axes = plt.subplots(4, 8, figsize=(18, 10))\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(imgs[i].squeeze(0), cmap=\"gray\")\n",
    "#     ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-chassis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T07:15:59.920534Z",
     "iopub.status.busy": "2021-04-02T07:15:59.920240Z",
     "iopub.status.idle": "2021-04-02T07:15:59.923276Z",
     "shell.execute_reply": "2021-04-02T07:15:59.922767Z",
     "shell.execute_reply.started": "2021-04-02T07:15:59.920510Z"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "### At the time of sentence prediction can we use HMMs or [Beam Search](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#overview) to make better decisions?\n",
    "\n",
    "Use different LSTM layers for each inchi substring like /c /h\n",
    "```\n",
    ">>> rnn = nn.LSTM(10, 20, 2)\n",
    ">>> input = torch.randn(1, 16, 10)\n",
    ">>> h0 = torch.randn(2, 16, 20)\n",
    ">>> c0 = torch.randn(2, 16, 20)\n",
    "```\n",
    "change number of layers here to num sublayers\n",
    "\n",
    "\n",
    "[Model from here](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-progressive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T09:09:45.905230Z",
     "iopub.status.busy": "2021-04-08T09:09:45.905073Z",
     "iopub.status.idle": "2021-04-08T09:09:45.952877Z",
     "shell.execute_reply": "2021-04-08T09:09:45.952451Z",
     "shell.execute_reply.started": "2021-04-08T09:09:45.905211Z"
    },
    "tags": []
   },
   "source": [
    "Network commented in this cell below DID NOT WORKED WELL. FALLING BACK...\n",
    "<!-- class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False, out_channels=512):\n",
    "        super().__init__()\n",
    "        last_stride = 2 if INP_SIZE[0] == 256 else 1\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.out_channels = out_channels\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.resout = nn.Sequential(\n",
    "            nn.Conv2d(512, out_channels, kernel_size=3, stride=last_stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.encout = nn.Sequential(\n",
    "            nn.Conv1d(16, 1, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.resout(out)\n",
    "        out = out.view(x.size(0), self.out_channels, -1)\n",
    "        out = out.permute(0,2,1)\n",
    "        out = self.encout(out).squeeze(1)\n",
    "#         print(\"ENC out =\", out.shape)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, enc_out_channels=512):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(vocab_size, EMB_SIZE)\n",
    "        self.lstm = nn.LSTM(enc_out_channels, vocab_size, batch_first=True)\n",
    "        self.encfc = nn.Linear(enc_out_channels, vocab_size)\n",
    "        self.embfc = nn.Linear(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, encoder_out, inp_seqs):\n",
    "        encoder_out = self.encfc(encoder_out)\n",
    "        encoder_out = encoder_out.unsqueeze(1)\n",
    "#         print(\"encoder_out =\", encoder_out.shape)\n",
    "        \n",
    "        emb = self.embd(inp_seqs)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "#         lstm_out = lstm_out.reshape(encoder_out.size(0), -1)\n",
    "#         print(\"lstm_out =\", lstm_out.shape)\n",
    "        enc_out = torch.repeat_interleave(encoder_out, lstm_out.size(1), dim=1)\n",
    "#         print(\"enc_out =\", enc_out.shape)\n",
    "        out = lstm_out + enc_out\n",
    "#         for i in range(lstm_out.size(1)):\n",
    "#             lstm_out[:,i,:] = self.embfc(lstm_out[:,i,:])\n",
    "#             lstm_out[:,i,:] = lstm_out[:,i,:] + encoder_out\n",
    "#         print(\"lstm_out after for =\", lstm_out.shape)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, encoder_out, tokenizer):\n",
    "#         print(encoder_out.shape)\n",
    "        encoder_out = self.encfc(encoder_out)\n",
    "        bs = encoder_out.size(0)\n",
    "        syn_inp_seqs = torch.tensor(tokenizer.vocab.ctoi(tokenizer.vocab.bos_token), device=encoder_out.device)\n",
    "        syn_inp_seqs = torch.repeat_interleave(syn_inp_seqs, bs)\n",
    "        syn_inp_seqs = syn_inp_seqs.view(bs, 1)\n",
    "        \n",
    "        # Predict next tokens to start token\n",
    "        pred_emb = []\n",
    "        for i in range(MAX_LEN):\n",
    "            emb = self.embd(syn_inp_seqs)\n",
    "            pred, _ = self.lstm(emb)\n",
    "            pred = pred.squeeze(1)\n",
    "            \n",
    "#             pred = self.embfc(pred)\n",
    "            pred = pred + encoder_out\n",
    "            pred = pred.unsqueeze(1)\n",
    "            pred_emb.append(pred)\n",
    "            syn_inp_seqs = pred.argmax(dim=-1)\n",
    "#         print(\"len =\", len(pred_emb))\n",
    "        pred_emb = torch.cat(pred_emb, dim=1)\n",
    "        return pred_emb\n",
    "            \n",
    "class InChINet(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_net = Encoder()\n",
    "        self.decoder_net = Decoder(vocab_size)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, imgs, inp_seqs):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "#         print(\"Encoder =\", encoder_out.shape)\n",
    "        pred_tokens = self.decoder_net(encoder_out, inp_seqs)\n",
    "        return pred_tokens\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = train_batch\n",
    "        output = self.forward(imgs, inp_seqs)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = val_batch\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        \n",
    "        lv_metric = self.calculate_lvdistance(output, inp_seqs)\n",
    "        self.logger.log_metrics({\"LvDistance\": lv_metric}, step=1)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, imgs, tokenizer):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "        pred_tokens = self.decoder_net.predict(encoder_out, tokenizer)\n",
    "        return pred_tokens\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer, LR, epochs=EPOCHS, steps_per_epoch=8996),\n",
    "            'name': 'OneCycleLR'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def inference(self, imgs):\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        return self.postprocessing(output)\n",
    "    \n",
    "    def calculate_lvdistance(self, output, target):\n",
    "        pred_seqs = self.postprocessing(output)\n",
    "        batch_distance = np.mean([\n",
    "            Levenshtein.distance(pred_seq, self.tokenizer.decode(inp_seq))\n",
    "            for pred_seq, inp_seq in zip(pred_seqs, target)\n",
    "        ])\n",
    "        return batch_distance\n",
    "    \n",
    "    def postprocessing(self, output):\n",
    "        final_preds = []\n",
    "        pred_tokens = output.argmax(dim=-1)\n",
    "        for i in range(pred_tokens.size(0)): # iterate on each sample\n",
    "            pred = pred_tokens[i].unique(dim=-1).tolist()\n",
    "            pred = self.tokenizer.decode(pred)\n",
    "            res = re.search(r'C', pred)\n",
    "            if res:\n",
    "                pred = pred[res.span()[0]:]\n",
    "            final_preds.append(pred)\n",
    "        return final_preds\n",
    "    \n",
    "\n",
    "# dm = ImgToInChIDataModule(tb_logger=tb_logger)\n",
    "# dm.prepare_data(verbose=True)\n",
    "# dm.setup('fit')\n",
    "# imgs, inp_seqs, attn_masks = next(iter(dm.train_dataloader()))\n",
    "\n",
    "# encoder_net = Encoder()\n",
    "# encoder_out = encoder_net(imgs)\n",
    "# print(\"Encoder =\", encoder_out.shape)\n",
    "\n",
    "# decoder_net = Decoder(dm.vocab_size)\n",
    "# pred_tokens = decoder_net(encoder_out, inp_seqs)\n",
    "# print(pred_tokens.shape)\n",
    "\n",
    "# pred_tokens = decoder_net.predict(encoder_out, dm.tokenizer)\n",
    "# pred_tokens.shape -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "controlling-avenue",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T20:11:56.729787Z",
     "iopub.status.busy": "2021-04-10T20:11:56.729445Z",
     "iopub.status.idle": "2021-04-10T20:11:56.973282Z",
     "shell.execute_reply": "2021-04-10T20:11:56.972350Z",
     "shell.execute_reply.started": "2021-04-10T20:11:56.729747Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder = torch.Size([256, 16, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (427) must match the size of tensor b (512) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-004b3c3ef970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mdecoder_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"main_layer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"main_layer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mpred_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_seqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"main_layer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-004b3c3ef970>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_out, inp_seqs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_out\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (427) must match the size of tensor b (512) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False, out_channels=512):\n",
    "        super().__init__()\n",
    "        last_stride = 2 if INP_SIZE[0] == 256 else 1\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.out_channels = out_channels\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.outfc = nn.Sequential(\n",
    "            nn.Conv2d(512, out_channels, kernel_size=3, stride=last_stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.outfc(out)\n",
    "        out = out.view(x.size(0), self.out_channels, -1)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, enc_out_channels=512):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(vocab_size, EMB_SIZE)\n",
    "        self.lstm = nn.LSTM(enc_out_channels, vocab_size, batch_first=True)\n",
    "        self.decfc = nn.Linear(64, max_len)\n",
    "    \n",
    "    def forward(self, encoder_out, inp_seqs):\n",
    "        inp_seqs = torch.tensor(inp_seqs, device=encoder_out.device)\n",
    "        emb = self.embd(inp_seqs)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        out = lstm_out + encoder_out\n",
    "        return out\n",
    "    \n",
    "    def predict(self, encoder_out, tokenizer):\n",
    "        bs = encoder_out.size(0)\n",
    "        syn_inp_seqs = torch.tensor(tokenizer.vocab.ctoi(tokenizer.vocab.bos_token), device=encoder_out.device)\n",
    "        syn_inp_seqs = torch.repeat_interleave(syn_inp_seqs, bs)\n",
    "        syn_inp_seqs = syn_inp_seqs.view(bs, 1)\n",
    "        \n",
    "        # Predict next tokens to start token\n",
    "        pred_emb = []\n",
    "        for i in range(MAX_LEN):\n",
    "            emb = self.embd(syn_inp_seqs)\n",
    "            pred, _ = self.lstm(emb)\n",
    "            pred_emb.append(pred)\n",
    "            syn_inp_seqs = pred.argmax(dim=-1)\n",
    "        pred_emb = torch.cat(pred_emb, dim=1)\n",
    "        out = pred_emb + encoder_out\n",
    "        return out\n",
    "            \n",
    "class InChINet(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_net = Encoder(out_channels=vocab_size)\n",
    "        self.decoder_net = Decoder(vocab_size)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, imgs, inp_seqs):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "#         print(\"Encoder =\", encoder_out.shape)\n",
    "        pred_tokens = self.decoder_net(encoder_out, inp_seqs)\n",
    "        return pred_tokens\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = train_batch\n",
    "        output = self.forward(imgs, inp_seqs)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = val_batch\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        \n",
    "        lv_metric = self.calculate_lvdistance(output, inp_seqs)\n",
    "        self.logger.log_metrics({\"LvDistance\": lv_metric}, step=1)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, imgs, tokenizer):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "        pred_tokens = self.decoder_net.predict(encoder_out, tokenizer)\n",
    "        return pred_tokens\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100),\n",
    "            'name': 'AnnealingLR'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def inference(self, imgs):\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        return self.postprocessing(output)\n",
    "    \n",
    "    def calculate_lvdistance(self, output, target):\n",
    "        pred_seqs = self.postprocessing(output)\n",
    "        batch_distance = np.mean([\n",
    "            Levenshtein.distance(pred_seq, self.tokenizer.decode(inp_seq))\n",
    "            for pred_seq, inp_seq in zip(pred_seqs, target)\n",
    "        ])\n",
    "        return batch_distance\n",
    "    \n",
    "    def postprocessing(self, output):\n",
    "        final_preds = []\n",
    "        pred_tokens = output.argmax(dim=-1)\n",
    "        for i in range(pred_tokens.size(0)): # iterate on each sample\n",
    "            pred = pred_tokens[i].unique(dim=-1).tolist()\n",
    "            pred = self.tokenizer.decode(pred)\n",
    "            res = re.search(r'C', pred)\n",
    "            if res:\n",
    "                pred = pred[res.span()[0]:]\n",
    "            final_preds.append(pred)\n",
    "        return final_preds\n",
    "    \n",
    "    \n",
    "# dm = ImgToInChIDataModule(tb_logger=tb_logger)\n",
    "# dm.prepare_data(verbose=True)\n",
    "# dm.setup('fit')\n",
    "# imgs, inp_seqs, attn_masks = next(iter(dm.train_dataloader()))\n",
    "\n",
    "# encoder_net = Encoder()\n",
    "# encoder_out = encoder_net(imgs)\n",
    "print(\"Encoder =\", encoder_out.shape)\n",
    "\n",
    "decoder_net = Decoder(dm.vocab_size[\"main_layer\"], dm.tokenizer.vocab.max_lengths[\"main_layer\"])\n",
    "pred_tokens = decoder_net(encoder_out, inp_seqs[\"main_layer\"])\n",
    "print(pred_tokens.shape)\n",
    "\n",
    "# pred_tokens = decoder_net.predict(encoder_out, tokenizer)\n",
    "# pred_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "revised-supplement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T20:02:08.096799Z",
     "iopub.status.busy": "2021-04-10T20:02:08.096287Z",
     "iopub.status.idle": "2021-04-10T20:02:08.139332Z",
     "shell.execute_reply": "2021-04-10T20:02:08.138683Z",
     "shell.execute_reply.started": "2021-04-10T20:02:08.096725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_layer': 13,\n",
       " 'c_layer': 187,\n",
       " 'h_layer': 112,\n",
       " 't_layer': 45,\n",
       " 'm_layer': 7,\n",
       " 's_layer': 5,\n",
       " 'b_layer': 68,\n",
       " 'i_layer': 51}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.tokenizer.vocab.max_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "super-bowling",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T20:02:16.011187Z",
     "iopub.status.busy": "2021-04-10T20:02:16.011003Z",
     "iopub.status.idle": "2021-04-10T20:02:16.036294Z",
     "shell.execute_reply": "2021-04-10T20:02:16.035624Z",
     "shell.execute_reply.started": "2021-04-10T20:02:16.011166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_layer': 427,\n",
       " 'c_layer': 101,\n",
       " 'h_layer': 101,\n",
       " 't_layer': 75,\n",
       " 'm_layer': 8,\n",
       " 's_layer': 6,\n",
       " 'b_layer': 81,\n",
       " 'i_layer': 55}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.tokenizer.vocab.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cubic-least",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T20:08:48.090917Z",
     "iopub.status.busy": "2021-04-10T20:08:48.090141Z",
     "iopub.status.idle": "2021-04-10T20:08:48.140771Z",
     "shell.execute_reply": "2021-04-10T20:08:48.140266Z",
     "shell.execute_reply.started": "2021-04-10T20:08:48.090823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-12-10-20(28)30)27-11-9-16-21(23(25)31)26-29(22(16)24(27)32)18-5-3-4-6-19(18)33-2/h3-8,13H,9-12H2,1-2H3,(H2,25,31)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.df.InChI[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-class",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-access",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-absence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "loving-industry",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-embassy",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-10T18:04:51.503059Z",
     "iopub.status.idle": "2021-04-10T18:04:51.503450Z",
     "shell.execute_reply": "2021-04-10T18:04:51.503255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir {CHKPTDIR}\n",
    "\n",
    "dm = ImgToInChIDataModule(tb_logger=tb_logger)\n",
    "dm.prepare_data(verbose=True)\n",
    "\n",
    "model = InChINet(dm.vocab_size, dm.tokenizer)\n",
    "# Add network graph to tensorboard\n",
    "# tb_logger.log_graph(model, [imgs[0].unsqueeze(0).to(model.device), inp_seqs[0].unsqueeze(0).to(model.device)])\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, auto_lr_find=True, max_epochs=1, precision=PRECISION, profiler=\"simple\", \n",
    "                     default_root_dir=CHKPTDIR, logger=tb_logger, callbacks=[lr_monitor])\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-occurrence",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-10T18:04:51.504685Z",
     "iopub.status.idle": "2021-04-10T18:04:51.505136Z",
     "shell.execute_reply": "2021-04-10T18:04:51.504899Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "total_distance = []\n",
    "for (imgs, inp_seqs, attn_masks) in progress_bar(dm.val_dataloader()):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    imgs = imgs.to(device)\n",
    "    pred_seqs = model.inference(imgs)\n",
    "#     print([(pred_seq, dm.tokenizer.decode(inp_seq)) for pred_seq, inp_seq in zip(pred_seqs, inp_seqs)])\n",
    "#     break\n",
    "    batch_distance = [\n",
    "        Levenshtein.distance(pred_seq, dm.tokenizer.decode(inp_seq))\n",
    "        for pred_seq, inp_seq in zip(pred_seqs, inp_seqs)\n",
    "    ]\n",
    "    total_distance += batch_distance\n",
    "np.mean(total_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-lingerie",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-10T18:04:51.506104Z",
     "iopub.status.idle": "2021-04-10T18:04:51.506678Z",
     "shell.execute_reply": "2021-04-10T18:04:51.506348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.3149080108901905"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt)",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
