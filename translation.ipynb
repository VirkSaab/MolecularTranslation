{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ethical-complement",
   "metadata": {},
   "source": [
    "## Try these two experiments first\n",
    "1. Create original input image to inchi string translator and get accuracy and loss\n",
    "2. Create inchi image to inchi string translator and get accuracy and loss\n",
    "\n",
    "Now, compare the two. If the accuracy of inchi image -> inchi string is significantly higher than the original image -> inchi string then think about ***reconstruction experiments*** from original image to inchi image. [try this then](https://www.google.com/search?channel=fs&client=ubuntu&q=converting+shapes+from+one+to+another+using+deep+learning).\n",
    "\n",
    "# InChI decoding \n",
    "[Source](https://link.springer.com/content/pdf/10.1186%2Fs13321-015-0068-4.pdf)\n",
    "\n",
    "1. **Skeletal connections layer** This layer prefixed with `/c` represents connections between skeletal atoms by listing the canonical numbers in the chain of connected atoms. \n",
    "2. ***branches are given in parentheses***\n",
    "3. The canonical atomic numbers, which are used throughout the InChI, are always given in the formula’s element order. i.e. precendence is given to element according to periodic table while numbering elements. For example, `/C10H16N5O13P3` (the beginning of InChI for adenosine triphosphate) implies that atoms numbered 1–10 are carbons, 11–15 arenitrogens, 16–28 are oxygens, and 29–31 are phosporus. Hydrogen atoms are not explicitly numbered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-kruger",
   "metadata": {
    "tags": []
   },
   "source": [
    "## image to inchi string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intermediate-riverside",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:30.490963Z",
     "iopub.status.busy": "2021-04-03T22:23:30.490793Z",
     "iopub.status.idle": "2021-04-03T22:23:30.493721Z",
     "shell.execute_reply": "2021-04-03T22:23:30.493024Z",
     "shell.execute_reply.started": "2021-04-03T22:23:30.490944Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Special Packages\n",
    "# !pip install PeriodicElements\n",
    "# !pip install albumentations\n",
    "# !pip install timm\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "boolean-convert",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:30.495030Z",
     "iopub.status.busy": "2021-04-03T22:23:30.494783Z",
     "iopub.status.idle": "2021-04-03T22:23:30.514154Z",
     "shell.execute_reply": "2021-04-03T22:23:30.513578Z",
     "shell.execute_reply.started": "2021-04-03T22:23:30.495000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "printable-tours",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:30.515178Z",
     "iopub.status.busy": "2021-04-03T22:23:30.515019Z",
     "iopub.status.idle": "2021-04-03T22:23:32.445582Z",
     "shell.execute_reply": "2021-04-03T22:23:32.444909Z",
     "shell.execute_reply.started": "2021-04-03T22:23:30.515159Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import torch, torchmetrics, timm, re, pickle, Levenshtein\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from fastprogress import progress_bar\n",
    "from typing import Optional, Union\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from elements import elements\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "torch.manual_seed(manualSeed);\n",
    "\n",
    "# This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cooperative-armstrong",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.446711Z",
     "iopub.status.busy": "2021-04-03T22:23:32.446519Z",
     "iopub.status.idle": "2021-04-03T22:23:32.473473Z",
     "shell.execute_reply": "2021-04-03T22:23:32.473070Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.446683Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHKPTDIR = Path(\"TranslationChkpts\")\n",
    "DATADIR = \"/home/virk/devs/Projects/MolecularTranslationKaggleCompetition/data/\"\n",
    "LABELS_CSV_PATH = \"data/train_labels.csv\"\n",
    "VOCAB_FILEPATH = CHKPTDIR/\"vocab.pt\"\n",
    "TRAINPATHS_PATH = CHKPTDIR/\"train_paths.feather\"\n",
    "TESTPATHS_PATH = CHKPTDIR/\"test_paths.feather\"\n",
    "CHKPTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tb_logger = pl.loggers.TensorBoardLogger(CHKPTDIR, name=\"InchINet\")\n",
    "\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 128\n",
    "MAX_LEN = 16 # computed using corpus - max([len(vocab.tokenize(c)) for c in corpus]) 9 + 1 pad + 2 enclosing tokens\n",
    "EMB_SIZE = 512\n",
    "HDN_SIZE = 10\n",
    "INP_SIZE = (128, 128)\n",
    "N_INP_CH = 1\n",
    "N_OUT_CH = 3\n",
    "LR = 1e-2\n",
    "EPOCHS = 10\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liquid-gates",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.474999Z",
     "iopub.status.busy": "2021-04-03T22:23:32.474858Z",
     "iopub.status.idle": "2021-04-03T22:23:32.635887Z",
     "shell.execute_reply": "2021-04-03T22:23:32.634443Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.474981Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test  train  train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATADIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-intensity",
   "metadata": {},
   "source": [
    "# Data Block\n",
    "\n",
    "### LightningDataModule API\n",
    "\n",
    "To define a DataModule define 5 methods:\n",
    "1. prepare_data (how to download(), tokenize, etc…)\n",
    "2. setup (how to split, etc…)\n",
    "3. train_dataloader\n",
    "4. val_dataloader(s)\n",
    "5. test_dataloader(s)\n",
    "\n",
    "#### prepare_data\n",
    "Use this method to do things that might write to disk or that need to be done only from a single process in distributed settings.\n",
    "1. download\n",
    "2. tokenize\n",
    "3. etc…\n",
    "\n",
    "#### setup\n",
    "There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "1. count number of classes\n",
    "2. build vocabulary\n",
    "3. perform train/val/test splits\n",
    "4. apply transforms (defined explicitly in your datamodule or assigned in init)\n",
    "5. etc…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-survivor",
   "metadata": {},
   "source": [
    "## Vocab, Tokenizer, and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unavailable-engineer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.640066Z",
     "iopub.status.busy": "2021-04-03T22:23:32.639587Z",
     "iopub.status.idle": "2021-04-03T22:23:32.699775Z",
     "shell.execute_reply": "2021-04-03T22:23:32.699309Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.640003Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab, add_special_tokens=True):\n",
    "        self.vocab = vocab\n",
    "        # Get all elements sorted according to Atomic number\n",
    "        data = elements.Elements\n",
    "        # All elements in periodic table\n",
    "        self.elements = sorted(data, key=lambda i:i.AtomicNumber)  # Based on their AtomicNumber\n",
    "        # Sort longer names first for regex pattern formation\n",
    "        self.element_symbols = sorted([e.Symbol for e in self.elements], key=lambda e: len(e), reverse=True)\n",
    "        # Create regex pattern\n",
    "        self.pattern = f\"({'|'.join([f'{e}[0-9]*' for e in self.element_symbols])})\"\n",
    "        \n",
    "        if type(vocab) != type(None):\n",
    "            if add_special_tokens:\n",
    "                self.pad_token = \"<pad>\"\n",
    "                self.unk_token = \"<unk>\"\n",
    "                self.bos_token = \"<bos>\"\n",
    "                self.eos_token = \"<eos>\"\n",
    "                self.vocab = [self.pad_token, self.unk_token,self.bos_token,self.eos_token] + list(self.vocab)\n",
    "            \n",
    "            # Adding elements names into vocab and sort according to atomic number\n",
    "#             self.vocab = np.unique(self.vocab.tolist() + self.element_symbols)\n",
    "#             self.vocab = sorted(self.vocab.tolist(), key=lambda x: eval(f\"elements.{''.join(re.findall(r'[A-Za-z]', x))}.AtomicNumber\"))\n",
    "            # create class to index mapping\n",
    "            self._ctoi = defaultdict(lambda : self.unk_token, {c:i for i, c in enumerate(self.vocab)})\n",
    "                \n",
    "    def tokenize(self, string):\n",
    "        tokens = re.split(self.pattern, string)\n",
    "        tokens = list(filter(None, tokens))\n",
    "        return tokens\n",
    "    \n",
    "    def ctoi(self, c):\n",
    "        return self._ctoi[c]\n",
    "    \n",
    "    def itoc(self, i):\n",
    "        return self.vocab[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def save_vocab(self, path):\n",
    "        torch.save(self.vocab, path)\n",
    "        print(\"Saved @\", path)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        v = cls(None)\n",
    "        vocab = np.unique([w for s in corpus for w in v.tokenize(s)])\n",
    "        return cls(vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_vocab(cls, path):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab)\n",
    "    \n",
    "# Reference - https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#pretrainedtokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab=None):\n",
    "        self.vocab = vocab # Vocab class instance\n",
    "    \n",
    "    def tokenizer(self, x):\n",
    "        return self.vocab.tokenizer(x)\n",
    "    \n",
    "    def encode(self, s, max_len=None):\n",
    "        tokens = self.vocab.tokenize(s)\n",
    "        seq = [self.vocab.ctoi(t) for t in tokens]\n",
    "        attn_mask = [1]*len(seq)\n",
    "        \n",
    "        if max_len:\n",
    "            # Add padding to input\n",
    "            extra_len = max_len - len(seq) - 2 # 2 for start and end tokens\n",
    "            # Add start input token\n",
    "            seq = [self.vocab.ctoi(self.vocab.bos_token)] + seq\n",
    "            # Add end input token\n",
    "            seq += [self.vocab.ctoi(self.vocab.eos_token)]\n",
    "            attn_mask += [1, 1]\n",
    "            # Add padding token\n",
    "            seq += [self.vocab.ctoi(self.vocab.pad_token)]*extra_len\n",
    "            attn_mask += [0]*extra_len\n",
    "            \n",
    "        return {\"inp_seq\": seq, \"attn_mask\": attn_mask}\n",
    "    \n",
    "    def decode(self, tokens, inp_seq_name=\"inp_seq\"):\n",
    "        if isinstance(tokens, dict):\n",
    "            seq = tokens[inp_seq_name]\n",
    "            if isinstance(seq, (torch.Tensor, np.ndarray)):\n",
    "                seq = seq.tolist()\n",
    "        else:\n",
    "            seq = tokens\n",
    "        seq = ''.join([self.vocab.itoc(t) for t in seq])\n",
    "        # remove special tokens\n",
    "        for special_token in [self.vocab.pad_token, self.vocab.bos_token, self.vocab.eos_token]:\n",
    "            seq = seq.replace(special_token, '')\n",
    "        return seq\n",
    "    \n",
    "    @classmethod\n",
    "    def fit(cls, corpus):\n",
    "        vocab = Vocab.from_corpus(orcpus)\n",
    "        return cls(vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, path):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab)\n",
    "    \n",
    "class ImgtoInChIDataset(Dataset):\n",
    "    def __init__(self, paths, df=None, tsfms=None):\n",
    "        self.paths = paths\n",
    "        self.df = df\n",
    "        self.tsfms = tsfms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imgpath = Path(self.paths.iloc[idx,0])\n",
    "        imgid = imgpath.stem\n",
    "        \n",
    "        if any(self.df):\n",
    "            _imgid, inchi_string = self.df[self.df.image_id == imgid].values[0]\n",
    "            inchi_string = inchi_string.split(\"/\")[1]\n",
    "            assert _imgid == imgid\n",
    "        else:\n",
    "            inchi_string = 'TEST_SAMPLE'\n",
    "            \n",
    "        img = plt.imread(imgpath)\n",
    "        if any(self.tsfms):\n",
    "            img = self.tsfms(image=img)[\"image\"]\n",
    "        \n",
    "        return img, inchi_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-hartford",
   "metadata": {},
   "source": [
    "## Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efficient-nickname",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.700702Z",
     "iopub.status.busy": "2021-04-03T22:23:32.700508Z",
     "iopub.status.idle": "2021-04-03T22:23:32.744257Z",
     "shell.execute_reply": "2021-04-03T22:23:32.743657Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.700685Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImgToInChIDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, labels_csv_path:Union[str, Path], valset_ratio=0.05) -> None:\n",
    "        super().__init__()\n",
    "        self.labels_csv_path = labels_csv_path\n",
    "#         self.batch_size = batch_size\n",
    "        self.valset_ratio = valset_ratio\n",
    "        self.dims = (1, *INP_SIZE)\n",
    "        \n",
    "        self.train_tsfms = A.Compose([\n",
    "            A.Resize(*INP_SIZE, always_apply=True),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.RandomCrop(*INP_SIZE),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.test_tsfms = A.Compose([\n",
    "            A.Resize(*INP_SIZE, always_apply=True),\n",
    "            A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self, verbose=False):\n",
    "        \"\"\"Use this method to do things that might write to disk or that\n",
    "        need to be done only from a single process in distributed settings.\"\"\"\n",
    "        # Load labels in DataFrame\n",
    "        if verbose: print(\"Loading labels data...\", end=' ')\n",
    "        self.df = pd.read_csv(self.labels_csv_path)\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Load image paths\n",
    "        if verbose: print(\"Loading paths...\", end=' ')\n",
    "        if TRAINPATHS_PATH.exists():\n",
    "            self.train_paths = pd.read_feather(TRAINPATHS_PATH)\n",
    "        else:\n",
    "            self.train_paths = pd.DataFrame(list((Path(DATADIR)/\"train\").rglob(\"*.*\")), columns=[\"train_paths\"])\n",
    "            self.train_paths = self.train_paths.applymap(lambda x: str(x))\n",
    "            self.train_paths.to_feather(TRAINPATHS_PATH)\n",
    "        if TESTPATHS_PATH.exists():\n",
    "            self.test_paths = pd.read_feather(TESTPATHS_PATH)\n",
    "        else:\n",
    "            self.test_paths = pd.DataFrame(list((Path(DATADIR)/\"test\").rglob(\"*.*\")), columns=[\"test_paths\"])\n",
    "            self.test_paths = self.test_paths.applymap(lambda x: str(x))\n",
    "            self.test_paths.to_feather(TESTPATHS_PATH)\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Get Vocab and Tokenizer\n",
    "        if verbose: print(\"Loading vocab and tokenizer...\", end=' ')\n",
    "        if Path(VOCAB_FILEPATH).exists():\n",
    "            vocab = Vocab.load_vocab(VOCAB_FILEPATH)\n",
    "        else:\n",
    "            corpus = [s.split(\"/\")[1] for s in self.df.InChI.tolist()]\n",
    "            vocab = Vocab.from_corpus(corpus)\n",
    "#             print(\"# words =\", len(vocab))\n",
    "            vocab.save_vocab(VOCAB_FILEPATH)\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.tokenizer = Tokenizer(vocab)\n",
    "        if verbose: print(\"DONE!\")\n",
    "                \n",
    "    def setup(self, stage:Optional[str]=None) -> None:\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            trainpaths, valpaths = train_test_split(self.train_paths, test_size=self.valset_ratio)\n",
    "            self.trainset = ImgtoInChIDataset(trainpaths, self.df, self.train_tsfms)\n",
    "            self.valset = ImgtoInChIDataset(valpaths, self.df, self.test_tsfms)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.testset = ImgtoInChIDataset(self.test_paths, tsfms=self.test_tsfms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valset, BATCH_SIZE, shuffle=False, \n",
    "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        imgs = torch.cat([ins[0].unsqueeze(0) for ins in batch])\n",
    "        targets = [ins[1] for ins in batch]\n",
    "        targets = [self.tokenizer.encode(t, MAX_LEN) for t in targets]\n",
    "        inp_seqs = torch.Tensor([t[\"inp_seq\"] for t in targets]).long()\n",
    "        attn_masks = torch.Tensor([t[\"attn_mask\"] for t in targets]).float()\n",
    "        return imgs, inp_seqs, attn_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "healthy-valentine",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.745117Z",
     "iopub.status.busy": "2021-04-03T22:23:32.744955Z",
     "iopub.status.idle": "2021-04-03T22:23:32.768812Z",
     "shell.execute_reply": "2021-04-03T22:23:32.768166Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.745092Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imgs, inp_seqs, attn_masks = next(iter(trainloader))\n",
    "# tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
    "\n",
    "\n",
    "# print(\"SAMPLE BATCH =\", imgs.shape)\n",
    "# fig, axes = plt.subplots(4, 8, figsize=(18, 10))\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(imgs[i].squeeze(0), cmap=\"gray\")\n",
    "#     ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-folks",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T07:15:59.920534Z",
     "iopub.status.busy": "2021-04-02T07:15:59.920240Z",
     "iopub.status.idle": "2021-04-02T07:15:59.923276Z",
     "shell.execute_reply": "2021-04-02T07:15:59.922767Z",
     "shell.execute_reply.started": "2021-04-02T07:15:59.920510Z"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "### At the time of sentence prediction can we use HMMs or [Beam Search](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#overview) to make better decisions?\n",
    "\n",
    "Use different LSTM layers for each inchi substring like /c /h\n",
    "```\n",
    ">>> rnn = nn.LSTM(10, 20, 2)\n",
    ">>> input = torch.randn(1, 16, 10)\n",
    ">>> h0 = torch.randn(2, 16, 20)\n",
    ">>> c0 = torch.randn(2, 16, 20)\n",
    "```\n",
    "change number of layers here to num sublayers\n",
    "\n",
    "\n",
    "[Model from here](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "danish-killer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.769769Z",
     "iopub.status.busy": "2021-04-03T22:23:32.769632Z",
     "iopub.status.idle": "2021-04-03T22:23:32.809238Z",
     "shell.execute_reply": "2021-04-03T22:23:32.808687Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.769752Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.n_features = self.cnn.fc.in_features\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        features = self.cnn(x)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        return features\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention network for calculate attention value\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param attention_dim: input size of attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"Decoder network with attention network used for training\"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: input size of attention network\n",
    "        :param embed_dim: input size of embedding network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param vocab_size: total number of characters used in training\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        :param encoder_out: output of encoder network\n",
    "        :param encoded_captions: transformed sequence from character to integer\n",
    "        :param caption_lengths: length of transformed sequence\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        # embedding transformed sequence for vector\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        # set decode length by caption length - 1 because of omitting start token\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        # embed start tocken for LSTM input\n",
    "        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n",
    "        embeddings = self.embedding(start_tockens)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(decode_lengths):\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                (h, c))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:, t, :] = preds\n",
    "            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "measured-basketball",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.810074Z",
     "iopub.status.busy": "2021-04-03T22:23:32.809932Z",
     "iopub.status.idle": "2021-04-03T22:23:32.832480Z",
     "shell.execute_reply": "2021-04-03T22:23:32.831587Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.810056Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder_net = Encoder()\n",
    "# encoder_net(imgs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "disabled-finger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.833705Z",
     "iopub.status.busy": "2021-04-03T22:23:32.833461Z",
     "iopub.status.idle": "2021-04-03T22:23:32.876557Z",
     "shell.execute_reply": "2021-04-03T22:23:32.875993Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.833676Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False, out_channels=512):\n",
    "        super().__init__()\n",
    "        last_stride = 2 if INP_SIZE[0] == 256 else 1\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.out_channels = out_channels\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.outfc = nn.Sequential(\n",
    "            nn.Conv2d(512, out_channels, kernel_size=3, stride=last_stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(4,1,ceil_mode=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.outfc(out)\n",
    "        out = out.view(x.size(0), self.out_channels, -1)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        \n",
    "#         out = self.maxpool(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, enc_out_channels=512, device=torch.device(\"cuda\")):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.embd = nn.Embedding(vocab_size, EMB_SIZE)\n",
    "        \n",
    "        self.init_h = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial hidden state\n",
    "        self.init_c = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial cell state\n",
    "        self.lstm = nn.LSTM(enc_out_channels, vocab_size, batch_first=True)\n",
    "        self.decfc = nn.Linear(64, MAX_LEN)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.device = device\n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, encoder_out, inp_seqs):\n",
    "#         print(\"inp_seq before emb =\", inp_seqs.shape)\n",
    "        emb = self.embd(inp_seqs)\n",
    "#         print(f\"emb = {emb.shape}, encoder_out = {encoder_out.shape}\")\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        out = lstm_out + encoder_out\n",
    "        return out\n",
    "    \n",
    "    def predict(self, encoder_out, tokenizer):\n",
    "        bs = encoder_out.size(0)\n",
    "        syn_inp_seqs = torch.tensor(tokenizer.vocab.ctoi(tokenizer.vocab.bos_token), device=encoder_out.device)\n",
    "        syn_inp_seqs = torch.repeat_interleave(syn_inp_seqs, bs)\n",
    "        syn_inp_seqs = syn_inp_seqs.view(bs, 1)\n",
    "#         print(\"syn_inp_seqs before emb =\", syn_inp_seqs.shape)\n",
    "        \n",
    "        # Predict next tokens to start token\n",
    "        pred_emb = []\n",
    "        for i in range(MAX_LEN):\n",
    "            emb = self.embd(syn_inp_seqs)\n",
    "            pred, _ = self.lstm(emb)\n",
    "            pred_emb.append(pred)\n",
    "            syn_inp_seqs = pred.argmax(dim=-1)\n",
    "#         print(\"len =\", len(pred_emb))\n",
    "        pred_emb = torch.cat(pred_emb, dim=1)\n",
    "#         print(\"pred_emb =\", pred_emb.shape)\n",
    "        out = pred_emb + encoder_out\n",
    "        return out\n",
    "            \n",
    "class InChINet(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_net = Encoder(out_channels=vocab_size)\n",
    "        self.decoder_net = Decoder(vocab_size)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, imgs, inp_seqs):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "#         print(\"Encoder =\", encoder_out.shape)\n",
    "        pred_tokens = self.decoder_net(encoder_out, inp_seqs)\n",
    "        return pred_tokens\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = train_batch\n",
    "        output = self.forward(imgs, inp_seqs)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = val_batch\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        self.log('val_loss', loss, logger=True)\n",
    "        \n",
    "        lv_metric = self.calculate_lvdistance(output, inp_seqs)\n",
    "        self.logger.log_metrics({\"LvDistance\": lv_metric})\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, imgs, tokenizer):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "        pred_tokens = self.decoder_net.predict(encoder_out, tokenizer)\n",
    "        return pred_tokens\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100),\n",
    "            'name': 'AnnealingLR'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def inference(self, imgs):\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        return self.postprocessing(output)\n",
    "    \n",
    "    def calculate_lvdistance(self, output, target):\n",
    "        pred_seqs = self.postprocessing(output)\n",
    "        batch_distance = np.mean([\n",
    "            Levenshtein.distance(pred_seq, self.tokenizer.decode(inp_seq))\n",
    "            for pred_seq, inp_seq in zip(pred_seqs, target)\n",
    "        ])\n",
    "        return batch_distance\n",
    "    \n",
    "    def postprocessing(self, output):\n",
    "        final_preds = []\n",
    "        pred_tokens = output.argmax(dim=-1)\n",
    "        for i in range(pred_tokens.size(0)): # iterate on each sample\n",
    "            pred = pred_tokens[i].unique(dim=-1).tolist()\n",
    "            pred = self.tokenizer.decode(pred)\n",
    "            res = re.search(r'C', pred)\n",
    "            if res:\n",
    "                pred = pred[res.span()[0]:]\n",
    "            final_preds.append(pred)\n",
    "        return final_preds\n",
    "    \n",
    "# encoder_net = Encoder()\n",
    "# decoder_net = Decoder(len(vocab))\n",
    "# encoder_out = encoder_net(imgs)\n",
    "# print(\"Encoder =\", encoder_out.shape)\n",
    "# print(inp_seqs.shape)\n",
    "# pred_tokens = decoder_net(encoder_out, inp_seqs, tokenizer)\n",
    "# print(pred_tokens.shape)\n",
    "# pred_tokens = decoder_net.predict(encoder_out, tokenizer)\n",
    "# pred_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suited-shoot",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T22:23:32.877900Z",
     "iopub.status.busy": "2021-04-03T22:23:32.877576Z",
     "iopub.status.idle": "2021-04-03T22:25:52.510405Z",
     "shell.execute_reply": "2021-04-03T22:25:52.509145Z",
     "shell.execute_reply.started": "2021-04-03T22:23:32.877859Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 1069718), started 7:26:13 ago. (Use '!kill 1069718' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-43748de711c78c0a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-43748de711c78c0a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels data... DONE!\n",
      "Loading paths... DONE!\n",
      "Loading vocab and tokenizer... DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder_net | Encoder          | 13.2 M\n",
      "1 | decoder_net | Decoder          | 1.9 M \n",
      "2 | loss_fn     | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "15.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 M    Total params\n",
      "60.055    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7712295d93db40cbaaeafcaba6c9719a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-e87cd1cbabdf>\", line 13, in <module>\n",
      "    trainer.fit(model, dm)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 669, in run_train\n",
      "    self.train_loop.on_train_end()\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\", line 134, in on_train_end\n",
      "    self.check_checkpoint_callback(should_update=True, is_last=True)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\", line 164, in check_checkpoint_callback\n",
      "    cb.on_validation_end(self.trainer, model)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 212, in on_validation_end\n",
      "    self.save_checkpoint(trainer, pl_module)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 262, in save_checkpoint\n",
      "    self._save_last_checkpoint(trainer, pl_module, monitor_candidates)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 546, in _save_last_checkpoint\n",
      "    self._save_model(last_filepath, trainer, pl_module)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in _save_model\n",
      "    self.save_function(filepath, self.save_weights_only)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/properties.py\", line 327, in save_checkpoint\n",
      "    self.checkpoint_connector.save_checkpoint(filepath, weights_only)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 404, in save_checkpoint\n",
      "    atomic_save(checkpoint, filepath)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 65, in atomic_save\n",
      "    f.write(bytesbuffer.getvalue())\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-e87cd1cbabdf>\", line 13, in <module>\n",
      "    trainer.fit(model, dm)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 669, in run_train\n",
      "    self.train_loop.on_train_end()\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\", line 134, in on_train_end\n",
      "    self.check_checkpoint_callback(should_update=True, is_last=True)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\", line 164, in check_checkpoint_callback\n",
      "    cb.on_validation_end(self.trainer, model)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 212, in on_validation_end\n",
      "    self.save_checkpoint(trainer, pl_module)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 262, in save_checkpoint\n",
      "    self._save_last_checkpoint(trainer, pl_module, monitor_candidates)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 546, in _save_last_checkpoint\n",
      "    self._save_model(last_filepath, trainer, pl_module)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in _save_model\n",
      "    self.save_function(filepath, self.save_weights_only)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/properties.py\", line 327, in save_checkpoint\n",
      "    self.checkpoint_connector.save_checkpoint(filepath, weights_only)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 404, in save_checkpoint\n",
      "    atomic_save(checkpoint, filepath)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 65, in atomic_save\n",
      "    f.write(bytesbuffer.getvalue())\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3454, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2063, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 752, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 721, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e87cd1cbabdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_checkpoint_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshould_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mcheck_checkpoint_callback\u001b[0;34m(self, should_update, is_last)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_validation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36mon_validation_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Mode 2: save the last checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_last_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_last_checkpoint\u001b[0;34m(self, trainer, pl_module, ckpt_name_metrics)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         if (\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, filepath, trainer, pl_module)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/properties.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, filepath, weights_only)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, filepath, weights_only)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                 \u001b[0matomic_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/utilities/cloud_io.py\u001b[0m in \u001b[0;36matomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytesbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[0;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[1;32m   3356\u001b[0m                         \u001b[0masy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3357\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0masync_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3358\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TypeError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple)\u001b[0m\n\u001b[1;32m   3163\u001b[0m                     \u001b[0minteractivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'async'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3165\u001b[0;31m                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n\u001b[0m\u001b[1;32m   3166\u001b[0m                        interactivity=interactivity, compiler=compiler, result=result)\n\u001b[1;32m   3167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[0;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[1;32m   3374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3375\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_before_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mchained_exc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n\u001b[0m\u001b[1;32m   1143\u001b[0m                                                                      chained_exceptions_tb_offset)\n\u001b[1;32m   1144\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir {CHKPTDIR}\n",
    "\n",
    "dm = ImgToInChIDataModule(LABELS_CSV_PATH)\n",
    "dm.prepare_data(verbose=True)\n",
    "model = InChINet(dm.vocab_size, dm.tokenizer)\n",
    "# Add network graph to tensorboard\n",
    "# tb_logger.log_graph(model, [imgs[0].unsqueeze(0).to(model.device), inp_seqs[0].unsqueeze(0).to(model.device)])\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, auto_lr_find=True, max_epochs=10, precision=16, profiler=\"simple\", \n",
    "                     default_root_dir=CHKPTDIR, logger=tb_logger, callbacks=[lr_monitor])\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-premium",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-03T22:25:52.511234Z",
     "iopub.status.idle": "2021-04-03T22:25:52.511464Z",
     "shell.execute_reply": "2021-04-03T22:25:52.511352Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "total_distance = []\n",
    "for (imgs, inp_seqs, attn_masks) in progress_bar(valloader):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    imgs = imgs.to(device)\n",
    "    pred_seqs = model.inference(imgs)\n",
    "    batch_distance = np.mean([\n",
    "        Levenshtein.distance(pred_seq, tokenizer.decode(inp_seq))\n",
    "        for pred_seq, inp_seq in zip(pred_seqs, inp_seqs)\n",
    "    ])\n",
    "#     print(batch_distance)\n",
    "    total_distance += batch_distance\n",
    "np.nanmean(total_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-ivory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt)",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
