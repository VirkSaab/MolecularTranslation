{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "inchitranslationON.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn6tdwCudiJg"
      },
      "source": [
        "## Try these two experiments first\n",
        "1. Create original input image to inchi string translator and get accuracy and loss\n",
        "2. Create inchi image to inchi string translator and get accuracy and loss\n",
        "\n",
        "Now, compare the two. If the accuracy of inchi image -> inchi string is significantly higher than the original image -> inchi string then think about ***reconstruction experiments*** from original image to inchi image. [try this then](https://www.google.com/search?channel=fs&client=ubuntu&q=converting+shapes+from+one+to+another+using+deep+learning).\n",
        "\n",
        "# InChI decoding \n",
        "[Source](https://link.springer.com/content/pdf/10.1186%2Fs13321-015-0068-4.pdf)\n",
        "\n",
        "1. **Skeletal connections layer** This layer prefixed with `/c` represents connections between skeletal atoms by listing the canonical numbers in the chain of connected atoms. \n",
        "2. ***branches are given in parentheses***\n",
        "3. The canonical atomic numbers, which are used throughout the InChI, are always given in the formula’s element order. i.e. precendence is given to element according to periodic table while numbering elements. For example, `/C10H16N5O13P3` (the beginning of InChI for adenosine triphosphate) implies that atoms numbered 1–10 are carbons, 11–15 arenitrogens, 16–28 are oxygens, and 29–31 are phosporus. Hydrogen atoms are not explicitly numbered.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSqfBbfOorRk",
        "outputId": "6d160594-bdc0-4efa-f60d-5b4f9ee3d758"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303/kaggle-1.5.12-cp37-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H43CWjb3eNXq",
        "outputId": "eef70a47-7856-41af-eab1-e0de3d312f71"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/ColabNotebooks/MoleculeTranslationKC\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "api_token = {\"username\":\"virksaab\",\"key\":\"5f297e1cfab495d7db172252855d1fa0\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/ColabNotebooks/MoleculeTranslationKC\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y-k_dxSlWab",
        "outputId": "305731e6-1892-42b7-8eb2-f7215eacdede"
      },
      "source": [
        "!kaggle competitions download -c bms-molecular-translation -p ./data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "429 - Too Many Requests\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "tcLxAWLIdiJm"
      },
      "source": [
        "## image to inchi string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5KebmoUkDsQ",
        "outputId": "e0a24036-9164-420c-9245-04c6ce916d41"
      },
      "source": [
        "!ls /content/drive/MyDrive/ColabNotebooks"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cifar100_fastai.ipynb\t    kaggle.json\n",
            " inchitranslationON.ipynb   MoleculeTranslationKC\n",
            " jutils\t\t\t   'PANDA Challenge - make traindata.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oB--4LrPdiJm"
      },
      "source": [
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "\n",
        "# ! pip install pytorch-lightning --upgrade\n",
        "# ! pip install pytorch-lightning-bolts"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "scrolled": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb3nWfAqdiJn",
        "outputId": "e49d1ef0-b4f9-4868-85c0-aaa33a25d5e5"
      },
      "source": [
        "# Special Packages\n",
        "!pip install PeriodicElements\n",
        "!pip install albumentations\n",
        "!pip install timm\n",
        "!pip install python-Levenshtein\n",
        "!pip install torchmetrics"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PeriodicElements\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/13/7af7213deb980210f7699bbf7acfe43048329e565f7480c8bd74d7f4ae68/PeriodicElements-1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: PeriodicElements\n",
            "Successfully installed PeriodicElements-1.0\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.2)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp37-none-any.whl size=654019 sha256=bd78ad9974939e2780a69722d66f99404c7d9c40cdba98025a662577839694ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed imgaug-0.2.6\n",
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/d94f59780b5dd973154bf506d8ce598f6bfe7cc44dd445d644d6d3be8c39/timm-0.4.5-py3-none-any.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.5\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (54.2.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149807 sha256=51cf8ee0ee0cab3da00ef9dac46575443e19f56f8854221d382b08d5fcfb61ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n",
            "Collecting torchmetrics\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (1.19.5)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "QzrImZzhdiJp"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "9oJ__tVjdiJp",
        "outputId": "24675f66-6771-45a2-c002-a924e0ae6dd8"
      },
      "source": [
        "import torch, torchmetrics, timm, re, pickle, Levenshtein\n",
        "import torch.nn as nn\n",
        "import torchvision as tv\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from collections import defaultdict\n",
        "from fastprogress import progress_bar\n",
        "from typing import Optional, Union\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from elements import elements\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "torch.manual_seed(manualSeed);\n",
        "\n",
        "# This monkey-patch is there to be able to plot tensors\n",
        "torch.Tensor.ndim = property(lambda x: len(x.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b8d56a38da70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "XYQqLjw3diJq"
      },
      "source": [
        "CHKPTDIR = Path(\"TranslationChkpts\")\n",
        "DATADIR = \"../input/bms-molecular-translation\"\n",
        "LABELS_CSV_PATH = f\"{DATADIR}/train_labels.csv\"\n",
        "VOCAB_FILEPATH = CHKPTDIR/\"vocab.pt\"\n",
        "TRAINPATHS_PATH = CHKPTDIR/\"train_paths.feather\"\n",
        "TESTPATHS_PATH = CHKPTDIR/\"test_paths.feather\"\n",
        "CHKPTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tb_logger = pl.loggers.TensorBoardLogger(CHKPTDIR, name=\"InchINet\")\n",
        "\n",
        "N_WORKERS = 8\n",
        "N_SAMPLES = 100000\n",
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 16 # computed using corpus - max([len(vocab.tokenize(c)) for c in corpus]) 9 + 1 pad + 2 enclosing tokens\n",
        "EMB_SIZE = 512\n",
        "HDN_SIZE = 10\n",
        "INP_SIZE = (128, 128)\n",
        "N_INP_CH = 1\n",
        "N_OUT_CH = 3\n",
        "LR = 1e-2\n",
        "EPOCHS = 10\n",
        "beta1 = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "A7tA86gVdiJq"
      },
      "source": [
        "!ls {DATADIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edqiNXyediJq"
      },
      "source": [
        "# Data Block\n",
        "\n",
        "### LightningDataModule API\n",
        "\n",
        "To define a DataModule define 5 methods:\n",
        "1. prepare_data (how to download(), tokenize, etc…)\n",
        "2. setup (how to split, etc…)\n",
        "3. train_dataloader\n",
        "4. val_dataloader(s)\n",
        "5. test_dataloader(s)\n",
        "\n",
        "#### prepare_data\n",
        "Use this method to do things that might write to disk or that need to be done only from a single process in distributed settings.\n",
        "1. download\n",
        "2. tokenize\n",
        "3. etc…\n",
        "\n",
        "#### setup\n",
        "There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
        "1. count number of classes\n",
        "2. build vocabulary\n",
        "3. perform train/val/test splits\n",
        "4. apply transforms (defined explicitly in your datamodule or assigned in init)\n",
        "5. etc…\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPTDc9v3diJr"
      },
      "source": [
        "## Vocab, Tokenizer, and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "nz6f_PTjdiJr"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, vocab, add_special_tokens=True):\n",
        "        self.vocab = vocab\n",
        "        # Get all elements sorted according to Atomic number\n",
        "        data = elements.Elements\n",
        "        # All elements in periodic table\n",
        "        self.elements = sorted(data, key=lambda i:i.AtomicNumber)  # Based on their AtomicNumber\n",
        "        # Sort longer names first for regex pattern formation\n",
        "        self.element_symbols = sorted([e.Symbol for e in self.elements], key=lambda e: len(e), reverse=True)\n",
        "        # Create regex pattern\n",
        "        self.pattern = f\"({'|'.join([f'{e}[0-9]*' for e in self.element_symbols])})\"\n",
        "        \n",
        "        if type(vocab) != type(None):\n",
        "            if add_special_tokens:\n",
        "                self.pad_token = \"<pad>\"\n",
        "                self.unk_token = \"<unk>\"\n",
        "                self.bos_token = \"<bos>\"\n",
        "                self.eos_token = \"<eos>\"\n",
        "                self.vocab = [self.pad_token, self.unk_token,self.bos_token,self.eos_token] + list(self.vocab)\n",
        "            \n",
        "            # Adding elements names into vocab and sort according to atomic number\n",
        "#             self.vocab = np.unique(self.vocab.tolist() + self.element_symbols)\n",
        "#             self.vocab = sorted(self.vocab.tolist(), key=lambda x: eval(f\"elements.{''.join(re.findall(r'[A-Za-z]', x))}.AtomicNumber\"))\n",
        "            # create class to index mapping\n",
        "            self._ctoi = defaultdict(lambda : self.unk_token, {c:i for i, c in enumerate(self.vocab)})\n",
        "                \n",
        "    def tokenize(self, string):\n",
        "        tokens = re.split(self.pattern, string)\n",
        "        tokens = list(filter(None, tokens))\n",
        "        return tokens\n",
        "    \n",
        "    def ctoi(self, c):\n",
        "        return self._ctoi[c]\n",
        "    \n",
        "    def itoc(self, i):\n",
        "        return self.vocab[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "    \n",
        "    def save_vocab(self, path):\n",
        "        torch.save(self.vocab, path)\n",
        "        print(\"Saved @\", path)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_corpus(cls, corpus):\n",
        "        v = cls(None)\n",
        "        vocab = np.unique([w for s in corpus for w in v.tokenize(s)])\n",
        "        return cls(vocab)\n",
        "    \n",
        "    @classmethod\n",
        "    def load_vocab(cls, path):\n",
        "        vocab = torch.load(path)\n",
        "        return cls(vocab)\n",
        "    \n",
        "# Reference - https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#pretrainedtokenizer\n",
        "class Tokenizer:\n",
        "    def __init__(self, vocab=None):\n",
        "        self.vocab = vocab # Vocab class instance\n",
        "    \n",
        "    def tokenizer(self, x):\n",
        "        return self.vocab.tokenizer(x)\n",
        "    \n",
        "    def encode(self, s, max_len=None):\n",
        "        tokens = self.vocab.tokenize(s)\n",
        "        seq = [self.vocab.ctoi(t) for t in tokens]\n",
        "        attn_mask = [1]*len(seq)\n",
        "        \n",
        "        if max_len:\n",
        "            # Add padding to input\n",
        "            extra_len = max_len - len(seq) - 2 # 2 for start and end tokens\n",
        "            # Add start input token\n",
        "            seq = [self.vocab.ctoi(self.vocab.bos_token)] + seq\n",
        "            # Add end input token\n",
        "            seq += [self.vocab.ctoi(self.vocab.eos_token)]\n",
        "            attn_mask += [1, 1]\n",
        "            # Add padding token\n",
        "            seq += [self.vocab.ctoi(self.vocab.pad_token)]*extra_len\n",
        "            attn_mask += [0]*extra_len\n",
        "            \n",
        "        return {\"inp_seq\": seq, \"attn_mask\": attn_mask}\n",
        "    \n",
        "    def decode(self, tokens, inp_seq_name=\"inp_seq\"):\n",
        "        if isinstance(tokens, dict):\n",
        "            seq = tokens[inp_seq_name]\n",
        "            if isinstance(seq, (torch.Tensor, np.ndarray)):\n",
        "                seq = seq.tolist()\n",
        "        else:\n",
        "            seq = tokens\n",
        "        seq = ''.join([self.vocab.itoc(t) for t in seq])\n",
        "        # remove special tokens\n",
        "        for special_token in [self.vocab.pad_token, self.vocab.bos_token, self.vocab.eos_token]:\n",
        "            seq = seq.replace(special_token, '')\n",
        "        return seq\n",
        "    \n",
        "    @classmethod\n",
        "    def fit(cls, corpus):\n",
        "        vocab = Vocab.from_corpus(orcpus)\n",
        "        return cls(vocab)\n",
        "    \n",
        "    @classmethod\n",
        "    def load_from_file(cls, path):\n",
        "        vocab = torch.load(path)\n",
        "        return cls(vocab)\n",
        "    \n",
        "class ImgtoInChIDataset(Dataset):\n",
        "    def __init__(self, paths, df=None, tsfms=None):\n",
        "        self.paths = paths\n",
        "        self.df = df\n",
        "        self.tsfms = tsfms\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        imgpath = Path(self.paths.iloc[idx,0])\n",
        "        imgid = imgpath.stem\n",
        "        \n",
        "        if any(self.df):\n",
        "            _imgid, inchi_string = self.df[self.df.image_id == imgid].values[0]\n",
        "            inchi_string = inchi_string.split(\"/\")[1]\n",
        "            assert _imgid == imgid\n",
        "        else:\n",
        "            inchi_string = 'TEST_SAMPLE'\n",
        "            \n",
        "        img = plt.imread(imgpath)\n",
        "        if any(self.tsfms):\n",
        "            img = self.tsfms(image=img)[\"image\"]\n",
        "        \n",
        "        return img, inchi_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYzROO0adiJt"
      },
      "source": [
        "## Lightning Data Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "26Jw0COWdiJu"
      },
      "source": [
        "print(\"Train...\")\n",
        "if TRAINPATHS_PATH.exists():\n",
        "    train_paths = pd.read_feather(TRAINPATHS_PATH)\n",
        "else:\n",
        "    traingen = (Path(DATADIR)/\"train\").rglob(\"*.*\")\n",
        "    train_paths = [str(next(traingen)) for i in range(N_SAMPLES)]\n",
        "    train_paths = pd.DataFrame(train_paths, columns=[\"train_paths\"])\n",
        "    train_paths = train_paths.applymap(lambda x: str(x))\n",
        "    train_paths.to_feather(TRAINPATHS_PATH)\n",
        "\n",
        "print(\"Test...\")\n",
        "if TESTPATHS_PATH.exists():\n",
        "    test_paths = pd.read_feather(TESTPATHS_PATH)\n",
        "else:\n",
        "    testgen = (Path(DATADIR)/\"test\").rglob(\"*.*\")\n",
        "    test_paths = [str(next(testgen)) for i in range(N_SAMPLES)]\n",
        "    test_paths = pd.DataFrame(test_paths, columns=[\"test_paths\"])\n",
        "    test_paths = test_paths.applymap(lambda x: str(x))\n",
        "    test_paths.to_feather(TESTPATHS_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "MmkWt3cAdiJv"
      },
      "source": [
        "class ImgToInChIDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, labels_csv_path:Union[str, Path], valset_ratio=0.05) -> None:\n",
        "        super().__init__()\n",
        "        self.labels_csv_path = labels_csv_path\n",
        "#         self.batch_size = batch_size\n",
        "        self.valset_ratio = valset_ratio\n",
        "        self.dims = (1, *INP_SIZE)\n",
        "        \n",
        "        self.train_tsfms = A.Compose([\n",
        "            A.Resize(*INP_SIZE, always_apply=True),\n",
        "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "            A.RandomCrop(*INP_SIZE),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.5), std=(0.229)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.test_tsfms = A.Compose([\n",
        "            A.Resize(*INP_SIZE, always_apply=True),\n",
        "            A.Normalize(mean=(0.5), std=(0.229)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        \n",
        "    def prepare_data(self, verbose=False):\n",
        "        \"\"\"Use this method to do things that might write to disk or that\n",
        "        need to be done only from a single process in distributed settings.\"\"\"\n",
        "        # Load labels in DataFrame\n",
        "        if verbose: print(\"Loading labels data...\", end=' ')\n",
        "        self.df = pd.read_csv(self.labels_csv_path)\n",
        "        if verbose: print(\"DONE!\")\n",
        "        \n",
        "        # Load image paths\n",
        "        if verbose: print(\"Loading paths...\", end=' ')\n",
        "        if TRAINPATHS_PATH.exists():\n",
        "            self.train_paths = pd.read_feather(TRAINPATHS_PATH)\n",
        "        else:\n",
        "            self.train_paths = pd.DataFrame(list((Path(DATADIR)/\"train\").rglob(\"*.*\")), columns=[\"train_paths\"])\n",
        "            self.train_paths = self.train_paths.applymap(lambda x: str(x))\n",
        "            self.train_paths.to_feather(TRAINPATHS_PATH)\n",
        "        if TESTPATHS_PATH.exists():\n",
        "            self.test_paths = pd.read_feather(TESTPATHS_PATH)\n",
        "        else:\n",
        "            self.test_paths = pd.DataFrame(list((Path(DATADIR)/\"test\").rglob(\"*.*\")), columns=[\"test_paths\"])\n",
        "            self.test_paths = self.test_paths.applymap(lambda x: str(x))\n",
        "            self.test_paths.to_feather(TESTPATHS_PATH)\n",
        "        if verbose: print(\"DONE!\")\n",
        "        \n",
        "        # Get Vocab and Tokenizer\n",
        "        if verbose: print(\"Loading vocab and tokenizer...\", end=' ')\n",
        "        if Path(VOCAB_FILEPATH).exists():\n",
        "            vocab = Vocab.load_vocab(VOCAB_FILEPATH)\n",
        "        else:\n",
        "            corpus = [s.split(\"/\")[1] for s in self.df.InChI.tolist()]\n",
        "            vocab = Vocab.from_corpus(corpus)\n",
        "#             print(\"# words =\", len(vocab))\n",
        "            vocab.save_vocab(VOCAB_FILEPATH)\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.tokenizer = Tokenizer(vocab)\n",
        "        if verbose: print(\"DONE!\")\n",
        "                \n",
        "    def setup(self, stage:Optional[str]=None) -> None:\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == 'fit' or stage is None:\n",
        "            trainpaths, valpaths = train_test_split(self.train_paths, test_size=self.valset_ratio)\n",
        "            self.trainset = ImgtoInChIDataset(trainpaths, self.df, self.train_tsfms)\n",
        "            self.valset = ImgtoInChIDataset(valpaths, self.df, self.test_tsfms)\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.testset = ImgtoInChIDataset(self.test_paths, tsfms=self.test_tsfms)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.trainset, BATCH_SIZE, shuffle=True, \n",
        "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.valset, BATCH_SIZE, shuffle=False, \n",
        "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        imgs = torch.cat([ins[0].unsqueeze(0) for ins in batch])\n",
        "        targets = [ins[1] for ins in batch]\n",
        "        targets = [self.tokenizer.encode(t, MAX_LEN) for t in targets]\n",
        "        inp_seqs = torch.Tensor([t[\"inp_seq\"] for t in targets]).long()\n",
        "        attn_masks = torch.Tensor([t[\"attn_mask\"] for t in targets]).float()\n",
        "        return imgs, inp_seqs, attn_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "YqbXa3GtdiJw"
      },
      "source": [
        "# imgs, inp_seqs, attn_masks = next(iter(trainloader))\n",
        "# tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
        "\n",
        "\n",
        "# print(\"SAMPLE BATCH =\", imgs.shape)\n",
        "# fig, axes = plt.subplots(4, 8, figsize=(18, 10))\n",
        "# for i, ax in enumerate(axes.flat):\n",
        "#     ax.imshow(imgs[i].squeeze(0), cmap=\"gray\")\n",
        "#     ax.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-02T07:15:59.920534Z",
          "iopub.status.busy": "2021-04-02T07:15:59.920240Z",
          "iopub.status.idle": "2021-04-02T07:15:59.923276Z",
          "shell.execute_reply": "2021-04-02T07:15:59.922767Z",
          "shell.execute_reply.started": "2021-04-02T07:15:59.920510Z"
        },
        "tags": [],
        "id": "tG6zCMBRdiJx"
      },
      "source": [
        "# Model\n",
        "### At the time of sentence prediction can we use HMMs or [Beam Search](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#overview) to make better decisions?\n",
        "\n",
        "Use different LSTM layers for each inchi substring like /c /h\n",
        "```\n",
        ">>> rnn = nn.LSTM(10, 20, 2)\n",
        ">>> input = torch.randn(1, 16, 10)\n",
        ">>> h0 = torch.randn(2, 16, 20)\n",
        ">>> c0 = torch.randn(2, 16, 20)\n",
        "```\n",
        "change number of layers here to num sublayers\n",
        "\n",
        "\n",
        "[Model from here](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "TIh82fv0diJx"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model_name='resnet18', pretrained=False):\n",
        "        super().__init__()\n",
        "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
        "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.n_features = self.cnn.fc.in_features\n",
        "        self.cnn.global_pool = nn.Identity()\n",
        "        self.cnn.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs = x.size(0)\n",
        "        features = self.cnn(x)\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        return features\n",
        "    \n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention network for calculate attention value\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: input size of encoder network\n",
        "        :param decoder_dim: input size of decoder network\n",
        "        :param attention_dim: input size of attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "        return attention_weighted_encoding, alpha\n",
        "    \n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"Decoder network with attention network used for training\"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: input size of attention network\n",
        "        :param embed_dim: input size of embedding network\n",
        "        :param decoder_dim: input size of decoder network\n",
        "        :param vocab_size: total number of characters used in training\n",
        "        :param encoder_dim: input size of encoder network\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        :param encoder_out: output of encoder network\n",
        "        :param encoded_captions: transformed sequence from character to integer\n",
        "        :param caption_lengths: length of transformed sequence\n",
        "        \"\"\"\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "        # embedding transformed sequence for vector\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "        # initialize hidden state and cell state of LSTM cell\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "        # set decode length by caption length - 1 because of omitting start token\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n",
        "        # predict sequence\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
        "    \n",
        "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "        # embed start tocken for LSTM input\n",
        "        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n",
        "        embeddings = self.embedding(start_tockens)\n",
        "        # initialize hidden state and cell state of LSTM cell\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n",
        "        # predict sequence\n",
        "        for t in range(decode_lengths):\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
        "            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
        "                (h, c))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:, t, :] = preds\n",
        "            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n",
        "                break\n",
        "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "OR1-thMndiJx"
      },
      "source": [
        "# encoder_net = Encoder()\n",
        "# encoder_net(imgs).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "UUbhy76-diJx"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model_name='resnet18', pretrained=False, out_channels=512):\n",
        "        super().__init__()\n",
        "        last_stride = 2 if INP_SIZE[0] == 256 else 1\n",
        "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
        "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.out_channels = out_channels\n",
        "        self.cnn.global_pool = nn.Identity()\n",
        "        self.cnn.fc = nn.Identity()\n",
        "        self.outfc = nn.Sequential(\n",
        "            nn.Conv2d(512, out_channels, kernel_size=3, stride=last_stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.maxpool = nn.MaxPool2d(4,1,ceil_mode=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = self.outfc(out)\n",
        "        out = out.view(x.size(0), self.out_channels, -1)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        \n",
        "#         out = self.maxpool(out)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, enc_out_channels=512, device=torch.device(\"cuda\")):\n",
        "        super().__init__()\n",
        "        \n",
        "        \n",
        "        self.embd = nn.Embedding(vocab_size, EMB_SIZE)\n",
        "        \n",
        "        self.init_h = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial hidden state\n",
        "        self.init_c = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial cell state\n",
        "        self.lstm = nn.LSTM(enc_out_channels, vocab_size, batch_first=True)\n",
        "        self.decfc = nn.Linear(64, MAX_LEN)\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.device = device\n",
        "    \n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "    \n",
        "    def forward(self, encoder_out, inp_seqs):\n",
        "#         print(\"inp_seq before emb =\", inp_seqs.shape)\n",
        "        emb = self.embd(inp_seqs)\n",
        "#         print(f\"emb = {emb.shape}, encoder_out = {encoder_out.shape}\")\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        out = lstm_out + encoder_out\n",
        "        return out\n",
        "    \n",
        "    def predict(self, encoder_out, tokenizer):\n",
        "        bs = encoder_out.size(0)\n",
        "        syn_inp_seqs = torch.tensor(tokenizer.vocab.ctoi(tokenizer.vocab.bos_token), device=encoder_out.device)\n",
        "        syn_inp_seqs = torch.repeat_interleave(syn_inp_seqs, bs)\n",
        "        syn_inp_seqs = syn_inp_seqs.view(bs, 1)\n",
        "#         print(\"syn_inp_seqs before emb =\", syn_inp_seqs.shape)\n",
        "        \n",
        "        # Predict next tokens to start token\n",
        "        pred_emb = []\n",
        "        for i in range(MAX_LEN):\n",
        "            emb = self.embd(syn_inp_seqs)\n",
        "            pred, _ = self.lstm(emb)\n",
        "            pred_emb.append(pred)\n",
        "            syn_inp_seqs = pred.argmax(dim=-1)\n",
        "#         print(\"len =\", len(pred_emb))\n",
        "        pred_emb = torch.cat(pred_emb, dim=1)\n",
        "#         print(\"pred_emb =\", pred_emb.shape)\n",
        "        out = pred_emb + encoder_out\n",
        "        return out\n",
        "            \n",
        "class InChINet(pl.LightningModule):\n",
        "    def __init__(self, vocab_size, tokenizer):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.encoder_net = Encoder(out_channels=vocab_size)\n",
        "        self.decoder_net = Decoder(vocab_size)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        \n",
        "    def forward(self, imgs, inp_seqs):\n",
        "        encoder_out = self.encoder_net(imgs)\n",
        "#         print(\"Encoder =\", encoder_out.shape)\n",
        "        pred_tokens = self.decoder_net(encoder_out, inp_seqs)\n",
        "        return pred_tokens\n",
        "    \n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        imgs, inp_seqs, attn_masks = train_batch\n",
        "        output = self.forward(imgs, inp_seqs)\n",
        "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log('train_loss', loss, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def training_epoch_end(self, outputs):\n",
        "        for name,params in self.named_parameters():\n",
        "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
        "    \n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        imgs, inp_seqs, attn_masks = val_batch\n",
        "        output = self.predict(imgs, self.tokenizer)\n",
        "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
        "        self.log('val_loss', loss, logger=True)\n",
        "        \n",
        "        lv_metric = self.calculate_lvdistance(output, inp_seqs)\n",
        "        self.logger.log_metrics({\"LvDistance\": lv_metric})\n",
        "        return loss\n",
        "    \n",
        "    def predict(self, imgs, tokenizer):\n",
        "        encoder_out = self.encoder_net(imgs)\n",
        "        pred_tokens = self.decoder_net.predict(encoder_out, tokenizer)\n",
        "        return pred_tokens\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
        "        lr_scheduler = {\n",
        "            'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100),\n",
        "            'name': 'AnnealingLR'\n",
        "        }\n",
        "        return [optimizer], [lr_scheduler]\n",
        "    \n",
        "    def inference(self, imgs):\n",
        "        output = self.predict(imgs, self.tokenizer)\n",
        "        return self.postprocessing(output)\n",
        "    \n",
        "    def calculate_lvdistance(self, output, target):\n",
        "        pred_seqs = self.postprocessing(output)\n",
        "        batch_distance = np.mean([\n",
        "            Levenshtein.distance(pred_seq, self.tokenizer.decode(inp_seq))\n",
        "            for pred_seq, inp_seq in zip(pred_seqs, target)\n",
        "        ])\n",
        "        return batch_distance\n",
        "    \n",
        "    def postprocessing(self, output):\n",
        "        final_preds = []\n",
        "        pred_tokens = output.argmax(dim=-1)\n",
        "        for i in range(pred_tokens.size(0)): # iterate on each sample\n",
        "            pred = pred_tokens[i].unique(dim=-1).tolist()\n",
        "            pred = self.tokenizer.decode(pred)\n",
        "            res = re.search(r'C', pred)\n",
        "            if res:\n",
        "                pred = pred[res.span()[0]:]\n",
        "            final_preds.append(pred)\n",
        "        return final_preds\n",
        "    \n",
        "# encoder_net = Encoder()\n",
        "# decoder_net = Decoder(len(vocab))\n",
        "# encoder_out = encoder_net(imgs)\n",
        "# print(\"Encoder =\", encoder_out.shape)\n",
        "# print(inp_seqs.shape)\n",
        "# pred_tokens = decoder_net(encoder_out, inp_seqs, tokenizer)\n",
        "# print(pred_tokens.shape)\n",
        "# pred_tokens = decoder_net.predict(encoder_out, tokenizer)\n",
        "# pred_tokens.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "EfDDn8IvdiJy"
      },
      "source": [
        "# %tensorboard --logdir {CHKPTDIR}\n",
        "\n",
        "dm = ImgToInChIDataModule(LABELS_CSV_PATH)\n",
        "dm.prepare_data(verbose=True)\n",
        "model = InChINet(dm.vocab_size, dm.tokenizer)\n",
        "# Add network graph to tensorboard\n",
        "# tb_logger.log_graph(model, [imgs[0].unsqueeze(0).to(model.device), inp_seqs[0].unsqueeze(0).to(model.device)])\n",
        "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "trainer = pl.Trainer(gpus=1, auto_lr_find=True, max_epochs=10, precision=16, profiler=\"simple\", \n",
        "                     default_root_dir=CHKPTDIR, logger=tb_logger, callbacks=[lr_monitor])\n",
        "\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "trusted": true,
        "id": "r4BSO1kNdiJy"
      },
      "source": [
        "# device = torch.device(\"xla\")\n",
        "# total_distance = []\n",
        "# for (imgs, inp_seqs, attn_masks) in progress_bar(valloader):\n",
        "#     model.eval()\n",
        "#     model = model.to(device)\n",
        "#     imgs = imgs.to(device)\n",
        "#     pred_seqs = model.inference(imgs)\n",
        "#     batch_distance = np.mean([\n",
        "#         Levenshtein.distance(pred_seq, tokenizer.decode(inp_seq))\n",
        "#         for pred_seq, inp_seq in zip(pred_seqs, inp_seqs)\n",
        "#     ])\n",
        "# #     print(batch_distance)\n",
        "#     total_distance += batch_distance\n",
        "# np.nanmean(total_distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pqy7TIbfdiJy"
      },
      "source": [
        "torch."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BXp3QnavdiJy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGL_a3rhdiJz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}