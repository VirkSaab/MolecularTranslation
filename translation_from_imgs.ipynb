{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legendary-division",
   "metadata": {},
   "source": [
    "## Try these two experiments first\n",
    "1. Create original input image to inchi string translator and get accuracy and loss\n",
    "2. Create inchi image to inchi string translator and get accuracy and loss (Use [AutoEncoders](https://www.youtube.com/watch?v=E28CVTbNoSA&ab_channel=PascalPoupart))\n",
    "\n",
    "Now, compare the two. If the accuracy of inchi image -> inchi string is significantly higher than the original image -> inchi string then think about ***reconstruction experiments*** from original image to inchi image. [try this then](https://www.google.com/search?channel=fs&client=ubuntu&q=converting+shapes+from+one+to+another+using+deep+learning).\n",
    "\n",
    "# InChI decoding \n",
    "[Source](https://link.springer.com/content/pdf/10.1186%2Fs13321-015-0068-4.pdf)\n",
    "\n",
    "1. **Skeletal connections layer** This layer prefixed with `/c` represents connections between skeletal atoms by listing the canonical numbers in the chain of connected atoms. \n",
    "2. ***branches are given in parentheses***\n",
    "3. The canonical atomic numbers, which are used throughout the InChI, are always given in the formula’s element order. i.e. precendence is given to element according to periodic table while numbering elements. For example, `/C10H16N5O13P3` (the beginning of InChI for adenosine triphosphate) implies that atoms numbered 1–10 are carbons, 11–15 arenitrogens, 16–28 are oxygens, and 29–31 are phosporus. Hydrogen atoms are not explicitly numbered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-blind",
   "metadata": {
    "tags": []
   },
   "source": [
    "## image to inchi string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dietary-worcester",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:19.746174Z",
     "iopub.status.busy": "2021-04-08T07:41:19.746011Z",
     "iopub.status.idle": "2021-04-08T07:41:19.749214Z",
     "shell.execute_reply": "2021-04-08T07:41:19.748485Z",
     "shell.execute_reply.started": "2021-04-08T07:41:19.746153Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Special Packages\n",
    "# !pip install PeriodicElements\n",
    "# !pip install albumentations\n",
    "# !pip install timm\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuck-terror",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:19.750213Z",
     "iopub.status.busy": "2021-04-08T07:41:19.750062Z",
     "iopub.status.idle": "2021-04-08T07:41:19.775344Z",
     "shell.execute_reply": "2021-04-08T07:41:19.774854Z",
     "shell.execute_reply.started": "2021-04-08T07:41:19.750196Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "theoretical-merit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:19.776101Z",
     "iopub.status.busy": "2021-04-08T07:41:19.775970Z",
     "iopub.status.idle": "2021-04-08T07:41:23.151878Z",
     "shell.execute_reply": "2021-04-08T07:41:23.151056Z",
     "shell.execute_reply.started": "2021-04-08T07:41:19.776085Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import torch, torchmetrics, timm, re, pickle, Levenshtein\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from fastprogress import progress_bar\n",
    "from typing import Optional, Union\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from elements import elements\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from preprocessing import preprocess_image\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "torch.manual_seed(manualSeed);\n",
    "\n",
    "# This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "antique-average",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.153074Z",
     "iopub.status.busy": "2021-04-08T07:41:23.152838Z",
     "iopub.status.idle": "2021-04-08T07:41:23.183144Z",
     "shell.execute_reply": "2021-04-08T07:41:23.182577Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.153044Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHKPTDIR = Path(\"TranslationChkpts\")\n",
    "DATADIR = \"data/bms-molecular-translation\"\n",
    "LABELS_CSV_PATH = \"data/train_labels.csv\"\n",
    "VOCAB_FILEPATH = CHKPTDIR/\"vocab.pt\"\n",
    "TRAINPATHS_PATH = CHKPTDIR/\"train_paths.feather\"\n",
    "TESTPATHS_PATH = CHKPTDIR/\"test_paths.feather\"\n",
    "CHKPTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tb_logger = pl.loggers.TensorBoardLogger(CHKPTDIR, name=\"InchINet\")\n",
    "\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 256\n",
    "PRECISION = 16\n",
    "MAX_LEN = 12 # computed using corpus - max([len(vocab.tokenize(c)) for c in corpus]) 9 + 1 pad + 2 enclosing tokens\n",
    "EMB_SIZE = 512\n",
    "HDN_SIZE = 10\n",
    "INP_SIZE = (128, 128)\n",
    "N_INP_CH = 1\n",
    "N_OUT_CH = 3\n",
    "LR = 1e-2\n",
    "EPOCHS = 10\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flush-northeast",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.186376Z",
     "iopub.status.busy": "2021-04-08T07:41:23.186055Z",
     "iopub.status.idle": "2021-04-08T07:41:23.348247Z",
     "shell.execute_reply": "2021-04-08T07:41:23.347310Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.186321Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test  train  train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATADIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-recipient",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Block\n",
    "\n",
    "### LightningDataModule API\n",
    "\n",
    "To define a DataModule define 5 methods:\n",
    "1. prepare_data (how to download(), tokenize, etc…)\n",
    "2. setup (how to split, etc…)\n",
    "3. train_dataloader\n",
    "4. val_dataloader(s)\n",
    "5. test_dataloader(s)\n",
    "\n",
    "#### prepare_data\n",
    "Use this method to do things that might write to disk or that need to be done only from a single process in distributed settings.\n",
    "1. download\n",
    "2. tokenize\n",
    "3. etc…\n",
    "\n",
    "#### setup\n",
    "There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "1. count number of classes\n",
    "2. build vocabulary\n",
    "3. perform train/val/test splits\n",
    "4. apply transforms (defined explicitly in your datamodule or assigned in init)\n",
    "5. etc…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-argentina",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vocab and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "taken-portugal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.351563Z",
     "iopub.status.busy": "2021-04-08T07:41:23.351252Z",
     "iopub.status.idle": "2021-04-08T07:41:23.397825Z",
     "shell.execute_reply": "2021-04-08T07:41:23.397276Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.351526Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RawDataset(Dataset):\n",
    "    def __init__(self, datadir, df=None):\n",
    "        super().__init__()\n",
    "        self.paths = list(Path(datadir).rglob(\"*.*\"))\n",
    "        if df is not None:\n",
    "            self.idtoinchi_dict = {\n",
    "                _id:_inchi for _id, _inchi in\n",
    "                zip(df[\"image_id\"].values.tolist(), df[\"InChI\"].values.tolist())\n",
    "            }\n",
    "        if len(self.paths) == 0:\n",
    "            print(\"No paths found.\")\n",
    "        self.piltotensor = PILToTensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imgpath = self.paths[idx]\n",
    "        imgid = imgpath.stem\n",
    "        img = preprocess_image(imgpath, out_size=INP_SIZE)\n",
    "        img = torch.from_numpy(np.array(img))\n",
    "        if hasattr(self, \"idtoinchi_dict\"):\n",
    "            target = self.idtoinchi_dict[imgid]\n",
    "            return img, target\n",
    "        return img, \"test_placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "about-elevation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.398925Z",
     "iopub.status.busy": "2021-04-08T07:41:23.398719Z",
     "iopub.status.idle": "2021-04-08T07:41:23.432624Z",
     "shell.execute_reply": "2021-04-08T07:41:23.432065Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.398900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab, add_special_tokens=True):\n",
    "        self.vocab = vocab\n",
    "        # Get all elements sorted according to Atomic number\n",
    "        data = elements.Elements\n",
    "        # All elements in periodic table\n",
    "        self.elements = sorted(data, key=lambda i:i.AtomicNumber)  # Based on their AtomicNumber\n",
    "        # Sort longer names first for regex pattern formation\n",
    "        self.element_symbols = sorted([e.Symbol for e in self.elements], key=lambda e: len(e), reverse=True)\n",
    "        # Create regex pattern\n",
    "        self.pattern = f\"({'|'.join([f'{e}[0-9]*' for e in self.element_symbols])})\"\n",
    "        \n",
    "        if type(vocab) != type(None):\n",
    "            if add_special_tokens:\n",
    "                self.pad_token = \"<pad>\"\n",
    "                self.unk_token = \"<unk>\"\n",
    "                self.bos_token = \"<bos>\"\n",
    "                self.eos_token = \"<eos>\"\n",
    "                self.vocab = [self.pad_token, self.unk_token,self.bos_token,self.eos_token] + list(self.vocab)\n",
    "            \n",
    "            # Adding elements names into vocab and sort according to atomic number\n",
    "#             self.vocab = np.unique(self.vocab.tolist() + self.element_symbols)\n",
    "#             self.vocab = sorted(self.vocab.tolist(), key=lambda x: eval(f\"elements.{''.join(re.findall(r'[A-Za-z]', x))}.AtomicNumber\"))\n",
    "            # create class to index mapping\n",
    "            self._ctoi = defaultdict(lambda : self.unk_token, {c:i for i, c in enumerate(self.vocab)})\n",
    "                \n",
    "    def tokenize(self, string):\n",
    "        tokens = re.split(self.pattern, string)\n",
    "        tokens = list(filter(None, tokens))\n",
    "        return tokens\n",
    "    \n",
    "    def ctoi(self, c):\n",
    "        return self._ctoi[c]\n",
    "    \n",
    "    def itoc(self, i):\n",
    "        return self.vocab[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def save_vocab(self, path):\n",
    "        torch.save(self.vocab, path)\n",
    "        print(\"Saved @\", path)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        v = cls(None)\n",
    "        vocab = np.unique([w for s in corpus for w in v.tokenize(s)])\n",
    "        return cls(vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_vocab(cls, path):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab)\n",
    "    \n",
    "# Reference - https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#pretrainedtokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab=None):\n",
    "        self.vocab = vocab # Vocab class instance\n",
    "    \n",
    "    def tokenizer(self, x):\n",
    "        return self.vocab.tokenizer(x)\n",
    "    \n",
    "    def encode(self, s, max_len=None):\n",
    "        tokens = self.vocab.tokenize(s)\n",
    "        seq = [self.vocab.ctoi(t) for t in tokens]\n",
    "        attn_mask = [1]*len(seq)\n",
    "        \n",
    "        if max_len:\n",
    "            # Add padding to input\n",
    "            extra_len = max_len - len(seq) - 2 # 2 for start and end tokens\n",
    "            # Add start input token\n",
    "            seq = [self.vocab.ctoi(self.vocab.bos_token)] + seq\n",
    "            # Add end input token\n",
    "            seq += [self.vocab.ctoi(self.vocab.eos_token)]\n",
    "            attn_mask += [1, 1]\n",
    "            # Add padding token\n",
    "            seq += [self.vocab.ctoi(self.vocab.pad_token)]*extra_len\n",
    "            attn_mask += [0]*extra_len\n",
    "            \n",
    "        return {\"inp_seq\": seq, \"attn_mask\": attn_mask}\n",
    "    \n",
    "    def decode(self, tokens, inp_seq_name=\"inp_seq\"):\n",
    "        if isinstance(tokens, dict):\n",
    "            seq = tokens[inp_seq_name]\n",
    "            if isinstance(seq, (torch.Tensor, np.ndarray)):\n",
    "                seq = seq.tolist()\n",
    "        else:\n",
    "            seq = tokens\n",
    "        seq = ''.join([self.vocab.itoc(t) for t in seq])\n",
    "        # remove special tokens\n",
    "        for special_token in [self.vocab.pad_token, self.vocab.bos_token, self.vocab.eos_token]:\n",
    "            seq = seq.replace(special_token, '')\n",
    "        return seq\n",
    "    \n",
    "    @classmethod\n",
    "    def fit(cls, corpus):\n",
    "        vocab = Vocab.from_corpus(orcpus)\n",
    "        return cls(vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, path):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-replication",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "established-taiwan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.433522Z",
     "iopub.status.busy": "2021-04-08T07:41:23.433383Z",
     "iopub.status.idle": "2021-04-08T07:41:23.481994Z",
     "shell.execute_reply": "2021-04-08T07:41:23.481503Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.433504Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImgtoInChIDataset(Dataset):\n",
    "    def __init__(self, paths, df=None, tsfms=None):\n",
    "        self.paths = paths\n",
    "        if df is not None:\n",
    "            self.idtoinchi_dict = {\n",
    "                _id:_inchi for _id, _inchi in\n",
    "                zip(df[\"image_id\"].values.tolist(), df[\"InChI\"].values.tolist())\n",
    "            }\n",
    "        self.tsfms = tsfms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imgpath = self.paths[idx]\n",
    "        imgid = Path(imgpath).stem\n",
    "        img = np.array(preprocess_image(imgpath, out_size=INP_SIZE), dtype=np.float32)/255.\n",
    "        if self.tsfms is not None:\n",
    "            img = self.tsfms(image=img)[\"image\"]\n",
    "        \n",
    "        if hasattr(self, \"idtoinchi_dict\"):\n",
    "            target = self.idtoinchi_dict[imgid]\n",
    "            target = target.split(\"/\")[1]\n",
    "            return img, target\n",
    "        \n",
    "        return img, \"test_placeholder\"\n",
    "    \n",
    "class ImgToInChIDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tb_logger, valset_ratio=0.05) -> None:\n",
    "        super().__init__()\n",
    "        self.tb_logger = tb_logger\n",
    "        self.valset_ratio = valset_ratio\n",
    "        self.dims = (1, *INP_SIZE)\n",
    "        \n",
    "        self.train_tsfms = A.Compose([\n",
    "#             A.Resize(*INP_SIZE, always_apply=True),\n",
    "#             A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "#             A.RandomCrop(*INP_SIZE),\n",
    "#             A.RandomBrightnessContrast(p=0.5),\n",
    "#             A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.test_tsfms = A.Compose([\n",
    "#             A.Resize(*INP_SIZE, always_apply=True),\n",
    "#             A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self, verbose=False):\n",
    "        \"\"\"Use this method to do things that might write to disk or that\n",
    "        need to be done only from a single process in distributed settings.\"\"\"\n",
    "        # Load labels in DataFrame\n",
    "        if verbose: print(\"Loading labels data...\", end=' ')\n",
    "        self.df = pd.read_csv(LABELS_CSV_PATH)\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Load image paths\n",
    "        if verbose: print(\"Loading paths...\", end=' ')\n",
    "        if TRAINPATHS_PATH.exists():\n",
    "            self.train_paths = pd.read_feather(TRAINPATHS_PATH)\n",
    "            self.train_paths = self.train_paths.train_paths.tolist()\n",
    "        else:\n",
    "            self.train_paths = pd.DataFrame(list((Path(DATADIR)/\"train\").rglob(\"*.*\")), columns=[\"train_paths\"])\n",
    "            self.train_paths = self.train_paths.applymap(lambda x: str(x))\n",
    "            self.train_paths.to_feather(TRAINPATHS_PATH)\n",
    "        if TESTPATHS_PATH.exists():\n",
    "            self.test_paths = pd.read_feather(TESTPATHS_PATH)\n",
    "            self.test_paths = self.test_paths.test_paths.tolist()\n",
    "        else:\n",
    "            self.test_paths = pd.DataFrame(list((Path(DATADIR)/\"test\").rglob(\"*.*\")), columns=[\"test_paths\"])\n",
    "            self.test_paths = self.test_paths.applymap(lambda x: str(x))\n",
    "            self.test_paths.to_feather(TESTPATHS_PATH)\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Get Vocab and Tokenizer\n",
    "        if verbose: print(\"Loading vocab and tokenizer...\", end=' ')\n",
    "        if Path(VOCAB_FILEPATH).exists():\n",
    "            vocab = Vocab.load_vocab(VOCAB_FILEPATH)\n",
    "        else:\n",
    "            corpus = [s.split(\"/\")[1] for s in self.df.InChI.tolist()]\n",
    "            vocab = Vocab.from_corpus(corpus)\n",
    "#             print(\"# words =\", len(vocab))\n",
    "            vocab.save_vocab(VOCAB_FILEPATH)\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.tokenizer = Tokenizer(vocab)\n",
    "        if verbose: print(\"DONE!\")\n",
    "                \n",
    "    def setup(self, stage:Optional[str]=None) -> None:\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            trainpaths, valpaths = train_test_split(self.train_paths, test_size=self.valset_ratio)\n",
    "            self.trainset = ImgtoInChIDataset(trainpaths, self.df, self.train_tsfms)\n",
    "            self.valset = ImgtoInChIDataset(valpaths, self.df, self.test_tsfms)\n",
    "            \n",
    "            # Sample batch\n",
    "            imgs, inp_seqs, attn_masks = next(iter(self.train_dataloader()))\n",
    "            self.tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.testset = ImgtoInChIDataset(self.test_paths, tsfms=self.test_tsfms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valset, BATCH_SIZE, shuffle=False, \n",
    "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        imgs = torch.cat([ins[0].unsqueeze(0) for ins in batch])\n",
    "        targets = [ins[1] for ins in batch]\n",
    "        targets = [self.tokenizer.encode(t, MAX_LEN) for t in targets]\n",
    "        inp_seqs = torch.Tensor([t[\"inp_seq\"] for t in targets]).long()\n",
    "        attn_masks = torch.Tensor([t[\"attn_mask\"] for t in targets]).float()\n",
    "        return imgs, inp_seqs, attn_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extensive-dependence",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.483174Z",
     "iopub.status.busy": "2021-04-08T07:41:23.482901Z",
     "iopub.status.idle": "2021-04-08T07:41:23.505994Z",
     "shell.execute_reply": "2021-04-08T07:41:23.505540Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.483129Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imgs, inp_seqs, attn_masks = next(iter(trainloader))\n",
    "# tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
    "\n",
    "\n",
    "# print(\"SAMPLE BATCH =\", imgs.shape)\n",
    "# fig, axes = plt.subplots(4, 8, figsize=(18, 10))\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(imgs[i].squeeze(0), cmap=\"gray\")\n",
    "#     ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-tiger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T07:15:59.920534Z",
     "iopub.status.busy": "2021-04-02T07:15:59.920240Z",
     "iopub.status.idle": "2021-04-02T07:15:59.923276Z",
     "shell.execute_reply": "2021-04-02T07:15:59.922767Z",
     "shell.execute_reply.started": "2021-04-02T07:15:59.920510Z"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "### At the time of sentence prediction can we use HMMs or [Beam Search](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#overview) to make better decisions?\n",
    "\n",
    "Use different LSTM layers for each inchi substring like /c /h\n",
    "```\n",
    ">>> rnn = nn.LSTM(10, 20, 2)\n",
    ">>> input = torch.randn(1, 16, 10)\n",
    ">>> h0 = torch.randn(2, 16, 20)\n",
    ">>> c0 = torch.randn(2, 16, 20)\n",
    "```\n",
    "change number of layers here to num sublayers\n",
    "\n",
    "\n",
    "[Model from here](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handed-commons",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.507071Z",
     "iopub.status.busy": "2021-04-08T07:41:23.506841Z",
     "iopub.status.idle": "2021-04-08T07:41:23.544268Z",
     "shell.execute_reply": "2021-04-08T07:41:23.543692Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.507049Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.n_features = self.cnn.fc.in_features\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        features = self.cnn(x)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        return features\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention network for calculate attention value\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param attention_dim: input size of attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"Decoder network with attention network used for training\"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: input size of attention network\n",
    "        :param embed_dim: input size of embedding network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param vocab_size: total number of characters used in training\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        :param encoder_out: output of encoder network\n",
    "        :param encoded_captions: transformed sequence from character to integer\n",
    "        :param caption_lengths: length of transformed sequence\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        # embedding transformed sequence for vector\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        # set decode length by caption length - 1 because of omitting start token\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        # embed start tocken for LSTM input\n",
    "        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n",
    "        embeddings = self.embedding(start_tockens)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(decode_lengths):\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                (h, c))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:, t, :] = preds\n",
    "            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elementary-trick",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.545185Z",
     "iopub.status.busy": "2021-04-08T07:41:23.545017Z",
     "iopub.status.idle": "2021-04-08T07:41:23.567162Z",
     "shell.execute_reply": "2021-04-08T07:41:23.566604Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.545168Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder_net = Encoder()\n",
    "# encoder_net(imgs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "angry-december",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:41:23.568311Z",
     "iopub.status.busy": "2021-04-08T07:41:23.568106Z",
     "iopub.status.idle": "2021-04-08T07:41:23.591084Z",
     "shell.execute_reply": "2021-04-08T07:41:23.590468Z",
     "shell.execute_reply.started": "2021-04-08T07:41:23.568292Z"
    }
   },
   "outputs": [],
   "source": [
    "# WORKING MODEL\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, model_name='resnet18', pretrained=False, out_channels=512):\n",
    "#         super().__init__()\n",
    "#         last_stride = 2 if INP_SIZE[0] == 256 else 1\n",
    "#         self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "#         self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#         self.out_channels = out_channels\n",
    "#         self.cnn.global_pool = nn.Identity()\n",
    "#         self.cnn.fc = nn.Identity()\n",
    "#         self.outfc = nn.Sequential(\n",
    "#             nn.Conv2d(512, out_channels, kernel_size=3, stride=last_stride, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(True)\n",
    "#         )\n",
    "#         self.maxpool = nn.MaxPool2d(4,1,ceil_mode=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.cnn(x)\n",
    "#         out = self.outfc(out)\n",
    "#         out = out.view(x.size(0), self.out_channels, -1)\n",
    "#         out = out.permute(0, 2, 1)\n",
    "        \n",
    "# #         out = self.maxpool(out)\n",
    "# #         out = out.view(out.size(0), -1)\n",
    "#         return out\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, enc_out_channels=512, device=torch.device(\"cuda\")):\n",
    "#         super().__init__()\n",
    "        \n",
    "        \n",
    "#         self.embd = nn.Embedding(vocab_size, EMB_SIZE)\n",
    "        \n",
    "#         self.init_h = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial hidden state\n",
    "#         self.init_c = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial cell state\n",
    "#         self.lstm = nn.LSTM(enc_out_channels, vocab_size, batch_first=True)\n",
    "#         self.decfc = nn.Linear(64, MAX_LEN)\n",
    "        \n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "#         self.device = device\n",
    "    \n",
    "#     def init_hidden_state(self, encoder_out):\n",
    "#         mean_encoder_out = encoder_out.mean(dim=1)\n",
    "#         h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "#         c = self.init_c(mean_encoder_out)\n",
    "#         return h, c\n",
    "    \n",
    "#     def forward(self, encoder_out, inp_seqs):\n",
    "# #         print(\"inp_seq before emb =\", inp_seqs.shape)\n",
    "#         emb = self.embd(inp_seqs)\n",
    "# #         print(f\"emb = {emb.shape}, encoder_out = {encoder_out.shape}\")\n",
    "#         lstm_out, _ = self.lstm(emb)\n",
    "#         out = lstm_out + encoder_out\n",
    "#         return out\n",
    "    \n",
    "#     def predict(self, encoder_out, tokenizer):\n",
    "#         bs = encoder_out.size(0)\n",
    "#         syn_inp_seqs = torch.tensor(tokenizer.vocab.ctoi(tokenizer.vocab.bos_token), device=encoder_out.device)\n",
    "#         syn_inp_seqs = torch.repeat_interleave(syn_inp_seqs, bs)\n",
    "#         syn_inp_seqs = syn_inp_seqs.view(bs, 1)\n",
    "# #         print(\"syn_inp_seqs before emb =\", syn_inp_seqs.shape)\n",
    "        \n",
    "#         # Predict next tokens to start token\n",
    "#         pred_emb = []\n",
    "#         for i in range(MAX_LEN):\n",
    "#             emb = self.embd(syn_inp_seqs)\n",
    "#             pred, _ = self.lstm(emb)\n",
    "#             pred_emb.append(pred)\n",
    "#             syn_inp_seqs = pred.argmax(dim=-1)\n",
    "# #         print(\"len =\", len(pred_emb))\n",
    "#         pred_emb = torch.cat(pred_emb, dim=1)\n",
    "# #         print(\"pred_emb =\", pred_emb.shape)\n",
    "#         out = pred_emb + encoder_out\n",
    "#         return out\n",
    "            \n",
    "# class InChINet(pl.LightningModule):\n",
    "#     def __init__(self, vocab_size, tokenizer):\n",
    "#         super().__init__()\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.encoder_net = Encoder(out_channels=vocab_size)\n",
    "#         self.decoder_net = Decoder(vocab_size)\n",
    "#         self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "#     def forward(self, imgs, inp_seqs):\n",
    "#         encoder_out = self.encoder_net(imgs)\n",
    "# #         print(\"Encoder =\", encoder_out.shape)\n",
    "#         pred_tokens = self.decoder_net(encoder_out, inp_seqs)\n",
    "#         return pred_tokens\n",
    "    \n",
    "#     def training_step(self, train_batch, batch_idx):\n",
    "#         imgs, inp_seqs, attn_masks = train_batch\n",
    "#         output = self.forward(imgs, inp_seqs)\n",
    "#         loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "#         # Logging to TensorBoard by default\n",
    "#         self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def training_epoch_end(self, outputs):\n",
    "#         for name,params in self.named_parameters():\n",
    "#             self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "    \n",
    "#     def validation_step(self, val_batch, batch_idx):\n",
    "#         imgs, inp_seqs, attn_masks = val_batch\n",
    "#         output = self.predict(imgs, self.tokenizer)\n",
    "#         loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "#         self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        \n",
    "#         lv_metric = self.calculate_lvdistance(output, inp_seqs)\n",
    "#         self.logger.log_metrics({\"LvDistance\": lv_metric}, step=1)\n",
    "#         return loss\n",
    "    \n",
    "#     def predict(self, imgs, tokenizer):\n",
    "#         encoder_out = self.encoder_net(imgs)\n",
    "#         pred_tokens = self.decoder_net.predict(encoder_out, tokenizer)\n",
    "#         return pred_tokens\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "#         lr_scheduler = {\n",
    "#             'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100),\n",
    "#             'name': 'AnnealingLR'\n",
    "#         }\n",
    "#         return [optimizer], [lr_scheduler]\n",
    "    \n",
    "#     def inference(self, imgs):\n",
    "#         output = self.predict(imgs, self.tokenizer)\n",
    "#         return self.postprocessing(output)\n",
    "    \n",
    "#     def calculate_lvdistance(self, output, target):\n",
    "#         pred_seqs = self.postprocessing(output)\n",
    "#         batch_distance = np.mean([\n",
    "#             Levenshtein.distance(pred_seq, self.tokenizer.decode(inp_seq))\n",
    "#             for pred_seq, inp_seq in zip(pred_seqs, target)\n",
    "#         ])\n",
    "#         return batch_distance\n",
    "    \n",
    "#     def postprocessing(self, output):\n",
    "#         final_preds = []\n",
    "#         pred_tokens = output.argmax(dim=-1)\n",
    "#         for i in range(pred_tokens.size(0)): # iterate on each sample\n",
    "#             pred = pred_tokens[i].unique(dim=-1).tolist()\n",
    "#             pred = self.tokenizer.decode(pred)\n",
    "#             res = re.search(r'C', pred)\n",
    "#             if res:\n",
    "#                 pred = pred[res.span()[0]:]\n",
    "#             final_preds.append(pred)\n",
    "#         return final_preds\n",
    "    \n",
    "# # encoder_net = Encoder()\n",
    "# # decoder_net = Decoder(len(vocab))\n",
    "# # encoder_out = encoder_net(imgs)\n",
    "# # print(\"Encoder =\", encoder_out.shape)\n",
    "# # print(inp_seqs.shape)\n",
    "# # pred_tokens = decoder_net(encoder_out, inp_seqs, tokenizer)\n",
    "# # print(pred_tokens.shape)\n",
    "# # pred_tokens = decoder_net.predict(encoder_out, tokenizer)\n",
    "# # pred_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-editor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "european-sheet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:56:45.386297Z",
     "iopub.status.busy": "2021-04-08T07:56:45.386085Z",
     "iopub.status.idle": "2021-04-08T07:56:45.430568Z",
     "shell.execute_reply": "2021-04-08T07:56:45.429965Z",
     "shell.execute_reply.started": "2021-04-08T07:56:45.386247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False, out_channels=512):\n",
    "        super().__init__()\n",
    "        last_stride = 2 if INP_SIZE[0] == 256 else 1\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.out_channels = out_channels\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.resout = nn.Sequential(\n",
    "            nn.Conv2d(512, out_channels, kernel_size=3, stride=last_stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.encout = nn.Sequential(\n",
    "            nn.Conv1d(16, 1, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.resout(out)\n",
    "        out = out.view(x.size(0), self.out_channels, -1)\n",
    "        out = out.permute(0,2,1)\n",
    "        out = self.encout(out).squeeze(1)\n",
    "#         print(\"ENC out =\", out.shape)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, enc_out_channels=512):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(vocab_size, EMB_SIZE)\n",
    "        self.lstm = nn.LSTM(enc_out_channels, vocab_size, batch_first=True)\n",
    "        self.encfc = nn.Linear(enc_out_channels, vocab_size)\n",
    "        self.embfc = nn.Linear(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, encoder_out, inp_seqs):\n",
    "        encoder_out = self.encfc(encoder_out)\n",
    "        encoder_out = encoder_out.unsqueeze(1)\n",
    "#         print(\"encoder_out =\", encoder_out.shape)\n",
    "        \n",
    "        emb = self.embd(inp_seqs)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "#         lstm_out = lstm_out.reshape(encoder_out.size(0), -1)\n",
    "#         print(\"lstm_out =\", lstm_out.shape)\n",
    "        enc_out = torch.repeat_interleave(encoder_out, lstm_out.size(1), dim=1)\n",
    "#         print(\"enc_out =\", enc_out.shape)\n",
    "        out = lstm_out + enc_out\n",
    "#         for i in range(lstm_out.size(1)):\n",
    "#             lstm_out[:,i,:] = self.embfc(lstm_out[:,i,:])\n",
    "#             lstm_out[:,i,:] = lstm_out[:,i,:] + encoder_out\n",
    "#         print(\"lstm_out after for =\", lstm_out.shape)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, encoder_out, tokenizer):\n",
    "#         print(encoder_out.shape)\n",
    "        encoder_out = self.encfc(encoder_out)\n",
    "        bs = encoder_out.size(0)\n",
    "        syn_inp_seqs = torch.tensor(tokenizer.vocab.ctoi(tokenizer.vocab.bos_token), device=encoder_out.device)\n",
    "        syn_inp_seqs = torch.repeat_interleave(syn_inp_seqs, bs)\n",
    "        syn_inp_seqs = syn_inp_seqs.view(bs, 1)\n",
    "        \n",
    "        # Predict next tokens to start token\n",
    "        pred_emb = []\n",
    "        for i in range(MAX_LEN):\n",
    "            emb = self.embd(syn_inp_seqs)\n",
    "            pred, _ = self.lstm(emb)\n",
    "            pred = pred.squeeze(1)\n",
    "            \n",
    "#             pred = self.embfc(pred)\n",
    "            pred = pred + encoder_out\n",
    "            pred = pred.unsqueeze(1)\n",
    "            pred_emb.append(pred)\n",
    "            syn_inp_seqs = pred.argmax(dim=-1)\n",
    "#         print(\"len =\", len(pred_emb))\n",
    "        pred_emb = torch.cat(pred_emb, dim=1)\n",
    "        return pred_emb\n",
    "            \n",
    "class InChINet(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_net = Encoder()\n",
    "        self.decoder_net = Decoder(vocab_size)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, imgs, inp_seqs):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "#         print(\"Encoder =\", encoder_out.shape)\n",
    "        pred_tokens = self.decoder_net(encoder_out, inp_seqs)\n",
    "        return pred_tokens\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = train_batch\n",
    "        output = self.forward(imgs, inp_seqs)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = val_batch\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        \n",
    "        lv_metric = self.calculate_lvdistance(output, inp_seqs)\n",
    "        self.logger.log_metrics({\"LvDistance\": lv_metric}, step=1)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, imgs, tokenizer):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "        pred_tokens = self.decoder_net.predict(encoder_out, tokenizer)\n",
    "        return pred_tokens\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.05),\n",
    "            \"monitor\": \"val_loss\",\n",
    "            'name': 'ReduceLROnPlateau'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def inference(self, imgs):\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        return self.postprocessing(output)\n",
    "    \n",
    "    def calculate_lvdistance(self, output, target):\n",
    "        pred_seqs = self.postprocessing(output)\n",
    "        batch_distance = np.mean([\n",
    "            Levenshtein.distance(pred_seq, self.tokenizer.decode(inp_seq))\n",
    "            for pred_seq, inp_seq in zip(pred_seqs, target)\n",
    "        ])\n",
    "        return batch_distance\n",
    "    \n",
    "    def postprocessing(self, output):\n",
    "        final_preds = []\n",
    "        pred_tokens = output.argmax(dim=-1)\n",
    "        for i in range(pred_tokens.size(0)): # iterate on each sample\n",
    "            pred = pred_tokens[i].unique(dim=-1).tolist()\n",
    "            pred = self.tokenizer.decode(pred)\n",
    "            res = re.search(r'C', pred)\n",
    "            if res:\n",
    "                pred = pred[res.span()[0]:]\n",
    "            final_preds.append(pred)\n",
    "        return final_preds\n",
    "    \n",
    "\n",
    "# dm = ImgToInChIDataModule(tb_logger=tb_logger)\n",
    "# dm.prepare_data(verbose=True)\n",
    "# dm.setup('fit')\n",
    "# imgs, inp_seqs, attn_masks = next(iter(dm.train_dataloader()))\n",
    "\n",
    "# encoder_net = Encoder()\n",
    "# encoder_out = encoder_net(imgs)\n",
    "# print(\"Encoder =\", encoder_out.shape)\n",
    "\n",
    "# decoder_net = Decoder(dm.vocab_size)\n",
    "# pred_tokens = decoder_net(encoder_out, inp_seqs)\n",
    "# print(pred_tokens.shape)\n",
    "\n",
    "# pred_tokens = decoder_net.predict(encoder_out, dm.tokenizer)\n",
    "# pred_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-sound",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "further-helicopter",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "numeric-superintendent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:56:46.600522Z",
     "iopub.status.busy": "2021-04-08T07:56:46.600084Z",
     "iopub.status.idle": "2021-04-08T07:59:20.169507Z",
     "shell.execute_reply": "2021-04-08T07:59:20.168912Z",
     "shell.execute_reply.started": "2021-04-08T07:56:46.600470Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder_net | Encoder          | 13.5 M\n",
      "1 | decoder_net | Decoder          | 2.2 M \n",
      "2 | loss_fn     | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "15.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.121    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8c0c57a28341e58be623ee08afdba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  153.34         \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  147.72         \t|1              \t|  147.72         \t|  96.332         \t|\n",
      "run_training_batch                 \t|  0.72196        \t|200            \t|  144.39         \t|  94.164         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.71669        \t|200            \t|  143.34         \t|  93.478         \t|\n",
      "training_step_and_backward         \t|  0.22039        \t|200            \t|  44.078         \t|  28.745         \t|\n",
      "model_forward                      \t|  0.19947        \t|200            \t|  39.894         \t|  26.017         \t|\n",
      "training_step                      \t|  0.19911        \t|200            \t|  39.822         \t|  25.97          \t|\n",
      "model_backward                     \t|  0.019449       \t|200            \t|  3.8897         \t|  2.5367         \t|\n",
      "get_train_batch                    \t|  0.012895       \t|200            \t|  2.579          \t|  1.6819         \t|\n",
      "evaluation_step_and_end            \t|  0.29324        \t|2              \t|  0.58648        \t|  0.38247        \t|\n",
      "validation_step                    \t|  0.2928         \t|2              \t|  0.5856         \t|  0.3819         \t|\n",
      "on_train_batch_end                 \t|  0.002685       \t|199            \t|  0.53432        \t|  0.34846        \t|\n",
      "on_train_end                       \t|  0.054705       \t|1              \t|  0.054705       \t|  0.035676       \t|\n",
      "on_train_start                     \t|  0.028384       \t|1              \t|  0.028384       \t|  0.01851        \t|\n",
      "cache_result                       \t|  2.7112e-05     \t|1015           \t|  0.027518       \t|  0.017946       \t|\n",
      "on_validation_batch_end            \t|  0.0041622      \t|2              \t|  0.0083244      \t|  0.0054288      \t|\n",
      "on_batch_start                     \t|  3.8795e-05     \t|200            \t|  0.007759       \t|  0.00506        \t|\n",
      "on_train_batch_start               \t|  2.5598e-05     \t|200            \t|  0.0051195      \t|  0.0033387      \t|\n",
      "on_after_backward                  \t|  2.4597e-05     \t|200            \t|  0.0049193      \t|  0.0032081      \t|\n",
      "on_before_zero_grad                \t|  2.2931e-05     \t|200            \t|  0.0045861      \t|  0.0029908      \t|\n",
      "on_batch_end                       \t|  2.0213e-05     \t|199            \t|  0.0040224      \t|  0.0026232      \t|\n",
      "training_step_end                  \t|  1.6211e-05     \t|200            \t|  0.0032421      \t|  0.0021143      \t|\n",
      "on_epoch_start                     \t|  0.0018299      \t|1              \t|  0.0018299      \t|  0.0011934      \t|\n",
      "on_validation_end                  \t|  0.00049545     \t|1              \t|  0.00049545     \t|  0.00032311     \t|\n",
      "on_validation_batch_start          \t|  6.1987e-05     \t|2              \t|  0.00012397     \t|  8.0849e-05     \t|\n",
      "validation_step_end                \t|  1.7791e-05     \t|2              \t|  3.5582e-05     \t|  2.3205e-05     \t|\n",
      "on_validation_start                \t|  2.2924e-05     \t|1              \t|  2.2924e-05     \t|  1.495e-05      \t|\n",
      "on_train_epoch_start               \t|  2.0216e-05     \t|1              \t|  2.0216e-05     \t|  1.3184e-05     \t|\n",
      "on_fit_start                       \t|  1.7018e-05     \t|1              \t|  1.7018e-05     \t|  1.1098e-05     \t|\n",
      "on_validation_epoch_end            \t|  1.4599e-05     \t|1              \t|  1.4599e-05     \t|  9.5207e-06     \t|\n",
      "on_validation_epoch_start          \t|  1.2002e-05     \t|1              \t|  1.2002e-05     \t|  7.8271e-06     \t|\n",
      "on_epoch_end                       \t|  1.0133e-05     \t|1              \t|  1.0133e-05     \t|  6.6082e-06     \t|\n",
      "on_before_accelerator_backend_setup\t|  9.291e-06      \t|1              \t|  9.291e-06      \t|  6.0591e-06     \t|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %tensorboard --logdir {CHKPTDIR}\n",
    "\n",
    "# dm = ImgToInChIDataModule(tb_logger=tb_logger)\n",
    "# dm.prepare_data(verbose=True)\n",
    "\n",
    "model = InChINet(dm.vocab_size, dm.tokenizer)\n",
    "# Add network graph to tensorboard\n",
    "# tb_logger.log_graph(model, [imgs[0].unsqueeze(0).to(model.device), inp_seqs[0].unsqueeze(0).to(model.device)])\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, auto_lr_find=True, max_epochs=2, precision=PRECISION, profiler=\"simple\", \n",
    "                     default_root_dir=CHKPTDIR, logger=tb_logger, callbacks=[lr_monitor])\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "impaired-emission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T07:59:20.207441Z",
     "iopub.status.busy": "2021-04-08T07:59:20.207209Z",
     "iopub.status.idle": "2021-04-08T07:59:20.886104Z",
     "shell.execute_reply": "2021-04-08T07:59:20.881264Z",
     "shell.execute_reply.started": "2021-04-08T07:59:20.207413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/474 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-89ce437ba28f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_masks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "total_distance = []\n",
    "for (imgs, inp_seqs, attn_masks) in progress_bar(dm.val_dataloader()):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    imgs = imgs.to(device)\n",
    "    pred_seqs = model.inference(imgs)\n",
    "    batch_distance = [\n",
    "        Levenshtein.distance(pred_seq, dm.tokenizer.decode(inp_seq))\n",
    "        for pred_seq, inp_seq in zip(pred_seqs, inp_seqs)\n",
    "    ]\n",
    "    total_distance += batch_distance\n",
    "np.mean(total_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-hollywood",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-08T07:41:23.840371Z",
     "iopub.status.idle": "2021-04-08T07:41:23.840803Z",
     "shell.execute_reply": "2021-04-08T07:41:23.840606Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.3149080108901905"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt)",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06030de4a60247398210eca34582487a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_379533be51b6486193d0427b652aa331",
       "style": "IPY_MODEL_d0cd1b7a6b214a7aaf864e54f69533ef",
       "value": "Epoch 0:   2%"
      }
     },
     "0c97a9b8dedc46db9125423e4740b105": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9868685c3b2f4e5580b0ae3eeda00c12",
       "style": "IPY_MODEL_823675af6e934d0ca262d29673911ef4",
       "value": " 199/9470 [02:28&lt;1:55:36,  1.34it/s, loss=1.76, v_num=6]"
      }
     },
     "0e0c7bcb334f4a17a6abb9a8503cb09e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "164298a8a1db41d3ba41a051c30ef9a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1cffabb13af1410e9ae2937f6fb5ff43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1d24157643734ee8a6d10a4f07763df7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1e957d87e78440daaa8123f9ca1775b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "292cf469a58943b89e15bda39650317a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2f976bd366784966bff475c29806bf7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_8d995386429744b4a412a95aec076a95",
       "max": 2,
       "style": "IPY_MODEL_e3ff8a3a5ad04b1cbbdad80a37b1cb4d",
       "value": 2
      }
     },
     "3087d92810e44d49bc6953a1f1da4ee9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0e0c7bcb334f4a17a6abb9a8503cb09e",
       "style": "IPY_MODEL_913ca9f900674043a95d60a4eff357c6",
       "value": "Validation sanity check: 100%"
      }
     },
     "34cf2bb3a52f4d3cb1b40fa5b8a989de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7641a7276e8e4254a32e24638eb87a1f",
       "style": "IPY_MODEL_7127f5a9f1cb45d7883099669de93f58",
       "value": "Epoch 0:   7%"
      }
     },
     "379533be51b6486193d0427b652aa331": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3e8288ddb3d14ff5aa806b1a72081cf6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "42f69c95e9e7448fbbf333f2db34f671": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "4734e1183f6d434f88094bb749e95257": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ad4ce2cae268400b9ec0cf582a109f41",
       "style": "IPY_MODEL_7cfa94d1ed1a4114aaf857fad6f547b1",
       "value": " 689/9470 [08:56&lt;1:54:00,  1.28it/s, loss=1.45, v_num=6]"
      }
     },
     "5404b6ef898c40ce9a291e685395fcec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cc48d5cbb59c4ef1b0bd2164989b4e94",
       "style": "IPY_MODEL_cb7636f96b3f467e86bc0b093283c23f",
       "value": "Validation sanity check:   0%"
      }
     },
     "5b700375ea2e4654a4ddfb36148517c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_34cf2bb3a52f4d3cb1b40fa5b8a989de",
        "IPY_MODEL_e79bd458d9154c03ac64ed6d5300a46c",
        "IPY_MODEL_4734e1183f6d434f88094bb749e95257"
       ],
       "layout": "IPY_MODEL_b0a6ec43e7e94a68a8d741696e6225b0"
      }
     },
     "5c1575f3a2b84f1d86e68c66ad0351ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "6385f71990304322980b068cffcb402b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "7127f5a9f1cb45d7883099669de93f58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7641a7276e8e4254a32e24638eb87a1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7cfa94d1ed1a4114aaf857fad6f547b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "823675af6e934d0ca262d29673911ef4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8d995386429744b4a412a95aec076a95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "8da34974c5ed491fa30dccc00e9a1db3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_daec5368bfdf48eb97871ac1903fd720",
       "max": 2,
       "style": "IPY_MODEL_a0bb0b9e989649f3860b60c6440932a7",
       "value": 2
      }
     },
     "8dc475cdb08d40688936480c89f9635c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_968a3a495ca84e7aa90a43d6c168192c",
       "style": "IPY_MODEL_164298a8a1db41d3ba41a051c30ef9a4",
       "value": " 2/2 [00:02&lt;00:00,  1.20s/it]"
      }
     },
     "913ca9f900674043a95d60a4eff357c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "94f8e470faf6450387713eee8b1ac6c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5404b6ef898c40ce9a291e685395fcec",
        "IPY_MODEL_d4c1b557b1d24c2ebec63ba85c98bbd3",
        "IPY_MODEL_e6faf8d6003d482ea027b5741680bd39"
       ],
       "layout": "IPY_MODEL_bbfa69ba28f244ccac99ef5d40b938d2"
      }
     },
     "95e15c34f63a4fc29dcca8b359ccb85c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c440f11e892d4ab0a4547673aca2d25d",
       "style": "IPY_MODEL_cbd2d8582811406f85fb2c58623b3eae",
       "value": " 2/2 [00:02&lt;00:00,  1.21s/it]"
      }
     },
     "968a3a495ca84e7aa90a43d6c168192c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9868685c3b2f4e5580b0ae3eeda00c12": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9f8fa6528ea54673baadae327cb8064a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a0bb0b9e989649f3860b60c6440932a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ab8c0c57a28341e58be623ee08afdba5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_06030de4a60247398210eca34582487a",
        "IPY_MODEL_ed31e91e26e847e689b669dfd513f2a5",
        "IPY_MODEL_0c97a9b8dedc46db9125423e4740b105"
       ],
       "layout": "IPY_MODEL_5c1575f3a2b84f1d86e68c66ad0351ed"
      }
     },
     "ad4ce2cae268400b9ec0cf582a109f41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b0a6ec43e7e94a68a8d741696e6225b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "b2aef33b16974c5d83e36b877b434fd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "bbfa69ba28f244ccac99ef5d40b938d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "c440f11e892d4ab0a4547673aca2d25d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c59fa6053e134a07a47252516bf81076": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cb7636f96b3f467e86bc0b093283c23f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cbd2d8582811406f85fb2c58623b3eae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cc48d5cbb59c4ef1b0bd2164989b4e94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d0cd1b7a6b214a7aaf864e54f69533ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d4c1b557b1d24c2ebec63ba85c98bbd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_42f69c95e9e7448fbbf333f2db34f671",
       "max": 2,
       "style": "IPY_MODEL_f380bfed66be41269c326ae94d0f7e95"
      }
     },
     "d984f96433b14dbda125b8962cf82f55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "daec5368bfdf48eb97871ac1903fd720": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "db9298abd83040d99e787db98cb88650": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1cffabb13af1410e9ae2937f6fb5ff43",
       "style": "IPY_MODEL_292cf469a58943b89e15bda39650317a",
       "value": "Validation sanity check: 100%"
      }
     },
     "e3ff8a3a5ad04b1cbbdad80a37b1cb4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e6faf8d6003d482ea027b5741680bd39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d984f96433b14dbda125b8962cf82f55",
       "style": "IPY_MODEL_c59fa6053e134a07a47252516bf81076",
       "value": " 0/2 [00:00&lt;?, ?it/s]"
      }
     },
     "e79bd458d9154c03ac64ed6d5300a46c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_3e8288ddb3d14ff5aa806b1a72081cf6",
       "max": 9470,
       "style": "IPY_MODEL_1d24157643734ee8a6d10a4f07763df7",
       "value": 689
      }
     },
     "ed31e91e26e847e689b669dfd513f2a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_6385f71990304322980b068cffcb402b",
       "max": 9470,
       "style": "IPY_MODEL_9f8fa6528ea54673baadae327cb8064a",
       "value": 199
      }
     },
     "f380bfed66be41269c326ae94d0f7e95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
