{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civic-agriculture",
   "metadata": {},
   "source": [
    "## Try these two experiments first\n",
    "1. Create original input image to inchi string translator and get accuracy and loss\n",
    "2. Create inchi image to inchi string translator and get accuracy and loss\n",
    "\n",
    "Now, compare the two. If the accuracy of inchi image -> inchi string is significantly higher than the original image -> inchi string then think about ***reconstruction experiments*** from original image to inchi image. [try this then](https://www.google.com/search?channel=fs&client=ubuntu&q=converting+shapes+from+one+to+another+using+deep+learning).\n",
    "\n",
    "# InChI decoding \n",
    "[Source](https://link.springer.com/content/pdf/10.1186%2Fs13321-015-0068-4.pdf)\n",
    "\n",
    "1. **Skeletal connections layer** This layer prefixed with `/c` represents connections between skeletal atoms by listing the canonical numbers in the chain of connected atoms. \n",
    "2. ***branches are given in parentheses***\n",
    "3. The canonical atomic numbers, which are used throughout the InChI, are always given in the formula’s element order. i.e. precendence is given to element according to periodic table while numbering elements. For example, `/C10H16N5O13P3` (the beginning of InChI for adenosine triphosphate) implies that atoms numbered 1–10 are carbons, 11–15 arenitrogens, 16–28 are oxygens, and 29–31 are phosporus. Hydrogen atoms are not explicitly numbered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-concord",
   "metadata": {
    "tags": []
   },
   "source": [
    "## image to inchi string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unnecessary-material",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:16.374384Z",
     "iopub.status.busy": "2021-04-05T06:43:16.374162Z",
     "iopub.status.idle": "2021-04-05T06:43:16.377082Z",
     "shell.execute_reply": "2021-04-05T06:43:16.376388Z",
     "shell.execute_reply.started": "2021-04-05T06:43:16.374361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Special Packages\n",
    "# !pip install PeriodicElements\n",
    "# !pip install albumentations\n",
    "# !pip install timm\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arabic-solomon",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:16.378094Z",
     "iopub.status.busy": "2021-04-05T06:43:16.377901Z",
     "iopub.status.idle": "2021-04-05T06:43:16.394372Z",
     "shell.execute_reply": "2021-04-05T06:43:16.393840Z",
     "shell.execute_reply.started": "2021-04-05T06:43:16.378075Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fancy-indianapolis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:16.395470Z",
     "iopub.status.busy": "2021-04-05T06:43:16.395219Z",
     "iopub.status.idle": "2021-04-05T06:43:19.140813Z",
     "shell.execute_reply": "2021-04-05T06:43:19.140129Z",
     "shell.execute_reply.started": "2021-04-05T06:43:16.395439Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import torch, torchmetrics, timm, re, pickle, Levenshtein\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from fastprogress import progress_bar\n",
    "from typing import Optional, Union\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from elements import elements\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from preprocessing import preprocess_image\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "torch.manual_seed(manualSeed);\n",
    "\n",
    "# This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assumed-victoria",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.141989Z",
     "iopub.status.busy": "2021-04-05T06:43:19.141791Z",
     "iopub.status.idle": "2021-04-05T06:43:19.170462Z",
     "shell.execute_reply": "2021-04-05T06:43:19.169440Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.141957Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHKPTDIR = Path(\"TranslationChkpts\")\n",
    "DATADIR = \"data/bms-molecular-translation\"\n",
    "LABELS_CSV_PATH = \"data/train_labels.csv\"\n",
    "VOCAB_FILEPATH = CHKPTDIR/\"vocab.pt\"\n",
    "TRAINPATHS_PATH = CHKPTDIR/\"train_paths.feather\"\n",
    "TESTPATHS_PATH = CHKPTDIR/\"test_paths.feather\"\n",
    "CHKPTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tb_logger = pl.loggers.TensorBoardLogger(CHKPTDIR, name=\"InchINet\")\n",
    "\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 256\n",
    "PRECISION = 16\n",
    "MAX_LEN = 16 # computed using corpus - max([len(vocab.tokenize(c)) for c in corpus]) 9 + 1 pad + 2 enclosing tokens\n",
    "EMB_SIZE = 512\n",
    "HDN_SIZE = 10\n",
    "INP_SIZE = (128, 128)\n",
    "N_INP_CH = 1\n",
    "N_OUT_CH = 3\n",
    "LR = 1e-2\n",
    "EPOCHS = 10\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alleged-rochester",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.172986Z",
     "iopub.status.busy": "2021-04-05T06:43:19.172717Z",
     "iopub.status.idle": "2021-04-05T06:43:19.337043Z",
     "shell.execute_reply": "2021-04-05T06:43:19.336361Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.172957Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test  train  train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATADIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-station",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Block\n",
    "\n",
    "### LightningDataModule API\n",
    "\n",
    "To define a DataModule define 5 methods:\n",
    "1. prepare_data (how to download(), tokenize, etc…)\n",
    "2. setup (how to split, etc…)\n",
    "3. train_dataloader\n",
    "4. val_dataloader(s)\n",
    "5. test_dataloader(s)\n",
    "\n",
    "#### prepare_data\n",
    "Use this method to do things that might write to disk or that need to be done only from a single process in distributed settings.\n",
    "1. download\n",
    "2. tokenize\n",
    "3. etc…\n",
    "\n",
    "#### setup\n",
    "There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "1. count number of classes\n",
    "2. build vocabulary\n",
    "3. perform train/val/test splits\n",
    "4. apply transforms (defined explicitly in your datamodule or assigned in init)\n",
    "5. etc…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-porter",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vocab and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "simple-chick",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.338826Z",
     "iopub.status.busy": "2021-04-05T06:43:19.338570Z",
     "iopub.status.idle": "2021-04-05T06:43:19.368543Z",
     "shell.execute_reply": "2021-04-05T06:43:19.367986Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.338791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RawDataset(Dataset):\n",
    "    def __init__(self, datadir, df=None):\n",
    "        super().__init__()\n",
    "        self.paths = list(Path(datadir).rglob(\"*.*\"))\n",
    "        if df is not None:\n",
    "            self.idtoinchi_dict = {\n",
    "                _id:_inchi for _id, _inchi in\n",
    "                zip(df[\"image_id\"].values.tolist(), df[\"InChI\"].values.tolist())\n",
    "            }\n",
    "        if len(self.paths) == 0:\n",
    "            print(\"No paths found.\")\n",
    "        self.piltotensor = PILToTensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imgpath = self.paths[idx]\n",
    "        imgid = imgpath.stem\n",
    "        img = preprocess_image(imgpath, out_size=INP_SIZE)\n",
    "        img = torch.from_numpy(np.array(img))\n",
    "        if hasattr(self, \"idtoinchi_dict\"):\n",
    "            target = self.idtoinchi_dict[imgid]\n",
    "            return img, target\n",
    "        return img, \"test_placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "considerable-fishing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.369440Z",
     "iopub.status.busy": "2021-04-05T06:43:19.369295Z",
     "iopub.status.idle": "2021-04-05T06:43:19.402110Z",
     "shell.execute_reply": "2021-04-05T06:43:19.401620Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.369421Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab, add_special_tokens=True):\n",
    "        self.vocab = vocab\n",
    "        # Get all elements sorted according to Atomic number\n",
    "        data = elements.Elements\n",
    "        # All elements in periodic table\n",
    "        self.elements = sorted(data, key=lambda i:i.AtomicNumber)  # Based on their AtomicNumber\n",
    "        # Sort longer names first for regex pattern formation\n",
    "        self.element_symbols = sorted([e.Symbol for e in self.elements], key=lambda e: len(e), reverse=True)\n",
    "        # Create regex pattern\n",
    "        self.pattern = f\"({'|'.join([f'{e}[0-9]*' for e in self.element_symbols])})\"\n",
    "        \n",
    "        if type(vocab) != type(None):\n",
    "            if add_special_tokens:\n",
    "                self.pad_token = \"<pad>\"\n",
    "                self.unk_token = \"<unk>\"\n",
    "                self.bos_token = \"<bos>\"\n",
    "                self.eos_token = \"<eos>\"\n",
    "                self.vocab = [self.pad_token, self.unk_token,self.bos_token,self.eos_token] + list(self.vocab)\n",
    "            \n",
    "            # Adding elements names into vocab and sort according to atomic number\n",
    "#             self.vocab = np.unique(self.vocab.tolist() + self.element_symbols)\n",
    "#             self.vocab = sorted(self.vocab.tolist(), key=lambda x: eval(f\"elements.{''.join(re.findall(r'[A-Za-z]', x))}.AtomicNumber\"))\n",
    "            # create class to index mapping\n",
    "            self._ctoi = defaultdict(lambda : self.unk_token, {c:i for i, c in enumerate(self.vocab)})\n",
    "                \n",
    "    def tokenize(self, string):\n",
    "        tokens = re.split(self.pattern, string)\n",
    "        tokens = list(filter(None, tokens))\n",
    "        return tokens\n",
    "    \n",
    "    def ctoi(self, c):\n",
    "        return self._ctoi[c]\n",
    "    \n",
    "    def itoc(self, i):\n",
    "        return self.vocab[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def save_vocab(self, path):\n",
    "        torch.save(self.vocab, path)\n",
    "        print(\"Saved @\", path)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        v = cls(None)\n",
    "        vocab = np.unique([w for s in corpus for w in v.tokenize(s)])\n",
    "        return cls(vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_vocab(cls, path):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab)\n",
    "    \n",
    "# Reference - https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#pretrainedtokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab=None):\n",
    "        self.vocab = vocab # Vocab class instance\n",
    "    \n",
    "    def tokenizer(self, x):\n",
    "        return self.vocab.tokenizer(x)\n",
    "    \n",
    "    def encode(self, s, max_len=None):\n",
    "        tokens = self.vocab.tokenize(s)\n",
    "        seq = [self.vocab.ctoi(t) for t in tokens]\n",
    "        attn_mask = [1]*len(seq)\n",
    "        \n",
    "        if max_len:\n",
    "            # Add padding to input\n",
    "            extra_len = max_len - len(seq) - 2 # 2 for start and end tokens\n",
    "            # Add start input token\n",
    "            seq = [self.vocab.ctoi(self.vocab.bos_token)] + seq\n",
    "            # Add end input token\n",
    "            seq += [self.vocab.ctoi(self.vocab.eos_token)]\n",
    "            attn_mask += [1, 1]\n",
    "            # Add padding token\n",
    "            seq += [self.vocab.ctoi(self.vocab.pad_token)]*extra_len\n",
    "            attn_mask += [0]*extra_len\n",
    "            \n",
    "        return {\"inp_seq\": seq, \"attn_mask\": attn_mask}\n",
    "    \n",
    "    def decode(self, tokens, inp_seq_name=\"inp_seq\"):\n",
    "        if isinstance(tokens, dict):\n",
    "            seq = tokens[inp_seq_name]\n",
    "            if isinstance(seq, (torch.Tensor, np.ndarray)):\n",
    "                seq = seq.tolist()\n",
    "        else:\n",
    "            seq = tokens\n",
    "        seq = ''.join([self.vocab.itoc(t) for t in seq])\n",
    "        # remove special tokens\n",
    "        for special_token in [self.vocab.pad_token, self.vocab.bos_token, self.vocab.eos_token]:\n",
    "            seq = seq.replace(special_token, '')\n",
    "        return seq\n",
    "    \n",
    "    @classmethod\n",
    "    def fit(cls, corpus):\n",
    "        vocab = Vocab.from_corpus(orcpus)\n",
    "        return cls(vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, path):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-scratch",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thirty-dispute",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.403049Z",
     "iopub.status.busy": "2021-04-05T06:43:19.402902Z",
     "iopub.status.idle": "2021-04-05T06:43:19.442626Z",
     "shell.execute_reply": "2021-04-05T06:43:19.442023Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.403030Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImgtoInChIDataset(Dataset):\n",
    "    def __init__(self, paths, df=None, tsfms=None):\n",
    "        self.paths = paths\n",
    "        if df is not None:\n",
    "            self.idtoinchi_dict = {\n",
    "                _id:_inchi for _id, _inchi in\n",
    "                zip(df[\"image_id\"].values.tolist(), df[\"InChI\"].values.tolist())\n",
    "            }\n",
    "        self.tsfms = tsfms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imgpath = self.paths[idx]\n",
    "        imgid = Path(imgpath).stem\n",
    "        img = np.array(preprocess_image(imgpath, out_size=INP_SIZE), dtype=np.float32)/255.\n",
    "        if self.tsfms is not None:\n",
    "            img = self.tsfms(image=img)[\"image\"]\n",
    "        \n",
    "        if hasattr(self, \"idtoinchi_dict\"):\n",
    "            target = self.idtoinchi_dict[imgid]\n",
    "            target = target.split(\"/\")[1]\n",
    "            return img, target\n",
    "        \n",
    "        return img, \"test_placeholder\"\n",
    "    \n",
    "class ImgToInChIDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tb_logger, valset_ratio=0.05) -> None:\n",
    "        super().__init__()\n",
    "        self.tb_logger = tb_logger\n",
    "        self.valset_ratio = valset_ratio\n",
    "        self.dims = (1, *INP_SIZE)\n",
    "        \n",
    "        self.train_tsfms = A.Compose([\n",
    "#             A.Resize(*INP_SIZE, always_apply=True),\n",
    "#             A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "#             A.RandomCrop(*INP_SIZE),\n",
    "#             A.RandomBrightnessContrast(p=0.5),\n",
    "#             A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.test_tsfms = A.Compose([\n",
    "#             A.Resize(*INP_SIZE, always_apply=True),\n",
    "#             A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self, verbose=False):\n",
    "        \"\"\"Use this method to do things that might write to disk or that\n",
    "        need to be done only from a single process in distributed settings.\"\"\"\n",
    "        # Load labels in DataFrame\n",
    "        if verbose: print(\"Loading labels data...\", end=' ')\n",
    "        self.df = pd.read_csv(LABELS_CSV_PATH)\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Load image paths\n",
    "        if verbose: print(\"Loading paths...\", end=' ')\n",
    "        if TRAINPATHS_PATH.exists():\n",
    "            self.train_paths = pd.read_feather(TRAINPATHS_PATH)\n",
    "            self.train_paths = self.train_paths.train_paths.tolist()\n",
    "        else:\n",
    "            self.train_paths = pd.DataFrame(list((Path(DATADIR)/\"train\").rglob(\"*.*\")), columns=[\"train_paths\"])\n",
    "            self.train_paths = self.train_paths.applymap(lambda x: str(x))\n",
    "            self.train_paths.to_feather(TRAINPATHS_PATH)\n",
    "        if TESTPATHS_PATH.exists():\n",
    "            self.test_paths = pd.read_feather(TESTPATHS_PATH)\n",
    "            self.test_paths = self.test_paths.test_paths.tolist()\n",
    "        else:\n",
    "            self.test_paths = pd.DataFrame(list((Path(DATADIR)/\"test\").rglob(\"*.*\")), columns=[\"test_paths\"])\n",
    "            self.test_paths = self.test_paths.applymap(lambda x: str(x))\n",
    "            self.test_paths.to_feather(TESTPATHS_PATH)\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Get Vocab and Tokenizer\n",
    "        if verbose: print(\"Loading vocab and tokenizer...\", end=' ')\n",
    "        if Path(VOCAB_FILEPATH).exists():\n",
    "            vocab = Vocab.load_vocab(VOCAB_FILEPATH)\n",
    "        else:\n",
    "            corpus = [s.split(\"/\")[1] for s in self.df.InChI.tolist()]\n",
    "            vocab = Vocab.from_corpus(corpus)\n",
    "#             print(\"# words =\", len(vocab))\n",
    "            vocab.save_vocab(VOCAB_FILEPATH)\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.tokenizer = Tokenizer(vocab)\n",
    "        if verbose: print(\"DONE!\")\n",
    "                \n",
    "    def setup(self, stage:Optional[str]=None) -> None:\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            trainpaths, valpaths = train_test_split(self.train_paths, test_size=self.valset_ratio)\n",
    "            self.trainset = ImgtoInChIDataset(trainpaths, self.df, self.train_tsfms)\n",
    "            self.valset = ImgtoInChIDataset(valpaths, self.df, self.test_tsfms)\n",
    "            \n",
    "            # Sample batch\n",
    "            imgs, inp_seqs, attn_masks = next(iter(self.train_dataloader()))\n",
    "            self.tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.testset = ImgtoInChIDataset(self.test_paths, tsfms=self.test_tsfms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valset, BATCH_SIZE, shuffle=False, \n",
    "                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        imgs = torch.cat([ins[0].unsqueeze(0) for ins in batch])\n",
    "        targets = [ins[1] for ins in batch]\n",
    "        targets = [self.tokenizer.encode(t, MAX_LEN) for t in targets]\n",
    "        inp_seqs = torch.Tensor([t[\"inp_seq\"] for t in targets]).long()\n",
    "        attn_masks = torch.Tensor([t[\"attn_mask\"] for t in targets]).float()\n",
    "        return imgs, inp_seqs, attn_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "handled-probe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.443602Z",
     "iopub.status.busy": "2021-04-05T06:43:19.443429Z",
     "iopub.status.idle": "2021-04-05T06:43:19.467589Z",
     "shell.execute_reply": "2021-04-05T06:43:19.466856Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.443584Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imgs, inp_seqs, attn_masks = next(iter(trainloader))\n",
    "# tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
    "\n",
    "\n",
    "# print(\"SAMPLE BATCH =\", imgs.shape)\n",
    "# fig, axes = plt.subplots(4, 8, figsize=(18, 10))\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(imgs[i].squeeze(0), cmap=\"gray\")\n",
    "#     ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-thomas",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T07:15:59.920534Z",
     "iopub.status.busy": "2021-04-02T07:15:59.920240Z",
     "iopub.status.idle": "2021-04-02T07:15:59.923276Z",
     "shell.execute_reply": "2021-04-02T07:15:59.922767Z",
     "shell.execute_reply.started": "2021-04-02T07:15:59.920510Z"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "### At the time of sentence prediction can we use HMMs or [Beam Search](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#overview) to make better decisions?\n",
    "\n",
    "Use different LSTM layers for each inchi substring like /c /h\n",
    "```\n",
    ">>> rnn = nn.LSTM(10, 20, 2)\n",
    ">>> input = torch.randn(1, 16, 10)\n",
    ">>> h0 = torch.randn(2, 16, 20)\n",
    ">>> c0 = torch.randn(2, 16, 20)\n",
    "```\n",
    "change number of layers here to num sublayers\n",
    "\n",
    "\n",
    "[Model from here](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dietary-swimming",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.468598Z",
     "iopub.status.busy": "2021-04-05T06:43:19.468428Z",
     "iopub.status.idle": "2021-04-05T06:43:19.514959Z",
     "shell.execute_reply": "2021-04-05T06:43:19.514322Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.468580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.n_features = self.cnn.fc.in_features\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        features = self.cnn(x)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        return features\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention network for calculate attention value\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param attention_dim: input size of attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"Decoder network with attention network used for training\"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: input size of attention network\n",
    "        :param embed_dim: input size of embedding network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param vocab_size: total number of characters used in training\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        :param encoder_out: output of encoder network\n",
    "        :param encoded_captions: transformed sequence from character to integer\n",
    "        :param caption_lengths: length of transformed sequence\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        # embedding transformed sequence for vector\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        # set decode length by caption length - 1 because of omitting start token\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        # embed start tocken for LSTM input\n",
    "        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n",
    "        embeddings = self.embedding(start_tockens)\n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n",
    "        # predict sequence\n",
    "        for t in range(decode_lengths):\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                (h, c))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:, t, :] = preds\n",
    "            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "automated-motor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.516039Z",
     "iopub.status.busy": "2021-04-05T06:43:19.515857Z",
     "iopub.status.idle": "2021-04-05T06:43:19.541160Z",
     "shell.execute_reply": "2021-04-05T06:43:19.540346Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.516018Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder_net = Encoder()\n",
    "# encoder_net(imgs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "irish-charge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.543391Z",
     "iopub.status.busy": "2021-04-05T06:43:19.542928Z",
     "iopub.status.idle": "2021-04-05T06:43:19.588777Z",
     "shell.execute_reply": "2021-04-05T06:43:19.588291Z",
     "shell.execute_reply.started": "2021-04-05T06:43:19.543324Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False, out_channels=512):\n",
    "        super().__init__()\n",
    "        last_stride = 2 if INP_SIZE[0] == 256 else 1\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.out_channels = out_channels\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.outfc = nn.Sequential(\n",
    "            nn.Conv2d(512, out_channels, kernel_size=3, stride=last_stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(4,1,ceil_mode=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.outfc(out)\n",
    "        out = out.view(x.size(0), self.out_channels, -1)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        \n",
    "#         out = self.maxpool(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, enc_out_channels=512, device=torch.device(\"cuda\")):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.embd = nn.Embedding(vocab_size, EMB_SIZE)\n",
    "        \n",
    "        self.init_h = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial hidden state\n",
    "        self.init_c = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial cell state\n",
    "        self.lstm = nn.LSTM(enc_out_channels, vocab_size, batch_first=True)\n",
    "        self.decfc = nn.Linear(64, MAX_LEN)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.device = device\n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, encoder_out, inp_seqs):\n",
    "#         print(\"inp_seq before emb =\", inp_seqs.shape)\n",
    "        emb = self.embd(inp_seqs)\n",
    "#         print(f\"emb = {emb.shape}, encoder_out = {encoder_out.shape}\")\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        out = lstm_out + encoder_out\n",
    "        return out\n",
    "    \n",
    "    def predict(self, encoder_out, tokenizer):\n",
    "        bs = encoder_out.size(0)\n",
    "        syn_inp_seqs = torch.tensor(tokenizer.vocab.ctoi(tokenizer.vocab.bos_token), device=encoder_out.device)\n",
    "        syn_inp_seqs = torch.repeat_interleave(syn_inp_seqs, bs)\n",
    "        syn_inp_seqs = syn_inp_seqs.view(bs, 1)\n",
    "#         print(\"syn_inp_seqs before emb =\", syn_inp_seqs.shape)\n",
    "        \n",
    "        # Predict next tokens to start token\n",
    "        pred_emb = []\n",
    "        for i in range(MAX_LEN):\n",
    "            emb = self.embd(syn_inp_seqs)\n",
    "            pred, _ = self.lstm(emb)\n",
    "            pred_emb.append(pred)\n",
    "            syn_inp_seqs = pred.argmax(dim=-1)\n",
    "#         print(\"len =\", len(pred_emb))\n",
    "        pred_emb = torch.cat(pred_emb, dim=1)\n",
    "#         print(\"pred_emb =\", pred_emb.shape)\n",
    "        out = pred_emb + encoder_out\n",
    "        return out\n",
    "            \n",
    "class InChINet(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_net = Encoder(out_channels=vocab_size)\n",
    "        self.decoder_net = Decoder(vocab_size)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, imgs, inp_seqs):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "#         print(\"Encoder =\", encoder_out.shape)\n",
    "        pred_tokens = self.decoder_net(encoder_out, inp_seqs)\n",
    "        return pred_tokens\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = train_batch\n",
    "        output = self.forward(imgs, inp_seqs)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks = val_batch\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        \n",
    "        lv_metric = self.calculate_lvdistance(output, inp_seqs)\n",
    "        self.logger.log_metrics({\"LvDistance\": lv_metric}, step=1)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, imgs, tokenizer):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "        pred_tokens = self.decoder_net.predict(encoder_out, tokenizer)\n",
    "        return pred_tokens\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100),\n",
    "            'name': 'AnnealingLR'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def inference(self, imgs):\n",
    "        output = self.predict(imgs, self.tokenizer)\n",
    "        return self.postprocessing(output)\n",
    "    \n",
    "    def calculate_lvdistance(self, output, target):\n",
    "        pred_seqs = self.postprocessing(output)\n",
    "        batch_distance = np.mean([\n",
    "            Levenshtein.distance(pred_seq, self.tokenizer.decode(inp_seq))\n",
    "            for pred_seq, inp_seq in zip(pred_seqs, target)\n",
    "        ])\n",
    "        return batch_distance\n",
    "    \n",
    "    def postprocessing(self, output):\n",
    "        final_preds = []\n",
    "        pred_tokens = output.argmax(dim=-1)\n",
    "        for i in range(pred_tokens.size(0)): # iterate on each sample\n",
    "            pred = pred_tokens[i].unique(dim=-1).tolist()\n",
    "            pred = self.tokenizer.decode(pred)\n",
    "            res = re.search(r'C', pred)\n",
    "            if res:\n",
    "                pred = pred[res.span()[0]:]\n",
    "            final_preds.append(pred)\n",
    "        return final_preds\n",
    "    \n",
    "# encoder_net = Encoder()\n",
    "# decoder_net = Decoder(len(vocab))\n",
    "# encoder_out = encoder_net(imgs)\n",
    "# print(\"Encoder =\", encoder_out.shape)\n",
    "# print(inp_seqs.shape)\n",
    "# pred_tokens = decoder_net(encoder_out, inp_seqs, tokenizer)\n",
    "# print(pred_tokens.shape)\n",
    "# pred_tokens = decoder_net.predict(encoder_out, tokenizer)\n",
    "# pred_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-netherlands",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-branch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-05T06:43:19.589737Z",
     "iopub.status.busy": "2021-04-05T06:43:19.589577Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 106034), started 0:21:59 ago. (Use '!kill 106034' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1913f4a381a48583\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1913f4a381a48583\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels data... DONE!\n",
      "Loading paths... DONE!\n",
      "Loading vocab and tokenizer... DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder_net | Encoder          | 13.2 M\n",
      "1 | decoder_net | Decoder          | 1.9 M \n",
      "2 | loss_fn     | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "15.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 M    Total params\n",
      "60.055    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3f6238f5094491a7031bd1db36c5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f198a2919cd049dbb507aca422e360c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {CHKPTDIR}\n",
    "\n",
    "dm = ImgToInChIDataModule(tb_logger=tb_logger)\n",
    "dm.prepare_data(verbose=True)\n",
    "model = InChINet(dm.vocab_size, dm.tokenizer)\n",
    "# Add network graph to tensorboard\n",
    "# tb_logger.log_graph(model, [imgs[0].unsqueeze(0).to(model.device), inp_seqs[0].unsqueeze(0).to(model.device)])\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, auto_lr_find=True, max_epochs=10, precision=PRECISION, profiler=\"simple\", \n",
    "                     default_root_dir=CHKPTDIR, logger=tb_logger, callbacks=[lr_monitor])\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-building",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "# total_distance = []\n",
    "# for (imgs, inp_seqs, attn_masks) in progress_bar(dm.val_dataloader()):\n",
    "#     model.eval()\n",
    "#     model = model.to(device)\n",
    "#     imgs = imgs.to(device)\n",
    "#     pred_seqs = model.inference(imgs)\n",
    "#     batch_distance = np.mean([\n",
    "#         Levenshtein.distance(pred_seq, dm.tokenizer.decode(inp_seq))\n",
    "#         for pred_seq, inp_seq in zip(pred_seqs, inp_seqs)\n",
    "#     ])\n",
    "#     print(batch_distance)\n",
    "#     total_distance += batch_distance\n",
    "# np.nanmean(total_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-culture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt)",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06ad266c1eb1466db105a9ee03055e91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "08a45e4a86a64a5a98345249035ecda1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0ca8ffc639e54136950df7095dc3662c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "196732ccccb540f395ab29f7d8e68097": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1b970216f43c4fd79b67b48cc580707b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a2685112ed8140d0915b8b14e9b83d2b",
       "style": "IPY_MODEL_196732ccccb540f395ab29f7d8e68097",
       "value": "Epoch 0:  95%"
      }
     },
     "2663e8aa1e4b4095ac69a2cd2752c1fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3e842b13b29b4cd98209b3d0f4642743": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "46aab4c07aba43a9b04af4c121abb5e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "50dd28ed2fd94e1da6f0bcfdfc8697e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "53448f24ee934bf59f4fc68abd71a459": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "56df4b7b601746b8a42937c5e564b16b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_08a45e4a86a64a5a98345249035ecda1",
       "style": "IPY_MODEL_8a1926bce32f4ad8be2954acedbe802d",
       "value": "Validation sanity check: 100%"
      }
     },
     "590c47ecfda4418dbb31a186fc8fa989": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_f3d8b1327b8349b984284f1e4d234cef",
       "max": 2,
       "style": "IPY_MODEL_aebf3ab25a1c45ff97a6af5b5de1f53e",
       "value": 2
      }
     },
     "5973637ab5404366abce7f620f48a7f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_c34698c8b78d4da9946c44b894d0aa4a",
       "max": 132,
       "style": "IPY_MODEL_06ad266c1eb1466db105a9ee03055e91",
       "value": 5
      }
     },
     "6271beb4c94247b5a0f47e31412d22dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6cdf5633b5f649d9a588182c4a62efaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "7b01008793ff42c0920bac699f5e5810": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ae354606080c483d9e548cb173e61651",
       "style": "IPY_MODEL_6271beb4c94247b5a0f47e31412d22dc",
       "value": "Validating:   4%"
      }
     },
     "7fe7b4c8dfc24d5eaeef4dee054a954f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0ca8ffc639e54136950df7095dc3662c",
       "style": "IPY_MODEL_e8d233904d684d06a7768e191d3205cf",
       "value": " 2/2 [00:03&lt;00:00,  1.31s/it]"
      }
     },
     "8326f624baf54f248601c706ec8f9f17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3e842b13b29b4cd98209b3d0f4642743",
       "style": "IPY_MODEL_2663e8aa1e4b4095ac69a2cd2752c1fd",
       "value": " 5/132 [00:06&lt;02:30,  1.19s/it]"
      }
     },
     "8a1926bce32f4ad8be2954acedbe802d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8d391c362a944cf1bdc4a0745e08195e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a2685112ed8140d0915b8b14e9b83d2b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a5b43b1804b5489a8c4a3ef797169d46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_46aab4c07aba43a9b04af4c121abb5e5",
       "style": "IPY_MODEL_e4b62840811b4e3eaeb1910c8587894f",
       "value": " 2504/2630 [35:21&lt;01:46,  1.18it/s, loss=0.163, v_num=1]"
      }
     },
     "ae354606080c483d9e548cb173e61651": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aebf3ab25a1c45ff97a6af5b5de1f53e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b693dd6faea14d5a95bc282a53c2fbb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "bf7b13c510fc4cc49b3bb761078b3c08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_6cdf5633b5f649d9a588182c4a62efaa",
       "max": 2630,
       "style": "IPY_MODEL_8d391c362a944cf1bdc4a0745e08195e",
       "value": 2504
      }
     },
     "c34698c8b78d4da9946c44b894d0aa4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "ca3f6238f5094491a7031bd1db36c5d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1b970216f43c4fd79b67b48cc580707b",
        "IPY_MODEL_bf7b13c510fc4cc49b3bb761078b3c08",
        "IPY_MODEL_a5b43b1804b5489a8c4a3ef797169d46"
       ],
       "layout": "IPY_MODEL_50dd28ed2fd94e1da6f0bcfdfc8697e9"
      }
     },
     "e4b62840811b4e3eaeb1910c8587894f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e8d233904d684d06a7768e191d3205cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f198a2919cd049dbb507aca422e360c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7b01008793ff42c0920bac699f5e5810",
        "IPY_MODEL_5973637ab5404366abce7f620f48a7f9",
        "IPY_MODEL_8326f624baf54f248601c706ec8f9f17"
       ],
       "layout": "IPY_MODEL_53448f24ee934bf59f4fc68abd71a459"
      }
     },
     "f3d8b1327b8349b984284f1e4d234cef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
