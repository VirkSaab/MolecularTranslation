{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intense-salad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This notebook translates images to inchi strings using single network for whole string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "failing-infrastructure",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T14:33:16.935636Z",
     "iopub.status.busy": "2021-04-30T14:33:16.935330Z",
     "iopub.status.idle": "2021-04-30T14:33:18.991490Z",
     "shell.execute_reply": "2021-04-30T14:33:18.990861Z",
     "shell.execute_reply.started": "2021-04-30T14:33:16.935597Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch, torchmetrics, timm, re, pickle, Levenshtein, math\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from fastprogress import progress_bar\n",
    "from typing import Optional, Union, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from preprocessing import preprocess_image\n",
    "\n",
    "from inchi_utils import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "torch.manual_seed(manualSeed);\n",
    "\n",
    "pl.seed_everything(manualSeed)\n",
    "\n",
    "# This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offensive-joining",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T14:33:18.992469Z",
     "iopub.status.busy": "2021-04-30T14:33:18.992307Z",
     "iopub.status.idle": "2021-04-30T14:33:19.016915Z",
     "shell.execute_reply": "2021-04-30T14:33:19.016372Z",
     "shell.execute_reply.started": "2021-04-30T14:33:18.992450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHKPTDIR = Path(\"TONE2Chkpts\")\n",
    "DATADIR = \"data/bms-molecular-translation\"\n",
    "LABELS_CSV_PATH = \"data/train_labels.csv\"\n",
    "VOCAB_FILEPATH  = CHKPTDIR/\"vocab_dict.pt\"\n",
    "TRAINPATHS_PATH = CHKPTDIR/\"train_paths.feather\"\n",
    "TESTPATHS_PATH  = CHKPTDIR/\"test_paths.feather\"\n",
    "MODEL_SAVEPATH  = CHKPTDIR/\"saved_model.ckpt\"\n",
    "CHKPTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tb_logger = pl.loggers.TensorBoardLogger(CHKPTDIR, name=\"OneInchINet\")\n",
    "\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 64\n",
    "PRECISION = 16\n",
    "TRUNCATE_SEQ_TO = None # Overriding max length\n",
    "EMB_SIZE = 256\n",
    "INP_SIZE = (128, 128)\n",
    "LR = 1e-3\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "arctic-connectivity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T14:33:19.017655Z",
     "iopub.status.busy": "2021-04-30T14:33:19.017501Z",
     "iopub.status.idle": "2021-04-30T14:33:19.172268Z",
     "shell.execute_reply": "2021-04-30T14:33:19.171001Z",
     "shell.execute_reply.started": "2021-04-30T14:33:19.017635Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test  train  train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATADIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-karaoke",
   "metadata": {},
   "source": [
    "# Lightning Data Module\n",
    "### LightningDataModule API\n",
    "\n",
    "To define a DataModule define 5 methods:\n",
    "1. prepare_data (how to download(), tokenize, etc…)\n",
    "2. setup (how to split, etc…)\n",
    "3. train_dataloader\n",
    "4. val_dataloader(s)\n",
    "5. test_dataloader(s)\n",
    "\n",
    "#### prepare_data\n",
    "Use this method to do things that might write to disk or that need to be done only from a single process in distributed settings.\n",
    "1. download\n",
    "2. tokenize\n",
    "3. etc…\n",
    "\n",
    "#### setup\n",
    "There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "1. count number of classes\n",
    "2. build vocabulary\n",
    "3. perform train/val/test splits\n",
    "4. apply transforms (defined explicitly in your datamodule or assigned in init)\n",
    "5. etc…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "independent-separate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T14:33:19.174431Z",
     "iopub.status.busy": "2021-04-30T14:33:19.174089Z",
     "iopub.status.idle": "2021-04-30T14:33:19.234187Z",
     "shell.execute_reply": "2021-04-30T14:33:19.233702Z",
     "shell.execute_reply.started": "2021-04-30T14:33:19.174390Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImgtoInChIDataset(Dataset):\n",
    "    def __init__(self, paths:list, df:pd.DataFrame=None, tsfms:A.Compose=None) -> None:\n",
    "        self.paths = paths\n",
    "        if df is not None:\n",
    "            self.idtoinchi_dict = {\n",
    "                _id:_inchi for _id, _inchi in\n",
    "                zip(df[\"image_id\"].values.tolist(), df[\"InChI\"].values.tolist())\n",
    "            }\n",
    "        else: # Test time placeholder\n",
    "            self.idtoinchi_dict = defaultdict(lambda : \"test_placeholder\", {})\n",
    "            \n",
    "        self.tsfms = tsfms\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, str]:\n",
    "        imgpath = self.paths[idx]\n",
    "        imgid = Path(imgpath).stem\n",
    "        img = np.array(preprocess_image(imgpath, out_size=INP_SIZE), dtype=np.float32)/255.\n",
    "        if self.tsfms is not None:\n",
    "            img = self.tsfms(image=img)[\"image\"]\n",
    "        \n",
    "        target = self.idtoinchi_dict[imgid]\n",
    "        return img, target\n",
    "\n",
    "    \n",
    "class ImgToInChIDataModuleONE(pl.LightningDataModule):\n",
    "    def __init__(self, tb_logger, valset_ratio=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.tb_logger = tb_logger\n",
    "        self.valset_ratio = valset_ratio\n",
    "        self.dims = (1, *INP_SIZE)\n",
    "        \n",
    "        self.train_tsfms = A.Compose([\n",
    "            A.Flip(),\n",
    "#             A.Resize(*INP_SIZE, always_apply=True),\n",
    "#             A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "#             A.RandomCrop(*INP_SIZE),\n",
    "#             A.RandomBrightnessContrast(p=0.5),\n",
    "#             A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.test_tsfms = A.Compose([\n",
    "#             A.Resize(*INP_SIZE, always_apply=True),\n",
    "#             A.Normalize(mean=(0.5), std=(0.229)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self, verbose:bool=True) -> None:\n",
    "        \"\"\"Use this method to do things that might write to disk or that\n",
    "        need to be done only from a single process in distributed settings.\"\"\"\n",
    "        # Load labels in DataFrame\n",
    "        if verbose: print(\"Loading labels data...\", end=' ')\n",
    "        self.df = pd.read_csv(LABELS_CSV_PATH)\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Load image paths\n",
    "        if verbose: print(\"Loading paths...\", end=' ')\n",
    "        if TRAINPATHS_PATH.exists():\n",
    "            self.train_paths = pd.read_feather(TRAINPATHS_PATH)\n",
    "            self.train_paths = self.train_paths.train_paths.tolist()\n",
    "        else:\n",
    "            if verbose: print(\"Traning paths file not found. Creating...\", end=' ')\n",
    "            self.train_paths = pd.DataFrame(list((Path(DATADIR)/\"train\").rglob(\"*.*\")), columns=[\"train_paths\"])\n",
    "            self.train_paths = self.train_paths.applymap(lambda x: str(x))\n",
    "            self.train_paths.to_feather(TRAINPATHS_PATH)\n",
    "            self.train_paths = self.train_paths.train_paths.tolist()\n",
    "            if verbose: print(\"DONE!\")\n",
    "        if TESTPATHS_PATH.exists():\n",
    "            self.test_paths = pd.read_feather(TESTPATHS_PATH)\n",
    "            self.test_paths = self.test_paths.test_paths.tolist()\n",
    "        else:\n",
    "            if verbose: print(\"Test paths file not found. Creating...\", end=' ')\n",
    "            self.test_paths = pd.DataFrame(list((Path(DATADIR)/\"test\").rglob(\"*.*\")), columns=[\"test_paths\"])\n",
    "            self.test_paths = self.test_paths.applymap(lambda x: str(x))\n",
    "            self.test_paths.to_feather(TESTPATHS_PATH)\n",
    "            self.test_paths = self.test_paths.test_paths.tolist()\n",
    "            if verbose: print(\"DONE!\")\n",
    "        if verbose: print(\"DONE!\")\n",
    "        \n",
    "        # Get Vocab and Tokenizer\n",
    "        if verbose: print(\"Loading vocab and tokenizer...\", end=' ')\n",
    "        \n",
    "        if VOCAB_FILEPATH.exists():\n",
    "            self.tokenizer = Tokenizer.from_file(VOCAB_FILEPATH)\n",
    "        else:\n",
    "            if verbose: print(\"Vocab file not found. Creating...\", end=' ')\n",
    "            vocab = VocabONE.from_inchi_pandas_column(self.df.InChI)\n",
    "            vocab.save_vocab(VOCAB_FILEPATH)\n",
    "            self.tokenizer = Tokenizer(vocab)\n",
    "            if verbose: print(\"DONE!\")\n",
    "        \n",
    "        self.vocab_size = len(self.tokenizer.vocab)\n",
    "        if TRUNCATE_SEQ_TO != None:\n",
    "            self.max_len = TRUNCATE_SEQ_TO\n",
    "            self.tokenizer.vocab.max_len = TRUNCATE_SEQ_TO\n",
    "        else:  \n",
    "            self.max_len = self.tokenizer.vocab.max_len\n",
    "        if verbose: print(\"DONE!\")\n",
    "                \n",
    "    def setup(self, stage:Optional[str]=None) -> None:\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            trainpaths, valpaths = train_test_split(self.train_paths, test_size=self.valset_ratio)\n",
    "            self.trainset = ImgtoInChIDataset(trainpaths, self.df, self.train_tsfms)\n",
    "            self.valset = ImgtoInChIDataset(valpaths, self.df, self.test_tsfms)\n",
    "            \n",
    "#             # Sample batch\n",
    "#             imgs, inp_seqs, attn_masks, cap_lens = next(iter(self.train_dataloader()))\n",
    "#             self.tb_logger.experiment.add_images(\"Sample images\", imgs)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.testset = ImgtoInChIDataset(self.test_paths, tsfms=self.test_tsfms)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.trainset, BATCH_SIZE, shuffle=True, collate_fn=self.collate_fn, num_workers=N_WORKERS)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valset, BATCH_SIZE, shuffle=False, collate_fn=self.collate_fn, num_workers=N_WORKERS)\n",
    "    \n",
    "    def collate_fn(self, batch:tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        imgs = torch.cat([ins[0].unsqueeze(0) for ins in batch])\n",
    "        targets = [ins[1] for ins in batch]\n",
    "        targets = [self.tokenizer.encode(t, pad_to_custom_len=TRUNCATE_SEQ_TO) for t in targets]\n",
    "        \n",
    "        batch_inp_seqs = torch.tensor([t[\"inp_seq\"] for t in targets])\n",
    "        batch_attn_masks = torch.tensor([t[\"attn_mask\"] for t in targets])\n",
    "        batch_cap_lens = torch.tensor([t[\"cap_len\"] for t in targets])\n",
    "        \n",
    "        return imgs, batch_inp_seqs, batch_attn_masks, batch_cap_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "individual-remains",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T14:33:19.235957Z",
     "iopub.status.busy": "2021-04-30T14:33:19.235755Z",
     "iopub.status.idle": "2021-04-30T14:33:19.257876Z",
     "shell.execute_reply": "2021-04-30T14:33:19.257425Z",
     "shell.execute_reply.started": "2021-04-30T14:33:19.235938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# dm = ImgToInChIDataModuleONE(tb_logger)\n",
    "# dm.prepare_data()\n",
    "# dm.setup('fit')\n",
    "# imgs, inp_seqs, attn_masks, cap_lens = next(iter(dm.train_dataloader()))\n",
    "# imgs.shape, inp_seqs.shape, attn_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-builder",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model\n",
    "### At the time of sentence prediction can we use HMMs or [Beam Search](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#overview) to make better decisions?\n",
    "\n",
    "***Try this***:\n",
    "Use different LSTM layers for each inchi substring like /c /h\n",
    "```\n",
    ">>> rnn = nn.LSTM(10, 20, 2)\n",
    ">>> input = torch.randn(1, 16, 10)\n",
    ">>> h0 = torch.randn(2, 16, 20)\n",
    ">>> c0 = torch.randn(2, 16, 20)\n",
    "```\n",
    "change number of layers here to num sublayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "confused-offset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T15:23:36.661506Z",
     "iopub.status.busy": "2021-04-30T15:23:36.661269Z",
     "iopub.status.idle": "2021-04-30T15:23:38.518829Z",
     "shell.execute_reply": "2021-04-30T15:23:38.518259Z",
     "shell.execute_reply.started": "2021-04-30T15:23:36.661484Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder = torch.Size([256, 275, 128])\n",
      "@forward decoder_out = torch.Size([256, 152, 202])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', max_len=275, out_channels=512, pretrained=False):\n",
    "        super().__init__()\n",
    "#         n_out_pixels = 8*8 if INP_SIZE[0] == 256 else 4*4\n",
    "        self.out_channels = out_channels\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2, bias=False)\n",
    "        \n",
    "        self.out_channels, n_out_pixels = 128, 16*16\n",
    "        self.cnn.layer3 = nn.Identity()\n",
    "        self.cnn.layer4 = nn.Identity()\n",
    "        self.cnn.global_pool = nn.Identity()\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.encout = nn.Linear(n_out_pixels, max_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(x.size(0), self.out_channels, -1)\n",
    "        out = self.encout(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        return out\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/blob/master/models.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder.\n",
    "    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/blob/master/models.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim=256, embed_dim=256, decoder_dim=256, vocab_size=202, encoder_dim=128, dropout=0.2):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "#         print(\"caption_lengths before squeeze =\", caption_lengths.shape)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "#         print(\"encoded_captions =\", encoded_captions.shape)\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions.squeeze(1))  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "#         print(\"h, c, emb =\",h.shape, c.shape, embeddings.shape, encoded_captions.shape)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(encoder_out.device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(encoder_out.device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "#         print(\"max(decode_lengths) =\", max(decode_lengths))\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "#             print(t, attention_weighted_encoding.shape, embeddings.shape)\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "        \n",
    "class InChINetONE(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, max_len, tokenizer):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_net = Encoder(max_len=max_len-1)\n",
    "        self.decoder_net = DecoderWithAttention(vocab_size=vocab_size)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.alpha_c = 1.\n",
    "        \n",
    "    def forward(self, imgs, inp_seqs, cap_lens):\n",
    "        encoder_out = self.encoder_net(imgs)\n",
    "#         print(\"Encoder =\", encoder_out.shape)\n",
    "        output = self.decoder_net(encoder_out, inp_seqs, cap_lens)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        imgs, inp_seqs, attn_masks, cap_lens = train_batch\n",
    "        tgt_inp, tgt_out = inp_seqs[:, :-1].unsqueeze(1), inp_seqs[:, 1:]\n",
    "\n",
    "        output = self.forward(imgs, tgt_inp, cap_lens.unsqueeze(1))\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = output\n",
    "        targets = tgt_out[:, :scores.shape[1]]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn(scores.permute(0, 2, 1), targets)\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += self.alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            imgs, inp_seqs, attn_masks, cap_lens = val_batch\n",
    "        tgt_inp, tgt_out = inp_seqs[:, :-1].unsqueeze(1), inp_seqs[:, 1:]\n",
    "\n",
    "        output = self.forward(imgs, tgt_inp, cap_lens.unsqueeze(1))\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = output\n",
    "        targets = tgt_out[:, :scores.shape[1]]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn(scores.permute(0, 2, 1), targets)\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += self.alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LR)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100),\n",
    "            'name': 'AnnealingLR'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "\n",
    "# dm = ImgToInChIDataModuleONE(tb_logger=tb_logger)\n",
    "# dm.prepare_data(verbose=True)\n",
    "# dm.setup('fit')\n",
    "# imgs, inp_seqs, attn_masks, cap_lens = next(iter(dm.train_dataloader()))\n",
    "# print(\"inp_seqs =\", inp_seqs.shape)\n",
    "\n",
    "# # encoder_net = Encoder()\n",
    "# # encoder_out = encoder_net(imgs) \n",
    "# encoder_out = torch.rand([256, 275, 128])\n",
    "# print(\"Encoder =\", encoder_out.shape)\n",
    "\n",
    "# decoder_net = DecoderWithAttention(vocab_size=dm.vocab_size)\n",
    "# decoder_out = decoder_net(encoder_out, inp_seqs[:, 1:].unsqueeze(1), cap_lens.unsqueeze(1))\n",
    "# predictions, encoded_captions, decode_lengths, alphas, sort_ind = decoder_out\n",
    "# print(\"@forward decoder_out =\", predictions.shape)\n",
    "# decoder_out = decoder_net.predict(encoder_out, dm.tokenizer)\n",
    "# print(\"@predict decoder_out =\", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-algorithm",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "returning-duncan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T13:59:17.547181Z",
     "iopub.status.busy": "2021-04-30T13:59:17.546999Z",
     "iopub.status.idle": "2021-04-30T13:59:17.574643Z",
     "shell.execute_reply": "2021-04-30T13:59:17.574101Z",
     "shell.execute_reply.started": "2021-04-30T13:59:17.547159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name, params in encoder_net.cnn.named_parameters():\n",
    "#     if \"weight\" in name:\n",
    "#         print(params.requires_grad)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-firmware",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "flying-stupid",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T14:33:19.304251Z",
     "iopub.status.busy": "2021-04-30T14:33:19.304101Z",
     "iopub.status.idle": "2021-04-30T15:10:25.476266Z",
     "shell.execute_reply": "2021-04-30T15:10:25.475799Z",
     "shell.execute_reply.started": "2021-04-30T14:33:19.304216Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels data... DONE!\n",
      "Loading paths... DONE!\n",
      "Loading vocab and tokenizer... DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                 | Params\n",
      "-----------------------------------------------------\n",
      "0 | encoder_net | Encoder              | 746 K \n",
      "1 | decoder_net | DecoderWithAttention | 959 K \n",
      "2 | loss_fn     | CrossEntropyLoss     | 0     \n",
      "3 | softmax     | Softmax              | 0     \n",
      "-----------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.821     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ceb4c40657240479b133bb4e1b7e4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virk/miniconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/virk/miniconda3/envs/pt/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# %tensorboard --logdir {CHKPTDIR}\n",
    "\n",
    "dm = ImgToInChIDataModuleONE(tb_logger=tb_logger)\n",
    "dm.prepare_data(verbose=True)\n",
    "\n",
    "model = InChINetONE(dm.vocab_size, dm.max_len, dm.tokenizer)\n",
    "# Add network graph to tensorboard\n",
    "# tb_logger.log_graph(model, [imgs[0].unsqueeze(0).to(model.device), inp_seqs[0].unsqueeze(0).to(model.device)])\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "# checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "trainer = pl.Trainer(gpus=1, auto_lr_find=True, max_epochs=EPOCHS, precision=PRECISION, profiler=None,\n",
    "                     default_root_dir=CHKPTDIR, logger=tb_logger, callbacks=[lr_monitor])\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "trainer.save_checkpoint(MODEL_SAVEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-worse",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "italian-blogger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T15:19:01.960042Z",
     "iopub.status.busy": "2021-04-30T15:19:01.959835Z",
     "iopub.status.idle": "2021-04-30T15:19:03.258135Z",
     "shell.execute_reply": "2021-04-30T15:19:03.256758Z",
     "shell.execute_reply.started": "2021-04-30T15:19:01.960023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs, inp_seqs, attn_masks, cap_lens = next(iter(dm.val_dataloader()))\n",
    "tgt_out = inp_seqs[:, 1:]\n",
    "# Input <bos> as first token\n",
    "bs = inp_seqs.size(0)\n",
    "tgt_inp = torch.tensor(dm.tokenizer.vocab.ctoi(dm.tokenizer.vocab.bos_token), device=imgs.device)\n",
    "tgt_inp = torch.repeat_interleave(tgt_inp, bs)\n",
    "tgt_inp = tgt_inp.view(bs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "united-gateway",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T15:17:47.811218Z",
     "iopub.status.busy": "2021-04-30T15:17:47.810774Z",
     "iopub.status.idle": "2021-04-30T15:17:48.149054Z",
     "shell.execute_reply": "2021-04-30T15:17:48.148551Z",
     "shell.execute_reply.started": "2021-04-30T15:17:47.811164Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 276, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out = trainer.model.encoder_net(imgs)\n",
    "encoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "supported-light",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T15:25:55.024048Z",
     "iopub.status.busy": "2021-04-30T15:25:55.023299Z",
     "iopub.status.idle": "2021-04-30T15:25:55.071179Z",
     "shell.execute_reply": "2021-04-30T15:25:55.070484Z",
     "shell.execute_reply.started": "2021-04-30T15:25:55.023958Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_inp.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "isolated-climate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T15:25:27.039945Z",
     "iopub.status.busy": "2021-04-30T15:25:27.039222Z",
     "iopub.status.idle": "2021-04-30T15:25:27.093529Z",
     "shell.execute_reply": "2021-04-30T15:25:27.093017Z",
     "shell.execute_reply.started": "2021-04-30T15:25:27.039853Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 275, 128]), torch.Size([64, 1, 276]), torch.Size([64, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out.shape, inp_seqs[:, 1:].unsqueeze(1).shape, cap_lens.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "entire-assist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-30T15:26:02.548822Z",
     "iopub.status.busy": "2021-04-30T15:26:02.548641Z",
     "iopub.status.idle": "2021-04-30T15:26:02.615388Z",
     "shell.execute_reply": "2021-04-30T15:26:02.614279Z",
     "shell.execute_reply.started": "2021-04-30T15:26:02.548801Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-28689e9327e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-da0228ddc82a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_out, encoded_captions, caption_lengths)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;31m#             print(t, attention_weighted_encoding.shape, embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             h, c = self.decode_step(\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weighted_encoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n\u001b[1;32m    163\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size_t, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "output = model.decoder_net(encoder_out, tgt_inp.unsqueeze(-1), torch.tensor(dm.max_len).view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-street",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-electronics",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-30T14:10:38.513516Z",
     "iopub.status.idle": "2021-04-30T14:10:38.513969Z",
     "shell.execute_reply": "2021-04-30T14:10:38.513738Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in progress_bar(range(dm.max_len - 1)):\n",
    "    output = model.decoder_net(encoder_out, tgt_inp)\n",
    "    outputs.append(output)\n",
    "    print(i, output.shape)\n",
    "    tgt_inp = model.softmax(output).argmax(dim=-1)#[:, -1].unsqueeze(-1)\n",
    "#     print(\"pred_tgt_inp =\", pred_tgt_inp.shape)\n",
    "#     tgt_inp = torch.cat((tgt_inp, pred_tgt_inp), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-clark",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-30T14:10:38.515109Z",
     "iopub.status.idle": "2021-04-30T14:10:38.515519Z",
     "shell.execute_reply": "2021-04-30T14:10:38.515326Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "outputs = model.softmax(torch.cat(outputs, dim=1)).argmax(dim=-1)\n",
    "avg_distance = []\n",
    "for i in progress_bar(range(BATCH_SIZE-1)):\n",
    "    true = dm.tokenizer.decode(inp_seqs[i+1])\n",
    "    pred = dm.tokenizer.decode(outputs[i])\n",
    "    print(\"True =\", true)\n",
    "    print(\"Pred =\", pred)\n",
    "    print()\n",
    "#     print(\"Distance =\", Levenshtein.distance(true, pred))\n",
    "    avg_distance.append(Levenshtein.distance(true, pred))\n",
    "np.mean(avg_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt)",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0672d46cf3b849b78dec5ddec5e774f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_9abb65ae1a7e4a43ab368daf973b70fd",
       "max": 2,
       "style": "IPY_MODEL_8f92a9ebdfb84100b87a67593c399e1b",
       "value": 2
      }
     },
     "0ceb4c40657240479b133bb4e1b7e4ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_13a3a1d304274386a37f13279cbe406b",
        "IPY_MODEL_3ca871e796c94b45bd414657ab53e635",
        "IPY_MODEL_c2ec490568144a57931b16174a972840"
       ],
       "layout": "IPY_MODEL_c8b7565d9ff54a6085d40dfc36555eec"
      }
     },
     "134ac64d65834c55b72ebc3597d40f35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "13a3a1d304274386a37f13279cbe406b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fc65bcf4eddf4bed8624a708f0138733",
       "style": "IPY_MODEL_7e463f7aa8a64b83b5cc4e79af96c7e0",
       "value": "Epoch 0:   8%"
      }
     },
     "22946b1fd2f44ef9bfc399eacc547911": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "23bc820fac4742569fe29c0e7a059f81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "38c9ce1adb994c9aaa41679ea959699c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3ca871e796c94b45bd414657ab53e635": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_c33966004d3c40cf9dccc763be3e92c9",
       "max": 37879,
       "style": "IPY_MODEL_23bc820fac4742569fe29c0e7a059f81",
       "value": 3012
      }
     },
     "6a6375f97211491f8db1fbca8a3bae3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7e463f7aa8a64b83b5cc4e79af96c7e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "80a12c23015d48358090c50124a11d8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "8f92a9ebdfb84100b87a67593c399e1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "948a1f3618544302b8048049f371fc22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f26c47c38e1d412fadec604dda067571",
       "style": "IPY_MODEL_6a6375f97211491f8db1fbca8a3bae3a",
       "value": "Validation sanity check: 100%"
      }
     },
     "9abb65ae1a7e4a43ab368daf973b70fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "c2ec490568144a57931b16174a972840": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_38c9ce1adb994c9aaa41679ea959699c",
       "style": "IPY_MODEL_22946b1fd2f44ef9bfc399eacc547911",
       "value": " 3012/37879 [36:54&lt;7:07:19,  1.36it/s, loss=4.33, v_num=19]"
      }
     },
     "c33966004d3c40cf9dccc763be3e92c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "c8b7565d9ff54a6085d40dfc36555eec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "db15cebbd2a54cad9cb7f740d54694a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f39cf8a9a7124cbaa6f4046aef25b663",
       "style": "IPY_MODEL_134ac64d65834c55b72ebc3597d40f35",
       "value": " 2/2 [00:01&lt;00:00,  1.54it/s]"
      }
     },
     "f26c47c38e1d412fadec604dda067571": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f39cf8a9a7124cbaa6f4046aef25b663": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fc65bcf4eddf4bed8624a708f0138733": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
