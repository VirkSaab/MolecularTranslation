{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"031ba3d3bf9b406eb7d03d314d3da416":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"layout":"IPY_MODEL_c7cd48c2bf134ff38dbe21328b052d69","style":"IPY_MODEL_053cf750c2c3431ca17cb16f47cfff5d","value":" 2/2 [00:17&lt;00:00,  7.26s/it]"}},"053cf750c2c3431ca17cb16f47cfff5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"description_width":""}},"08edd4a4dac8435da1f9c6197d683bdf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"flex":"2"}},"102a428e665947458fbfd870350092b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"description_width":""}},"1b666bc167d947d794597b533e98c866":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{}},"2d2c7fd4380d46db879784757953fd1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"layout":"IPY_MODEL_a4b490fba2534ccaaef7bf073e1e7248","max":18939,"style":"IPY_MODEL_102a428e665947458fbfd870350092b5","value":19}},"35b137161a4f4ff5ae7f5892332be250":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"description_width":""}},"482fc38e0991409695dfea6af3f2e46f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"layout":"IPY_MODEL_08edd4a4dac8435da1f9c6197d683bdf","max":2,"style":"IPY_MODEL_9dc66f34550a4527b41af1cc232620c3","value":2}},"4d856a7430b24045b511753b28ee1342":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"layout":"IPY_MODEL_1b666bc167d947d794597b533e98c866","style":"IPY_MODEL_b2a40233e5dc4235a8b49238c8c5b428","value":"Validation sanity check: 100%"}},"5da178caef6d4bb3a2ff8ac217df145c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"layout":"IPY_MODEL_7bff32242cad4b0abb5bbc979a9668fc","style":"IPY_MODEL_35b137161a4f4ff5ae7f5892332be250","value":" 19/18939 [01:31&lt;25:26:03,  4.84s/it, loss=3.82, v_num=13]"}},"702be43d5633403080f936c91d742def":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"display":"inline-flex","flex_flow":"row wrap","width":"100%"}},"76d5960dedd74f90afa47e7ee4ca51f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"layout":"IPY_MODEL_eb2787c90f074bfc96ee4b1dab8c50b7","style":"IPY_MODEL_b569fc43df4b4ac284cc48e273b9c1f9","value":"Epoch 0:   0%"}},"7712295d93db40cbaaeafcaba6c9719a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"children":["IPY_MODEL_76d5960dedd74f90afa47e7ee4ca51f3","IPY_MODEL_2d2c7fd4380d46db879784757953fd1a","IPY_MODEL_5da178caef6d4bb3a2ff8ac217df145c"],"layout":"IPY_MODEL_e202b6ab57b24f9b850612edefbd113d"}},"7bff32242cad4b0abb5bbc979a9668fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{}},"9dc66f34550a4527b41af1cc232620c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"description_width":""}},"a4b490fba2534ccaaef7bf073e1e7248":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"flex":"2"}},"b2a40233e5dc4235a8b49238c8c5b428":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"description_width":""}},"b569fc43df4b4ac284cc48e273b9c1f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"description_width":""}},"c7cd48c2bf134ff38dbe21328b052d69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{}},"e202b6ab57b24f9b850612edefbd113d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"display":"inline-flex","flex_flow":"row wrap","width":"100%"}},"eb2787c90f074bfc96ee4b1dab8c50b7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Try these two experiments first\n1. Create original input image to inchi string translator and get accuracy and loss\n2. Create inchi image to inchi string translator and get accuracy and loss\n\nNow, compare the two. If the accuracy of inchi image -> inchi string is significantly higher than the original image -> inchi string then think about ***reconstruction experiments*** from original image to inchi image. [try this then](https://www.google.com/search?channel=fs&client=ubuntu&q=converting+shapes+from+one+to+another+using+deep+learning).\n\n# InChI decoding \n[Source](https://link.springer.com/content/pdf/10.1186%2Fs13321-015-0068-4.pdf)\n\n1. **Skeletal connections layer** This layer prefixed with `/c` represents connections between skeletal atoms by listing the canonical numbers in the chain of connected atoms. \n2. ***branches are given in parentheses***\n3. The canonical atomic numbers, which are used throughout the InChI, are always given in the formula’s element order. i.e. precendence is given to element according to periodic table while numbering elements. For example, `/C10H16N5O13P3` (the beginning of InChI for adenosine triphosphate) implies that atoms numbered 1–10 are carbons, 11–15 arenitrogens, 16–28 are oxygens, and 29–31 are phosporus. Hydrogen atoms are not explicitly numbered.\n","metadata":{}},{"cell_type":"markdown","source":"## image to inchi string","metadata":{"tags":[]}},{"cell_type":"code","source":"# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n\n# ! pip install pytorch-lightning --upgrade\n# ! pip install pytorch-lightning-bolts","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Special Packages\n!pip install PeriodicElements\n!pip install albumentations\n!pip install timm\n!pip install python-Levenshtein\n!pip install torchmetrics","metadata":{"tags":[],"scrolled":true,"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting PeriodicElements\n  Downloading PeriodicElements-1.0-py2.py3-none-any.whl (20 kB)\nInstalling collected packages: PeriodicElements\nSuccessfully installed PeriodicElements-1.0\nRequirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (0.5.2)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.19.5)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.5.1.48)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.18.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.5.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (5.3.1)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.4.0)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (3.3.4)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (7.2.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.5)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.3.5)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\nCollecting timm\n  Downloading timm-0.4.5-py3-none-any.whl (287 kB)\n\u001b[K     |████████████████████████████████| 287 kB 892 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.8.1)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (1.19.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (7.2.0)\nInstalling collected packages: timm\nSuccessfully installed timm-0.4.5\nRequirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.7/site-packages (0.12.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from python-Levenshtein) (49.6.0.post20210108)\nCollecting torchmetrics\n  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n\u001b[K     |████████████████████████████████| 176 kB 894 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.1->torchmetrics) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.1->torchmetrics) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.3.1->torchmetrics) (1.19.5)\nInstalling collected packages: torchmetrics\nSuccessfully installed torchmetrics-0.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"%load_ext tensorboard\n%load_ext autoreload\n%autoreload 2","metadata":{"tags":[],"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch, torchmetrics, timm, re, pickle, Levenshtein\nimport torch.nn as nn\nimport torchvision as tv\nimport pytorch_lightning as pl\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom pathlib import Path\nfrom functools import partial\nfrom collections import defaultdict\nfrom fastprogress import progress_bar\nfrom typing import Optional, Union\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nfrom elements import elements\nfrom albumentations.pytorch import ToTensorV2\n\n\n# Set random seed for reproducibility\nmanualSeed = 999\n#manualSeed = random.randint(1, 10000) # use if you want new results\nprint(\"Random Seed: \", manualSeed)\ntorch.manual_seed(manualSeed);\n\n# This monkey-patch is there to be able to plot tensors\ntorch.Tensor.ndim = property(lambda x: len(x.shape))","metadata":{"tags":[],"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Random Seed:  999\n","output_type":"stream"}]},{"cell_type":"code","source":"CHKPTDIR = Path(\"TranslationChkpts\")\nDATADIR = \"../input/bms-molecular-translation\"\nLABELS_CSV_PATH = f\"{DATADIR}/train_labels.csv\"\nVOCAB_FILEPATH = CHKPTDIR/\"vocab.pt\"\nTRAINPATHS_PATH = CHKPTDIR/\"train_paths.feather\"\nTESTPATHS_PATH = CHKPTDIR/\"test_paths.feather\"\nCHKPTDIR.mkdir(parents=True, exist_ok=True)\n\ntb_logger = pl.loggers.TensorBoardLogger(CHKPTDIR, name=\"InchINet\")\n\nN_WORKERS = 8\nN_SAMPLES = 100000\nBATCH_SIZE = 512\nMAX_LEN = 16 # computed using corpus - max([len(vocab.tokenize(c)) for c in corpus]) 9 + 1 pad + 2 enclosing tokens\nEMB_SIZE = 512\nHDN_SIZE = 10\nINP_SIZE = (128, 128)\nN_INP_CH = 1\nN_OUT_CH = 3\nLR = 1e-2\nEPOCHS = 10\nbeta1 = 0.5","metadata":{"tags":[],"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!ls {DATADIR}","metadata":{"tags":[],"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"sample_submission.csv  test  train  train_labels.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Block\n\n### LightningDataModule API\n\nTo define a DataModule define 5 methods:\n1. prepare_data (how to download(), tokenize, etc…)\n2. setup (how to split, etc…)\n3. train_dataloader\n4. val_dataloader(s)\n5. test_dataloader(s)\n\n#### prepare_data\nUse this method to do things that might write to disk or that need to be done only from a single process in distributed settings.\n1. download\n2. tokenize\n3. etc…\n\n#### setup\nThere are also data operations you might want to perform on every GPU. Use setup to do things like:\n1. count number of classes\n2. build vocabulary\n3. perform train/val/test splits\n4. apply transforms (defined explicitly in your datamodule or assigned in init)\n5. etc…\n","metadata":{}},{"cell_type":"markdown","source":"## Vocab, Tokenizer, and Dataset","metadata":{}},{"cell_type":"code","source":"class Vocab:\n    def __init__(self, vocab, add_special_tokens=True):\n        self.vocab = vocab\n        # Get all elements sorted according to Atomic number\n        data = elements.Elements\n        # All elements in periodic table\n        self.elements = sorted(data, key=lambda i:i.AtomicNumber)  # Based on their AtomicNumber\n        # Sort longer names first for regex pattern formation\n        self.element_symbols = sorted([e.Symbol for e in self.elements], key=lambda e: len(e), reverse=True)\n        # Create regex pattern\n        self.pattern = f\"({'|'.join([f'{e}[0-9]*' for e in self.element_symbols])})\"\n        \n        if type(vocab) != type(None):\n            if add_special_tokens:\n                self.pad_token = \"<pad>\"\n                self.unk_token = \"<unk>\"\n                self.bos_token = \"<bos>\"\n                self.eos_token = \"<eos>\"\n                self.vocab = [self.pad_token, self.unk_token,self.bos_token,self.eos_token] + list(self.vocab)\n            \n            # Adding elements names into vocab and sort according to atomic number\n#             self.vocab = np.unique(self.vocab.tolist() + self.element_symbols)\n#             self.vocab = sorted(self.vocab.tolist(), key=lambda x: eval(f\"elements.{''.join(re.findall(r'[A-Za-z]', x))}.AtomicNumber\"))\n            # create class to index mapping\n            self._ctoi = defaultdict(lambda : self.unk_token, {c:i for i, c in enumerate(self.vocab)})\n                \n    def tokenize(self, string):\n        tokens = re.split(self.pattern, string)\n        tokens = list(filter(None, tokens))\n        return tokens\n    \n    def ctoi(self, c):\n        return self._ctoi[c]\n    \n    def itoc(self, i):\n        return self.vocab[i]\n    \n    def __len__(self):\n        return len(self.vocab)\n    \n    def save_vocab(self, path):\n        torch.save(self.vocab, path)\n        print(\"Saved @\", path)\n    \n    @classmethod\n    def from_corpus(cls, corpus):\n        v = cls(None)\n        vocab = np.unique([w for s in corpus for w in v.tokenize(s)])\n        return cls(vocab)\n    \n    @classmethod\n    def load_vocab(cls, path):\n        vocab = torch.load(path)\n        return cls(vocab)\n    \n# Reference - https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#pretrainedtokenizer\nclass Tokenizer:\n    def __init__(self, vocab=None):\n        self.vocab = vocab # Vocab class instance\n    \n    def tokenizer(self, x):\n        return self.vocab.tokenizer(x)\n    \n    def encode(self, s, max_len=None):\n        tokens = self.vocab.tokenize(s)\n        seq = [self.vocab.ctoi(t) for t in tokens]\n        attn_mask = [1]*len(seq)\n        \n        if max_len:\n            # Add padding to input\n            extra_len = max_len - len(seq) - 2 # 2 for start and end tokens\n            # Add start input token\n            seq = [self.vocab.ctoi(self.vocab.bos_token)] + seq\n            # Add end input token\n            seq += [self.vocab.ctoi(self.vocab.eos_token)]\n            attn_mask += [1, 1]\n            # Add padding token\n            seq += [self.vocab.ctoi(self.vocab.pad_token)]*extra_len\n            attn_mask += [0]*extra_len\n            \n        return {\"inp_seq\": seq, \"attn_mask\": attn_mask}\n    \n    def decode(self, tokens, inp_seq_name=\"inp_seq\"):\n        if isinstance(tokens, dict):\n            seq = tokens[inp_seq_name]\n            if isinstance(seq, (torch.Tensor, np.ndarray)):\n                seq = seq.tolist()\n        else:\n            seq = tokens\n        seq = ''.join([self.vocab.itoc(t) for t in seq])\n        # remove special tokens\n        for special_token in [self.vocab.pad_token, self.vocab.bos_token, self.vocab.eos_token]:\n            seq = seq.replace(special_token, '')\n        return seq\n    \n    @classmethod\n    def fit(cls, corpus):\n        vocab = Vocab.from_corpus(orcpus)\n        return cls(vocab)\n    \n    @classmethod\n    def load_from_file(cls, path):\n        vocab = torch.load(path)\n        return cls(vocab)\n    \nclass ImgtoInChIDataset(Dataset):\n    def __init__(self, paths, df=None, tsfms=None):\n        self.paths = paths\n        self.df = df\n        self.tsfms = tsfms\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        imgpath = Path(self.paths.iloc[idx,0])\n        imgid = imgpath.stem\n        \n        if any(self.df):\n            _imgid, inchi_string = self.df[self.df.image_id == imgid].values[0]\n            inchi_string = inchi_string.split(\"/\")[1]\n            assert _imgid == imgid\n        else:\n            inchi_string = 'TEST_SAMPLE'\n            \n        img = plt.imread(imgpath)\n        if any(self.tsfms):\n            img = self.tsfms(image=img)[\"image\"]\n        \n        return img, inchi_string","metadata":{"tags":[],"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Lightning Data Module","metadata":{}},{"cell_type":"code","source":"print(\"Train...\")\nif TRAINPATHS_PATH.exists():\n    train_paths = pd.read_feather(TRAINPATHS_PATH)\nelse:\n    traingen = (Path(DATADIR)/\"train\").rglob(\"*.*\")\n    train_paths = [str(next(traingen)) for i in range(N_SAMPLES)]\n    train_paths = pd.DataFrame(train_paths, columns=[\"train_paths\"])\n    train_paths = train_paths.applymap(lambda x: str(x))\n    train_paths.to_feather(TRAINPATHS_PATH)\n\nprint(\"Test...\")\nif TESTPATHS_PATH.exists():\n    test_paths = pd.read_feather(TESTPATHS_PATH)\nelse:\n    testgen = (Path(DATADIR)/\"test\").rglob(\"*.*\")\n    test_paths = [str(next(testgen)) for i in range(N_SAMPLES)]\n    test_paths = pd.DataFrame(test_paths, columns=[\"test_paths\"])\n    test_paths = test_paths.applymap(lambda x: str(x))\n    test_paths.to_feather(TESTPATHS_PATH)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Train...\nTest...\n","output_type":"stream"}]},{"cell_type":"code","source":"class ImgToInChIDataModule(pl.LightningDataModule):\n    def __init__(self, labels_csv_path:Union[str, Path], valset_ratio=0.05) -> None:\n        super().__init__()\n        self.labels_csv_path = labels_csv_path\n#         self.batch_size = batch_size\n        self.valset_ratio = valset_ratio\n        self.dims = (1, *INP_SIZE)\n        \n        self.train_tsfms = A.Compose([\n            A.Resize(*INP_SIZE, always_apply=True),\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n            A.RandomCrop(*INP_SIZE),\n            A.RandomBrightnessContrast(p=0.5),\n            A.Normalize(mean=(0.5), std=(0.229)),\n            ToTensorV2(),\n        ])\n        self.test_tsfms = A.Compose([\n            A.Resize(*INP_SIZE, always_apply=True),\n            A.Normalize(mean=(0.5), std=(0.229)),\n            ToTensorV2(),\n        ])\n        \n    def prepare_data(self, verbose=False):\n        \"\"\"Use this method to do things that might write to disk or that\n        need to be done only from a single process in distributed settings.\"\"\"\n        # Load labels in DataFrame\n        if verbose: print(\"Loading labels data...\", end=' ')\n        self.df = pd.read_csv(self.labels_csv_path)\n        if verbose: print(\"DONE!\")\n        \n        # Load image paths\n        if verbose: print(\"Loading paths...\", end=' ')\n        if TRAINPATHS_PATH.exists():\n            self.train_paths = pd.read_feather(TRAINPATHS_PATH)\n        else:\n            self.train_paths = pd.DataFrame(list((Path(DATADIR)/\"train\").rglob(\"*.*\")), columns=[\"train_paths\"])\n            self.train_paths = self.train_paths.applymap(lambda x: str(x))\n            self.train_paths.to_feather(TRAINPATHS_PATH)\n        if TESTPATHS_PATH.exists():\n            self.test_paths = pd.read_feather(TESTPATHS_PATH)\n        else:\n            self.test_paths = pd.DataFrame(list((Path(DATADIR)/\"test\").rglob(\"*.*\")), columns=[\"test_paths\"])\n            self.test_paths = self.test_paths.applymap(lambda x: str(x))\n            self.test_paths.to_feather(TESTPATHS_PATH)\n        if verbose: print(\"DONE!\")\n        \n        # Get Vocab and Tokenizer\n        if verbose: print(\"Loading vocab and tokenizer...\", end=' ')\n        if Path(VOCAB_FILEPATH).exists():\n            vocab = Vocab.load_vocab(VOCAB_FILEPATH)\n        else:\n            corpus = [s.split(\"/\")[1] for s in self.df.InChI.tolist()]\n            vocab = Vocab.from_corpus(corpus)\n#             print(\"# words =\", len(vocab))\n            vocab.save_vocab(VOCAB_FILEPATH)\n        self.vocab_size = len(vocab)\n        self.tokenizer = Tokenizer(vocab)\n        if verbose: print(\"DONE!\")\n                \n    def setup(self, stage:Optional[str]=None) -> None:\n        # Assign train/val datasets for use in dataloaders\n        if stage == 'fit' or stage is None:\n            trainpaths, valpaths = train_test_split(self.train_paths, test_size=self.valset_ratio)\n            self.trainset = ImgtoInChIDataset(trainpaths, self.df, self.train_tsfms)\n            self.valset = ImgtoInChIDataset(valpaths, self.df, self.test_tsfms)\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == 'test' or stage is None:\n            self.testset = ImgtoInChIDataset(self.test_paths, tsfms=self.test_tsfms)\n\n    def train_dataloader(self):\n        return DataLoader(self.trainset, BATCH_SIZE, shuffle=True, \n                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.valset, BATCH_SIZE, shuffle=False, \n                          collate_fn=self.collate_fn, num_workers=N_WORKERS, pin_memory=True)\n    \n    def collate_fn(self, batch):\n        imgs = torch.cat([ins[0].unsqueeze(0) for ins in batch])\n        targets = [ins[1] for ins in batch]\n        targets = [self.tokenizer.encode(t, MAX_LEN) for t in targets]\n        inp_seqs = torch.Tensor([t[\"inp_seq\"] for t in targets]).long()\n        attn_masks = torch.Tensor([t[\"attn_mask\"] for t in targets]).float()\n        return imgs, inp_seqs, attn_masks","metadata":{"tags":[],"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# imgs, inp_seqs, attn_masks = next(iter(trainloader))\n# tb_logger.experiment.add_images(\"Sample images\", imgs)\n\n\n# print(\"SAMPLE BATCH =\", imgs.shape)\n# fig, axes = plt.subplots(4, 8, figsize=(18, 10))\n# for i, ax in enumerate(axes.flat):\n#     ax.imshow(imgs[i].squeeze(0), cmap=\"gray\")\n#     ax.axis(\"off\")","metadata":{"tags":[],"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model\n### At the time of sentence prediction can we use HMMs or [Beam Search](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#overview) to make better decisions?\n\nUse different LSTM layers for each inchi substring like /c /h\n```\n>>> rnn = nn.LSTM(10, 20, 2)\n>>> input = torch.randn(1, 16, 10)\n>>> h0 = torch.randn(2, 16, 20)\n>>> c0 = torch.randn(2, 16, 20)\n```\nchange number of layers here to num sublayers\n\n\n[Model from here](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)","metadata":{"execution":{"iopub.execute_input":"2021-04-02T07:15:59.920534Z","iopub.status.busy":"2021-04-02T07:15:59.920240Z","iopub.status.idle":"2021-04-02T07:15:59.923276Z","shell.execute_reply":"2021-04-02T07:15:59.922767Z","shell.execute_reply.started":"2021-04-02T07:15:59.920510Z"},"tags":[]}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, model_name='resnet18', pretrained=False):\n        super().__init__()\n        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.n_features = self.cnn.fc.in_features\n        self.cnn.global_pool = nn.Identity()\n        self.cnn.fc = nn.Identity()\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.cnn(x)\n        features = features.permute(0, 2, 3, 1)\n        return features\n    \nclass Attention(nn.Module):\n    \"\"\"\n    Attention network for calculate attention value\n    \"\"\"\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \"\"\"\n        :param encoder_dim: input size of encoder network\n        :param decoder_dim: input size of decoder network\n        :param attention_dim: input size of attention network\n        \"\"\"\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n        return attention_weighted_encoding, alpha\n    \nclass DecoderWithAttention(nn.Module):\n    \"\"\"Decoder network with attention network used for training\"\"\"\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n        \"\"\"\n        :param attention_dim: input size of attention network\n        :param embed_dim: input size of embedding network\n        :param decoder_dim: input size of decoder network\n        :param vocab_size: total number of characters used in training\n        :param encoder_dim: input size of encoder network\n        :param dropout: dropout rate\n        \"\"\"\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        self.device = device\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        :param encoder_out: output of encoder network\n        :param encoded_captions: transformed sequence from character to integer\n        :param caption_lengths: length of transformed sequence\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n        # embedding transformed sequence for vector\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n        # initialize hidden state and cell state of LSTM cell\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        # set decode length by caption length - 1 because of omitting start token\n        decode_lengths = (caption_lengths - 1).tolist()\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n        # predict sequence\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n    \n    def predict(self, encoder_out, decode_lengths, tokenizer):\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        # embed start tocken for LSTM input\n        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n        embeddings = self.embedding(start_tockens)\n        # initialize hidden state and cell state of LSTM cell\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n        # predict sequence\n        for t in range(decode_lengths):\n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n                (h, c))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n                break\n            embeddings = self.embedding(torch.argmax(preds, -1))\n        return predictions","metadata":{"tags":[],"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# encoder_net = Encoder()\n# encoder_net(imgs).shape","metadata":{"tags":[],"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, model_name='resnet18', pretrained=False, out_channels=512):\n        super().__init__()\n        last_stride = 2 if INP_SIZE[0] == 256 else 1\n        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.out_channels = out_channels\n        self.cnn.global_pool = nn.Identity()\n        self.cnn.fc = nn.Identity()\n        self.outfc = nn.Sequential(\n            nn.Conv2d(512, out_channels, kernel_size=3, stride=last_stride, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n        self.maxpool = nn.MaxPool2d(4,1,ceil_mode=True)\n\n    def forward(self, x):\n        out = self.cnn(x)\n        out = self.outfc(out)\n        out = out.view(x.size(0), self.out_channels, -1)\n        out = out.permute(0, 2, 1)\n        \n#         out = self.maxpool(out)\n#         out = out.view(out.size(0), -1)\n        return out\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, enc_out_channels=512, device=torch.device(\"cuda\")):\n        super().__init__()\n        \n        \n        self.embd = nn.Embedding(vocab_size, EMB_SIZE)\n        \n        self.init_h = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial hidden state\n        self.init_c = nn.Linear(enc_out_channels, MAX_LEN)  # linear layer to find initial cell state\n        self.lstm = nn.LSTM(enc_out_channels, vocab_size, batch_first=True)\n        self.decfc = nn.Linear(64, MAX_LEN)\n        \n        self.softmax = nn.Softmax(dim=-1)\n        self.device = device\n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n    \n    def forward(self, encoder_out, inp_seqs):\n#         print(\"inp_seq before emb =\", inp_seqs.shape)\n        emb = self.embd(inp_seqs)\n#         print(f\"emb = {emb.shape}, encoder_out = {encoder_out.shape}\")\n        lstm_out, _ = self.lstm(emb)\n        out = lstm_out + encoder_out\n        return out\n    \n    def predict(self, encoder_out, tokenizer):\n        bs = encoder_out.size(0)\n        syn_inp_seqs = torch.tensor(tokenizer.vocab.ctoi(tokenizer.vocab.bos_token), device=encoder_out.device)\n        syn_inp_seqs = torch.repeat_interleave(syn_inp_seqs, bs)\n        syn_inp_seqs = syn_inp_seqs.view(bs, 1)\n#         print(\"syn_inp_seqs before emb =\", syn_inp_seqs.shape)\n        \n        # Predict next tokens to start token\n        pred_emb = []\n        for i in range(MAX_LEN):\n            emb = self.embd(syn_inp_seqs)\n            pred, _ = self.lstm(emb)\n            pred_emb.append(pred)\n            syn_inp_seqs = pred.argmax(dim=-1)\n#         print(\"len =\", len(pred_emb))\n        pred_emb = torch.cat(pred_emb, dim=1)\n#         print(\"pred_emb =\", pred_emb.shape)\n        out = pred_emb + encoder_out\n        return out\n            \nclass InChINet(pl.LightningModule):\n    def __init__(self, vocab_size, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.encoder_net = Encoder(out_channels=vocab_size)\n        self.decoder_net = Decoder(vocab_size)\n        self.loss_fn = nn.CrossEntropyLoss()\n        \n    def forward(self, imgs, inp_seqs):\n        encoder_out = self.encoder_net(imgs)\n#         print(\"Encoder =\", encoder_out.shape)\n        pred_tokens = self.decoder_net(encoder_out, inp_seqs)\n        return pred_tokens\n    \n    def training_step(self, train_batch, batch_idx):\n        imgs, inp_seqs, attn_masks = train_batch\n        output = self.forward(imgs, inp_seqs)\n        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n        # Logging to TensorBoard by default\n        self.log('train_loss', loss, logger=True)\n        return loss\n    \n    def training_epoch_end(self, outputs):\n        for name,params in self.named_parameters():\n            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n    \n    def validation_step(self, val_batch, batch_idx):\n        imgs, inp_seqs, attn_masks = val_batch\n        output = self.predict(imgs, self.tokenizer)\n        loss = self.loss_fn(output.permute(0,2,1).float(), inp_seqs)\n        self.log('val_loss', loss, logger=True)\n        \n        lv_metric = self.calculate_lvdistance(output, inp_seqs)\n        self.logger.log_metrics({\"LvDistance\": lv_metric})\n        return loss\n    \n    def predict(self, imgs, tokenizer):\n        encoder_out = self.encoder_net(imgs)\n        pred_tokens = self.decoder_net.predict(encoder_out, tokenizer)\n        return pred_tokens\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n        lr_scheduler = {\n            'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100),\n            'name': 'AnnealingLR'\n        }\n        return [optimizer], [lr_scheduler]\n    \n    def inference(self, imgs):\n        output = self.predict(imgs, self.tokenizer)\n        return self.postprocessing(output)\n    \n    def calculate_lvdistance(self, output, target):\n        pred_seqs = self.postprocessing(output)\n        batch_distance = np.mean([\n            Levenshtein.distance(pred_seq, self.tokenizer.decode(inp_seq))\n            for pred_seq, inp_seq in zip(pred_seqs, target)\n        ])\n        return batch_distance\n    \n    def postprocessing(self, output):\n        final_preds = []\n        pred_tokens = output.argmax(dim=-1)\n        for i in range(pred_tokens.size(0)): # iterate on each sample\n            pred = pred_tokens[i].unique(dim=-1).tolist()\n            pred = self.tokenizer.decode(pred)\n            res = re.search(r'C', pred)\n            if res:\n                pred = pred[res.span()[0]:]\n            final_preds.append(pred)\n        return final_preds\n    \n# encoder_net = Encoder()\n# decoder_net = Decoder(len(vocab))\n# encoder_out = encoder_net(imgs)\n# print(\"Encoder =\", encoder_out.shape)\n# print(inp_seqs.shape)\n# pred_tokens = decoder_net(encoder_out, inp_seqs, tokenizer)\n# print(pred_tokens.shape)\n# pred_tokens = decoder_net.predict(encoder_out, tokenizer)\n# pred_tokens.shape","metadata":{"tags":[],"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# %tensorboard --logdir {CHKPTDIR}\n\ndm = ImgToInChIDataModule(LABELS_CSV_PATH)\ndm.prepare_data(verbose=True)\nmodel = InChINet(dm.vocab_size, dm.tokenizer)\n# Add network graph to tensorboard\n# tb_logger.log_graph(model, [imgs[0].unsqueeze(0).to(model.device), inp_seqs[0].unsqueeze(0).to(model.device)])\nlr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n\ntrainer = pl.Trainer(gpus=1, auto_lr_find=True, max_epochs=10, precision=16, profiler=\"simple\", \n                     default_root_dir=CHKPTDIR, logger=tb_logger, callbacks=[lr_monitor])\n\ntrainer.fit(model, dm)","metadata":{"tags":[],"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Loading labels data... DONE!\nLoading paths... DONE!\nLoading vocab and tokenizer... Saved @ TranslationChkpts/vocab.pt\nDONE!\n","output_type":"stream"},{"name":"stderr","text":"GPU available: True, used: True\nTPU available: None, using: 0 TPU cores\nUsing native 16bit precision.\n\n  | Name        | Type             | Params\n-------------------------------------------------\n0 | encoder_net | Encoder          | 13.1 M\n1 | decoder_net | Decoder          | 1.8 M \n2 | loss_fn     | CrossEntropyLoss | 0     \n-------------------------------------------------\n15.0 M    Trainable params\n0         Non-trainable params\n15.0 M    Total params\n59.886    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949f73884ed24a74906af91a17819edf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n  warnings.warn(*args, **kwargs)\n\n\nProfiler Report\n\nAction                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n------------------------------------------------------------------------------------------------------------------------------------\nTotal                              \t|  -              \t|_              \t|  2.3718e+04     \t|  100 %          \t|\n------------------------------------------------------------------------------------------------------------------------------------\nrun_training_epoch                 \t|  2.2606e+04     \t|1              \t|  2.2606e+04     \t|  95.311         \t|\nget_train_batch                    \t|  148.19         \t|151            \t|  2.2377e+04     \t|  94.347         \t|\nrun_training_batch                 \t|  0.39205        \t|151            \t|  59.199         \t|  0.2496         \t|\noptimizer_step_and_closure_0       \t|  0.38826        \t|151            \t|  58.627         \t|  0.24718        \t|\ntraining_step_and_backward         \t|  0.17772        \t|151            \t|  26.835         \t|  0.11314        \t|\nmodel_backward                     \t|  0.08173        \t|151            \t|  12.341         \t|  0.052033       \t|\nmodel_forward                      \t|  0.073887       \t|151            \t|  11.157         \t|  0.04704        \t|\ntraining_step                      \t|  0.071419       \t|151            \t|  10.784         \t|  0.045469       \t|\non_train_batch_end                 \t|  0.02996        \t|151            \t|  4.5239         \t|  0.019074       \t|\nevaluation_step_and_end            \t|  0.68891        \t|2              \t|  1.3778         \t|  0.0058091      \t|\nvalidation_step                    \t|  0.68791        \t|2              \t|  1.3758         \t|  0.0058007      \t|\ncache_result                       \t|  0.00019894     \t|772            \t|  0.15358        \t|  0.00064754     \t|\non_train_end                       \t|  0.13675        \t|1              \t|  0.13675        \t|  0.00057655     \t|\non_train_start                     \t|  0.11856        \t|1              \t|  0.11856        \t|  0.00049988     \t|\non_after_backward                  \t|  0.00018347     \t|151            \t|  0.027704       \t|  0.00011681     \t|\non_before_zero_grad                \t|  0.00016411     \t|151            \t|  0.02478        \t|  0.00010448     \t|\non_validation_batch_end            \t|  0.010862       \t|2              \t|  0.021724       \t|  9.1594e-05     \t|\non_batch_end                       \t|  0.00014259     \t|151            \t|  0.021531       \t|  9.0779e-05     \t|\non_train_batch_start               \t|  0.000139       \t|151            \t|  0.02099        \t|  8.8496e-05     \t|\ntraining_step_end                  \t|  0.00010593     \t|151            \t|  0.015995       \t|  6.7437e-05     \t|\non_batch_start                     \t|  7.1476e-05     \t|151            \t|  0.010793       \t|  4.5505e-05     \t|\non_validation_end                  \t|  0.0047137      \t|1              \t|  0.0047137      \t|  1.9874e-05     \t|\non_epoch_start                     \t|  0.0032343      \t|1              \t|  0.0032343      \t|  1.3636e-05     \t|\non_validation_batch_start          \t|  0.00010751     \t|2              \t|  0.00021501     \t|  9.0654e-07     \t|\non_epoch_end                       \t|  9.236e-05      \t|1              \t|  9.236e-05      \t|  3.8941e-07     \t|\non_validation_start                \t|  8.1936e-05     \t|1              \t|  8.1936e-05     \t|  3.4546e-07     \t|\nvalidation_step_end                \t|  3.6862e-05     \t|2              \t|  7.3725e-05     \t|  3.1084e-07     \t|\non_validation_epoch_end            \t|  5.8473e-05     \t|1              \t|  5.8473e-05     \t|  2.4653e-07     \t|\non_validation_epoch_start          \t|  4.456e-05      \t|1              \t|  4.456e-05      \t|  1.8787e-07     \t|\non_fit_start                       \t|  3.9517e-05     \t|1              \t|  3.9517e-05     \t|  1.6661e-07     \t|\non_train_epoch_start               \t|  2.679e-05      \t|1              \t|  2.679e-05      \t|  1.1295e-07     \t|\non_before_accelerator_backend_setup\t|  1.3015e-05     \t|1              \t|  1.3015e-05     \t|  5.4874e-08     \t|\n\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"# device = torch.device(\"xla\")\n# total_distance = []\n# for (imgs, inp_seqs, attn_masks) in progress_bar(valloader):\n#     model.eval()\n#     model = model.to(device)\n#     imgs = imgs.to(device)\n#     pred_seqs = model.inference(imgs)\n#     batch_distance = np.mean([\n#         Levenshtein.distance(pred_seq, tokenizer.decode(inp_seq))\n#         for pred_seq, inp_seq in zip(pred_seqs, inp_seqs)\n#     ])\n# #     print(batch_distance)\n#     total_distance += batch_distance\n# np.nanmean(total_distance)","metadata":{"tags":[],"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"torch.","metadata":{"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-978203ef2d91>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    torch.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-16-978203ef2d91>, line 1)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}